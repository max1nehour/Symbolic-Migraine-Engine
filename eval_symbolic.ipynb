{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85bbe28-0db7-4c5d-802e-ad21ad9224d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.getcwd())\n",
    "os.chdir(\"/Users/M1HR/Desktop/MIGRAINE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86843d2-7a5b-4c52-b238-4f29384140ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    classification_report, confusion_matrix\n",
    ")\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 8)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# LOAD DATA\n",
    "# =============================================================================\n",
    "\n",
    "def load_ground_truth(csv_path):\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"LOADING GROUND TRUTH\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    df = pd.read_csv(csv_path)\n",
    "    \n",
    "    # Add Patient_ID if not present\n",
    "    if 'Patient_ID' not in df.columns:\n",
    "        df['Patient_ID'] = df.index + 1\n",
    "    \n",
    "    print(f\" Loaded {len(df)} patients\")\n",
    "    \n",
    "    if 'Type' not in df.columns:\n",
    "        raise ValueError(\"CSV must have 'Type' column\")\n",
    "    \n",
    "    print(f\"\\nGround Truth Distribution:\")\n",
    "    dist = df['Type'].value_counts().sort_index()\n",
    "    for diag, count in dist.items():\n",
    "        print(f\"  {diag:40s}: {count:4d} ({count/len(df)*100:5.1f}%)\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def load_symbolic_predictions(json_path):\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"LOADING SYMBOLIC PREDICTIONS\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    with open(json_path, 'r') as f:\n",
    "        results = json.load(f)\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    rows = []\n",
    "    for result in results:\n",
    "        rows.append({\n",
    "            'patient_id': result['patient_id'],\n",
    "            'diagnosis': result['diagnosis'],\n",
    "            'code': result.get('code', 'N/A'),\n",
    "            'confidence': result.get('confidence', 'unknown')\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(rows)\n",
    "    \n",
    "    print(f\"✓ Loaded {len(df)} predictions\")\n",
    "    \n",
    "    print(f\"\\nPredicted Distribution:\")\n",
    "    dist = df['diagnosis'].value_counts().sort_index()\n",
    "    for diag, count in dist.items():\n",
    "        print(f\"  {diag:40s}: {count:4d} ({count/len(df)*100:5.1f}%)\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def normalize_labels(label):\n",
    "    \n",
    "    if pd.isna(label):\n",
    "        return 'Unknown'\n",
    "    \n",
    "    label = str(label).strip().lower()\n",
    "    \n",
    "    mappings = {\n",
    "        # Ground truth variations\n",
    "        'migraine without aura': 'Migraine without aura',\n",
    "        'migraine with aura': 'Typical aura with headache',\n",
    "        'migraine with typical aura': 'Typical aura with headache',\n",
    "        'typical aura with migraine': 'Typical aura with headache',\n",
    "        'typical aura with headache': 'Typical aura with headache',\n",
    "        'typical aura without headache': 'Typical aura without headache',\n",
    "        'typical aura without migraine': 'Typical aura without headache',\n",
    "        'familial hemiplegic migraine': 'Familial hemiplegic migraine',\n",
    "        'sporadic hemiplegic migraine': 'Sporadic hemiplegic migraine',\n",
    "        'basilar-type migraine': 'Basilar-type aura',\n",
    "        'basilar-type aura': 'Basilar-type aura',\n",
    "        'basilar migraine': 'Basilar-type aura',\n",
    "        'other': 'Other',\n",
    "        'no diagnosis': 'Other'\n",
    "    }\n",
    "    \n",
    "    return mappings.get(label, label.title())\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# MERGE AND EVALUATE\n",
    "# =============================================================================\n",
    "\n",
    "def merge_and_evaluate(ground_truth_df, predictions_df):\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"MERGING DATA\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    merged = ground_truth_df.merge(\n",
    "        predictions_df,\n",
    "        left_on='Patient_ID',\n",
    "        right_on='patient_id',\n",
    "        how='inner'\n",
    "    )\n",
    "    \n",
    "    print(f\" Merged {len(merged)} patients\")\n",
    "    \n",
    "    # Normalize labels\n",
    "    merged['true_diagnosis'] = merged['Type'].apply(normalize_labels)\n",
    "    merged['predicted_diagnosis'] = merged['diagnosis'].apply(normalize_labels)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    y_true = merged['true_diagnosis']\n",
    "    y_pred = merged['predicted_diagnosis']\n",
    "    \n",
    "    metrics = {\n",
    "        'accuracy': accuracy_score(y_true, y_pred),\n",
    "        'precision_macro': precision_score(y_true, y_pred, average='macro', zero_division=0),\n",
    "        'precision_weighted': precision_score(y_true, y_pred, average='weighted', zero_division=0),\n",
    "        'recall_macro': recall_score(y_true, y_pred, average='macro', zero_division=0),\n",
    "        'recall_weighted': recall_score(y_true, y_pred, average='weighted', zero_division=0),\n",
    "        'f1_macro': f1_score(y_true, y_pred, average='macro', zero_division=0),\n",
    "        'f1_weighted': f1_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"METRICS\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"\\n  Accuracy:             {metrics['accuracy']:.4f} ({metrics['accuracy']*100:.1f}%)\")\n",
    "    print(f\"  Precision (macro):    {metrics['precision_macro']:.4f}\")\n",
    "    print(f\"  Precision (weighted): {metrics['precision_weighted']:.4f}\")\n",
    "    print(f\"  Recall (macro):       {metrics['recall_macro']:.4f}\")\n",
    "    print(f\"  Recall (weighted):    {metrics['recall_weighted']:.4f}\")\n",
    "    print(f\"  F1 (macro):           {metrics['f1_macro']:.4f}\")\n",
    "    print(f\"  F1 (weighted):        {metrics['f1_weighted']:.4f}\")\n",
    "    \n",
    "    # Classification report\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"CLASSIFICATION REPORT\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    print(classification_report(y_true, y_pred, zero_division=0))\n",
    "    \n",
    "    return merged, metrics\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# ERROR ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "def analyze_errors(merged_df):\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"ERROR ANALYSIS\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    errors = merged_df[merged_df['true_diagnosis'] != merged_df['predicted_diagnosis']].copy()\n",
    "    \n",
    "    total = len(merged_df)\n",
    "    error_count = len(errors)\n",
    "    \n",
    "    print(f\"\\nTotal: {total} patients\")\n",
    "    print(f\"   Correct: {total - error_count} ({(total-error_count)/total*100:.1f}%)\")\n",
    "    print(f\"   Errors:  {error_count} ({error_count/total*100:.1f}%)\")\n",
    "    \n",
    "    if error_count == 0:\n",
    "        print(\"\\n Perfect accuracy!\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\n Top 10 Error Patterns:\")\n",
    "    print(f\"{'─'*80}\")\n",
    "    \n",
    "    error_patterns = errors.groupby(['true_diagnosis', 'predicted_diagnosis']).size()\n",
    "    error_patterns = error_patterns.sort_values(ascending=False).head(10)\n",
    "    \n",
    "    for (true, pred), count in error_patterns.items():\n",
    "        pct = (count / total) * 100\n",
    "        print(f\"  {true:35s} → {pred:35s}: {count:3d} ({pct:4.1f}%)\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# VISUALIZATIONS\n",
    "# =============================================================================\n",
    "\n",
    "def plot_confusion_matrix(merged_df, output_path):\n",
    "    \n",
    "    y_true = merged_df['true_diagnosis']\n",
    "    y_pred = merged_df['predicted_diagnosis']\n",
    "    \n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    labels = sorted(y_true.unique())\n",
    "    \n",
    "    cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12, 10))\n",
    "    \n",
    "    sns.heatmap(cm_norm, annot=True, fmt='.2f', cmap='inferno',\n",
    "                xticklabels=labels, yticklabels=labels,\n",
    "                ax=ax, cbar_kws={'label': 'Proportion'})\n",
    "    \n",
    "    ax.set_title('Symbolic Reasoning - Confusion Matrix', fontsize=14, fontweight='bold')\n",
    "    ax.set_xlabel('Predicted Diagnosis', fontsize=12)\n",
    "    ax.set_ylabel('True Diagnosis', fontsize=12)\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\" Saved: {output_path}\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_metrics(metrics, output_path):\n",
    "    \n",
    "    metric_names = ['Accuracy', 'Precision\\n(weighted)', 'Recall\\n(weighted)', 'F1\\n(weighted)']\n",
    "    values = [\n",
    "        metrics['accuracy'],\n",
    "        metrics['precision_weighted'],\n",
    "        metrics['recall_weighted'],\n",
    "        metrics['f1_weighted']\n",
    "    ]\n",
    "    \n",
    "    okabe_ito = {\n",
    "        'orange': '#E69F00',\n",
    "        'sky_blue': '#56B4E9',\n",
    "        'bluish_green': '#009E73'\n",
    "    }\n",
    "    \n",
    "    colors = []\n",
    "    for v in values:\n",
    "        if v >= 0.8:\n",
    "            colors.append(okabe_ito['orange'])       # Excellent\n",
    "        elif v >= 0.6:\n",
    "            colors.append(okabe_ito['sky_blue'])     # Good\n",
    "        else:\n",
    "            colors.append(okabe_ito['bluish_green']) # Fair\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    bars = ax.bar(metric_names, values, color=colors, alpha=0.8, edgecolor='black', linewidth=1.5)\n",
    "    \n",
    "    ax.set_ylabel('Score', fontsize=12)\n",
    "    ax.set_title('Symbolic Reasoning Performance', fontsize=14, fontweight='bold')\n",
    "    ax.set_ylim([0, 1.1])\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    for bar, val in zip(bars, values):\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "               f'{val:.3f}',\n",
    "               ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "    \n",
    "    ax.axhline(y=0.8, color='#E69F00', linestyle='--', alpha=0.4, label='Excellent (≥0.8)')\n",
    "    ax.axhline(y=0.6, color='#56B4E9', linestyle='--', alpha=0.4, label='Good (≥0.6)')\n",
    "    ax.legend(loc='lower right')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"Saved: {output_path}\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# SAVE RESULTS\n",
    "# =============================================================================\n",
    "\n",
    "def save_results(merged_df, metrics, output_dir):\n",
    "    \n",
    "    output_dir = Path(output_dir)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"SAVING RESULTS\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Evaluation results\n",
    "    results_path = output_dir / 'evaluation_results.csv'\n",
    "    merged_df.to_csv(results_path, index=False)\n",
    "    print(f\"✓ Results: {results_path}\")\n",
    "    \n",
    "    # Errors only\n",
    "    errors = merged_df[merged_df['true_diagnosis'] != merged_df['predicted_diagnosis']]\n",
    "    if len(errors) > 0:\n",
    "        errors_path = output_dir / 'error_cases.csv'\n",
    "        errors.to_csv(errors_path, index=False)\n",
    "        print(f\"✓ Errors: {errors_path}\")\n",
    "    \n",
    "    # Metrics\n",
    "    metrics_path = output_dir / 'metrics.json'\n",
    "    with open(metrics_path, 'w') as f:\n",
    "        json.dump(metrics, f, indent=2)\n",
    "    print(f\"✓ Metrics: {metrics_path}\")\n",
    "    \n",
    "    print(f\"\\n✓ All results saved to: {output_dir}\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN\n",
    "# =============================================================================\n",
    "\n",
    "def evaluate_symbolic(\n",
    "    ground_truth_path='data/migraine_with_id.csv',\n",
    "    predictions_path='data/diagnoses/ichd3_diagnoses_final.json',\n",
    "    output_dir='evaluation_results/symbolic'\n",
    "):\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"SYMBOLIC REASONING EVALUATION\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Load data\n",
    "    ground_truth = load_ground_truth(ground_truth_path)\n",
    "    predictions = load_symbolic_predictions(predictions_path)\n",
    "    \n",
    "    # Merge and evaluate\n",
    "    merged, metrics = merge_and_evaluate(ground_truth, predictions)\n",
    "    \n",
    "    # Error analysis\n",
    "    analyze_errors(merged)\n",
    "    \n",
    "    # Visualizations\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"GENERATING VISUALIZATIONS\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    output_dir = Path(output_dir)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    plot_confusion_matrix(merged, output_dir / 'confusion_matrix.png')\n",
    "    plot_metrics(metrics, output_dir / 'metrics.png')\n",
    "    \n",
    "    # Save results\n",
    "    save_results(merged, metrics, output_dir)\n",
    "    \n",
    "    # Final summary\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\" EVALUATION COMPLETE!\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    print(f\"\\n FINAL SUMMARY:\")\n",
    "    print(f\"  Total Patients:  {len(merged)}\")\n",
    "    print(f\"  Accuracy:        {metrics['accuracy']:.4f} ({metrics['accuracy']*100:.1f}%)\")\n",
    "    print(f\"  F1 (weighted):   {metrics['f1_weighted']:.4f}\")\n",
    "    print(f\"  Precision:       {metrics['precision_weighted']:.4f}\")\n",
    "    print(f\"  Recall:          {metrics['recall_weighted']:.4f}\")\n",
    "    \n",
    "    print(f\"\\n Results: {output_dir}\")\n",
    "    \n",
    "    return metrics, merged\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    metrics, results = evaluate_symbolic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53131a3f-040e-440b-b630-2f6467c29ceb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
