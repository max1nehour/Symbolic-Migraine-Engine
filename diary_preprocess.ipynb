{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b64e032-01ee-48e0-92e9-ab1c851849d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f667ff-d039-4666-90f1-f0b6f058a539",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.getcwd())\n",
    "os.chdir(\"/Users/M1HR/Desktop/MIGRAINE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9b2993-b596-4309-ac24-91337cb447db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "# =============================================================================\n",
    "# 1. Dictionaries\n",
    "# =============================================================================\n",
    "\n",
    "SAFE_TYPOS = {\n",
    "    \"wont\": \"won't\",\n",
    "    \"dont\": \"don't\",\n",
    "    \"doesnt\": \"doesn't\",\n",
    "    \"didnt\": \"didn't\",\n",
    "    \"havent\": \"haven't\",\n",
    "    \"decnet\": \"decent\",\n",
    "    \"ther\": \"there\",\n",
    "    \"teh\": \"the\",\n",
    "}\n",
    "\n",
    "MEDICAL_TYPOS = {\n",
    "    \"nausia\": \"nausea\",\n",
    "    \"nausua\": \"nausea\",\n",
    "    \"photophbia\": \"photophobia\",\n",
    "    \"phonophbia\": \"phonophobia\",\n",
    "    \"migrainea\": \"migraine\",\n",
    "    \"scotma\": \"scotoma\",\n",
    "}\n",
    "\n",
    "SYMPTOM_SYNONYMS = {\n",
    "    \"light sensitivity\": \"photophobia\",\n",
    "    \"light sensitive\": \"photophobia\",\n",
    "    \"bright lights hurt\": \"photophobia\",\n",
    "    \"light hurts my eyes\": \"photophobia\",\n",
    "    \"sound sensitivity\": \"phonophobia\",\n",
    "    \"noise sensitivity\": \"phonophobia\",\n",
    "    \"zigzag lines\": \"visual aura\",\n",
    "    \"blind spots\": \"scotoma\",\n",
    "    \"pounding\": \"throbbing\",\n",
    "    \"pulsating\": \"throbbing\",\n",
    "}\n",
    "\n",
    "# =============================================================================\n",
    "# 2. Basic cleaning\n",
    "# =============================================================================\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Clean text while preserving important content\"\"\"\n",
    "    text = text.replace(\"–\", \"-\")\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 3. Typo correction\n",
    "# =============================================================================\n",
    "\n",
    "def fix_typos(text):\n",
    "    for wrong, right in SAFE_TYPOS.items():\n",
    "        text = re.sub(rf\"\\b{wrong}\\b\", right, text, flags=re.IGNORECASE)\n",
    "\n",
    "    for wrong, right in MEDICAL_TYPOS.items():\n",
    "        text = re.sub(rf\"\\b{wrong}\\b\", right, text, flags=re.IGNORECASE)\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 4. Synonym normalization\n",
    "# =============================================================================\n",
    "\n",
    "def normalize_synonyms(text):\n",
    "    text_lower = text.lower()\n",
    "    for syn, canonical in SYMPTOM_SYNONYMS.items():\n",
    "        text_lower = text_lower.replace(syn, canonical)\n",
    "    return text_lower\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 5. Sentence segmentation\n",
    "# =============================================================================\n",
    "\n",
    "def segment_sentences(text):\n",
    "    try:\n",
    "        sentences = sent_tokenize(text)\n",
    "        return [s.strip() for s in sentences if len(s.strip()) > 1]\n",
    "    except:\n",
    "        # Fallback if NLTK fails\n",
    "        return [s.strip() for s in text.split('.') if len(s.strip()) > 1]\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 6. IMPROVED: Extract day entries with multiple format support\n",
    "# =============================================================================\n",
    "\n",
    "def normalize_day_headers(raw_text):\n",
    "\n",
    "     \n",
    "    pattern = re.compile(\n",
    "        r'\\*{0,2}\\s*day\\s*(\\d{1,2})\\s*\\*{0,2}\\s*[:：\\-]?',\n",
    "        re.IGNORECASE\n",
    "    )\n",
    "    \n",
    "    normalized_text = pattern.sub(lambda m: f\"\\nDay {int(m.group(1))}:\", raw_text)\n",
    "    \n",
    "    return normalized_text\n",
    "\n",
    "\n",
    "def extract_day_entries(raw_text, debug=False):\n",
    "    \n",
    "    if debug:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\"DEBUG: extract_day_entries\")\n",
    "        print(f\"{'='*80}\")\n",
    "        print(f\"Input length: {len(raw_text)} characters\")\n",
    "        print(f\"First 200 chars: {raw_text[:200]}\")\n",
    "    \n",
    "    # Step 1: Normalize headers\n",
    "    normalized = normalize_day_headers(raw_text)\n",
    "    \n",
    "    if debug:\n",
    "        print(f\"\\nAfter normalization (first 300 chars):\")\n",
    "        print(normalized[:300])\n",
    "    \n",
    "    # Step 2: Split into lines\n",
    "    lines = normalized.split('\\n')\n",
    "    lines = [line.strip() for line in lines if line.strip()]\n",
    "    \n",
    "    if debug:\n",
    "        print(f\"\\nTotal lines after split: {len(lines)}\")\n",
    "        print(f\"First 10 lines:\")\n",
    "        for i, line in enumerate(lines[:10]):\n",
    "            print(f\"  {i}: {line}\")\n",
    "    \n",
    "    # Step 3: Parse day entries\n",
    "    day_entries = {i: \"\" for i in range(1, 31)}\n",
    "    \n",
    "    # Pattern to match normalized day headers\n",
    "    day_pattern = re.compile(r'^Day (\\d{1,2}):', re.IGNORECASE)\n",
    "    \n",
    "    current_day = None\n",
    "    current_text_lines = []\n",
    "    \n",
    "    for line_idx, line in enumerate(lines):\n",
    "        match = day_pattern.match(line)\n",
    "        \n",
    "        if match:\n",
    "            # Save previous day if exists\n",
    "            if current_day is not None:\n",
    "                day_entries[current_day] = ' '.join(current_text_lines).strip()\n",
    "                \n",
    "                if debug and current_text_lines:\n",
    "                    print(f\"\\n✓ Saved Day {current_day}: {len(' '.join(current_text_lines))} chars\")\n",
    "            \n",
    "            # Start new day\n",
    "            current_day = int(match.group(1))\n",
    "            current_text_lines = []\n",
    "            \n",
    "            # Check if text follows on same line after colon\n",
    "            text_after_colon = line[match.end():].strip()\n",
    "            if text_after_colon:\n",
    "                current_text_lines.append(text_after_colon)\n",
    "                \n",
    "                if debug:\n",
    "                    print(f\"\\n→ Day {current_day} starts (inline text): {text_after_colon[:50]}\")\n",
    "            else:\n",
    "                if debug:\n",
    "                    print(f\"\\n→ Day {current_day} starts (text on next line)\")\n",
    "        \n",
    "        elif current_day is not None:\n",
    "            # This line is part of current day's text\n",
    "            if line:\n",
    "                current_text_lines.append(line)\n",
    "                \n",
    "                if debug:\n",
    "                    print(f\"  + Adding to Day {current_day}: {line[:50]}\")\n",
    "    \n",
    "    # Save last day\n",
    "    if current_day is not None:\n",
    "        day_entries[current_day] = ' '.join(current_text_lines).strip()\n",
    "        \n",
    "        if debug:\n",
    "            print(f\"\\n✓ Saved Day {current_day} (last): {len(' '.join(current_text_lines))} chars\")\n",
    "    \n",
    "    # Debug summary\n",
    "    if debug:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\"EXTRACTION SUMMARY\")\n",
    "        print(f\"{'='*80}\")\n",
    "        non_empty = sum(1 for v in day_entries.values() if v)\n",
    "        print(f\"Total days with content: {non_empty}/30\")\n",
    "        print(f\"\\nDay-by-day breakdown:\")\n",
    "        for day in range(1, 31):\n",
    "            text = day_entries[day]\n",
    "            status = \"✓\" if text else \"✗\"\n",
    "            preview = text[:60] + \"...\" if len(text) > 60 else text\n",
    "            print(f\"  {status} Day {day:2d}: {preview if text else '(empty)'}\")\n",
    "    \n",
    "    return day_entries\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 7. Full preprocessing for one entry\n",
    "# =============================================================================\n",
    "\n",
    "def preprocess_one_entry(text, day_num=None):\n",
    "    \n",
    "    if not text or not text.strip():\n",
    "        return {\n",
    "            \"day\": f\"Day {day_num}\" if day_num else \"\",\n",
    "            \"clean_text\": \"\",\n",
    "            \"sentences\": []\n",
    "        }\n",
    "    \n",
    "    # Clean and normalize\n",
    "    text = clean_text(text)\n",
    "    text = fix_typos(text)\n",
    "    text_normalized = normalize_synonyms(text)\n",
    "    \n",
    "    # Segment sentences\n",
    "    sentences = segment_sentences(text_normalized)\n",
    "    \n",
    "    return {\n",
    "        \"day\": f\"Day {day_num}\" if day_num else \"\",\n",
    "        \"clean_text\": text_normalized,\n",
    "        \"sentences\": sentences\n",
    "    }\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 8. Load diary files\n",
    "# =============================================================================\n",
    "\n",
    "def load_diary_files(folder):\n",
    "    folder = Path(folder)\n",
    "    \n",
    "    if not folder.exists():\n",
    "        print(f\" ERROR: Folder not found: {folder}\")\n",
    "        return []\n",
    "    \n",
    "    files = sorted(list(folder.glob(\"*.txt\")))\n",
    "    \n",
    "    if not files:\n",
    "        print(f\" WARNING: No .txt files found in {folder}\")\n",
    "        return []\n",
    "    \n",
    "    print(f\"✓ Found {len(files)} files in {folder}\")\n",
    "    \n",
    "    data = []\n",
    "    for f in files:\n",
    "        try:\n",
    "            with open(f, \"r\", encoding=\"utf-8\") as infile:\n",
    "                content = infile.read().strip()\n",
    "            data.append({\"filename\": f.name, \"raw\": content})\n",
    "        except Exception as e:\n",
    "            print(f\" ERROR reading {f.name}: {e}\")\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 9. Preprocess entire corpus\n",
    "# =============================================================================\n",
    "\n",
    "def preprocess_corpus(folder, model_name, outdir, debug=False):\n",
    "\n",
    "    outdir = Path(outdir)\n",
    "    outdir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"PREPROCESSING: {model_name}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Input folder: {folder}\")\n",
    "    print(f\"Output folder: {outdir}\")\n",
    "    \n",
    "    # Load raw diary files\n",
    "    raw_files = load_diary_files(folder)\n",
    "    \n",
    "    if not raw_files:\n",
    "        print(f\" No files to process!\")\n",
    "        return []\n",
    "    \n",
    "    processed_rows = []\n",
    "    \n",
    "    for file_idx, file in enumerate(raw_files):\n",
    "        print(f\"\\n{'─'*80}\")\n",
    "        print(f\"Processing file {file_idx + 1}/{len(raw_files)}: {file['filename']}\")\n",
    "        print(f\"{'─'*80}\")\n",
    "        \n",
    "        # Extract day entries\n",
    "        day_entries = extract_day_entries(file[\"raw\"], debug=debug)\n",
    "        \n",
    "        # Preprocess each day\n",
    "        for day_num in range(1, 31):\n",
    "            text = day_entries[day_num]\n",
    "            \n",
    "            # Preprocess\n",
    "            processed = preprocess_one_entry(text, day_num=day_num)\n",
    "            \n",
    "            # Store result\n",
    "            processed_rows.append({\n",
    "                \"model\": model_name,\n",
    "                \"filename\": file[\"filename\"],\n",
    "                \"day\": processed[\"day\"],\n",
    "                \"raw_text\": text,  # Keep original\n",
    "                \"clean_text\": processed[\"clean_text\"],\n",
    "                \"sentences\": processed[\"sentences\"],\n",
    "                \"has_content\": bool(text.strip())\n",
    "            })\n",
    "        \n",
    "        # Summary for this file\n",
    "        non_empty = sum(1 for d in range(1, 31) if day_entries[d])\n",
    "        print(f\"\\n✓ Extracted {non_empty}/30 days with content\")\n",
    "    \n",
    "    # Save JSON\n",
    "    out_json = outdir / f\"{model_name}.json\"\n",
    "    with open(out_json, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(processed_rows, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"\\n✓ Saved {len(processed_rows)} entries → {out_json}\")\n",
    "    \n",
    "    # Save summary CSV\n",
    "    summary_df = pd.DataFrame(processed_rows)\n",
    "    out_csv = outdir / f\"{model_name}.csv\"\n",
    "    summary_df.to_csv(out_csv, index=False)\n",
    "    print(f\"✓ Saved summary → {out_csv}\")\n",
    "    \n",
    "    return processed_rows\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 10. DEBUG: Test extraction on sample texts\n",
    "# =============================================================================\n",
    "\n",
    "def test_extraction():\n",
    "    \n",
    "    test_cases = [\n",
    "        # Case 1: Standard format\n",
    "        \"\"\"Patient 1\n",
    "Day 1: No headache today.\n",
    "Day 2: HA started at 2pm.\n",
    "Day 3: Felt fine.\"\"\",\n",
    "        \n",
    "        # Case 2: Lowercase\n",
    "        \"\"\"day 1: No headache.\n",
    "day 2: Bad HA today.\n",
    "day 3: Fine.\"\"\",\n",
    "        \n",
    "        # Case 3: Bold with asterisks\n",
    "        \"\"\"**Day 1:** No headache.\n",
    "**Day 2:** HA at noon.\n",
    "**Day 3:** Good day.\"\"\",\n",
    "        \n",
    "        # Case 4: No space, no colon\n",
    "        \"\"\"Day1 No headache\n",
    "Day2 HA today\n",
    "Day3 Fine\"\"\",\n",
    "        \n",
    "        # Case 5: Mixed formats\n",
    "        \"\"\"Patient 5\n",
    "Day 1: No headache today.\n",
    "\n",
    "**Day 2:**\n",
    "Bad headache started at 2pm. Took meds.\n",
    "\n",
    "day 3: felt fine\n",
    "\n",
    "Day4 - HA again\n",
    "\n",
    "**day 5**\n",
    "No HA today.\"\"\",\n",
    "        \n",
    "        # Case 6: Text on next line\n",
    "        \"\"\"Day 1:\n",
    "No headache today. Felt great.\n",
    "\n",
    "Day 2:\n",
    "Woke up w/ bad HA. 7/10. Took ibuprofen.\"\"\"\n",
    "    ]\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"TESTING EXTRACTION WITH VARIOUS FORMATS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    for i, test_text in enumerate(test_cases, 1):\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"TEST CASE {i}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        print(f\"Input:\\n{test_text}\")\n",
    "        print(f\"\\n{'─'*80}\")\n",
    "        \n",
    "        entries = extract_day_entries(test_text, debug=True)\n",
    "        \n",
    "        print(f\"\\n{'─'*80}\")\n",
    "        print(\"RESULT:\")\n",
    "        for day, text in entries.items():\n",
    "            if text:\n",
    "                print(f\"  Day {day}: {text}\")\n",
    "        \n",
    "        input(f\"\\nPress Enter to continue to test case {i+1}...\\n\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 11. Full pipeline\n",
    "# =============================================================================\n",
    "\n",
    "def run_full_pipeline(debug=False):\n",
    "    \n",
    "    qwen_dir = \"data/selected_diary/qwen\"\n",
    "    llama_dir = \"data/selected_diary/llama3\"\n",
    "    output_root = Path(\"data/preprocessed\")\n",
    "    output_root.mkdir(exist_ok=True)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"STARTING FULL PREPROCESSING PIPELINE\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Process Qwen\n",
    "    qwen = preprocess_corpus(qwen_dir, \"qwen\", output_root, debug=debug)\n",
    "    \n",
    "    # Process Llama\n",
    "    llama = preprocess_corpus(llama_dir, \"llama3\", output_root, debug=debug)\n",
    "    \n",
    "    # Combine all into a single CSV\n",
    "    if qwen or llama:\n",
    "        df = pd.DataFrame(qwen + llama)\n",
    "        combined_csv = output_root / \"all_diaries.csv\"\n",
    "        df.to_csv(combined_csv, index=False)\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\"FINAL SUMMARY\")\n",
    "        print(f\"{'='*80}\")\n",
    "        print(f\"Qwen entries: {len(qwen)}\")\n",
    "        print(f\"Llama entries: {len(llama)}\")\n",
    "        print(f\"Total entries: {len(df)}\")\n",
    "        print(f\"Combined CSV: {combined_csv}\")\n",
    "        \n",
    "        # Statistics\n",
    "        non_empty = df['has_content'].sum()\n",
    "        print(f\"\\n Statistics:\")\n",
    "        print(f\"  Entries with content: {non_empty}/{len(df)} ({non_empty/len(df)*100:.1f}%)\")\n",
    "        print(f\"  Avg chars per entry: {df['clean_text'].str.len().mean():.1f}\")\n",
    "        \n",
    "        print(f\"\\n ALL DONE!\")\n",
    "    else:\n",
    "        print(\"\\n No data processed!\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 12. MAIN\n",
    "# =============================================================================\n",
    "if __name__ == \"__main__\":\n",
    "    run_full_pipeline(debug=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
