{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f72c9859-d531-4326-aff5-b3b964eb1c8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/M1HR/Desktop/MIGRAINE/code\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())\n",
    "os.chdir(\"/Users/M1HR/Desktop/MIGRAINE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "850e245a-e880-47fa-bc93-703b9d489e95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (2.9.1)\n",
      "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch) (3.6)\n",
      "Requirement already satisfied: jinja2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch) (2025.10.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from jinja2->torch) (3.0.3)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca630c3-0b17-42a9-a36a-e542636aa476",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install transformers[torch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be45607-3719-4b64-add5-547436958546",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install seqeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8ca528-ccb0-499a-a43d-65eb4566c5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import json\n",
    "import re\n",
    "import random\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForTokenClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForTokenClassification\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from seqeval.metrics import classification_report, f1_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# =============================================================================\n",
    "# 1. CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "CONFIG = {\n",
    "    'model_name': 'emilyalsentzer/Bio_ClinicalBERT',  # Pre-trained on clinical text\n",
    "    'max_length': 128,  # Increased from 64 for longer contexts\n",
    "    'batch_size': 8,    # Increased from 4 for better training\n",
    "    'learning_rate': 2e-5,\n",
    "    'num_epochs': 1,    # Increased from 1 for better learning\n",
    "    'output_dir': 'models/clinical_ner',\n",
    "    'data_dir': 'data/preprocessed',\n",
    "    'sample_ratio': 1.0  # Use all data (was 0.2)\n",
    "}\n",
    "\n",
    "\n",
    "ENTITY_LABELS = [\n",
    "    'O',  # Outside\n",
    "    \n",
    "    # Pain characteristics \n",
    "    'B-LOCATION', 'I-LOCATION',           # unilateral/bilateral\n",
    "    'B-CHARACTER', 'I-CHARACTER',         # throbbing/pressing\n",
    "    'B-INTENSITY', 'I-INTENSITY',         # mild/moderate/severe\n",
    "    \n",
    "    # Primary symptoms \n",
    "    'B-NAUSEA', 'I-NAUSEA',\n",
    "    'B-VOMIT', 'I-VOMIT',\n",
    "    'B-PHONOPHOBIA', 'I-PHONOPHOBIA',     # Sound sensitivity\n",
    "    'B-PHOTOPHOBIA', 'I-PHOTOPHOBIA',     # Light sensitivity\n",
    "    \n",
    "    # Visual symptoms \n",
    "    'B-VISUAL', 'I-VISUAL',               # Visual aura\n",
    "    'B-DIPLOPIA', 'I-DIPLOPIA',           # Double vision\n",
    "    'B-VISUAL_DEFECT', 'I-VISUAL_DEFECT', # Visual field defect\n",
    "    \n",
    "    # Sensory symptoms \n",
    "    'B-SENSORY', 'I-SENSORY',\n",
    "    'B-PARESTHESIA', 'I-PARESTHESIA',     # Bilateral tingling\n",
    "    \n",
    "    # Speech/language \n",
    "    'B-DYSPHASIA', 'I-DYSPHASIA',         # Word-finding difficulty\n",
    "    'B-DYSARTHRIA', 'I-DYSARTHRIA',       # Slurred speech\n",
    "    \n",
    "    # Balance/coordination \n",
    "    'B-VERTIGO', 'I-VERTIGO',             # Dizziness\n",
    "    'B-ATAXIA', 'I-ATAXIA',               # Coordination problems\n",
    "    \n",
    "    # Auditory \n",
    "    'B-TINNITUS', 'I-TINNITUS',           # Ringing in ears\n",
    "    'B-HYPOACUSIS', 'I-HYPOACUSIS',       # Hearing loss\n",
    "    \n",
    "    # Consciousness \n",
    "    'B-CONSCIENCE', 'I-CONSCIENCE',       # Altered consciousness\n",
    "    \n",
    "    # Temporal/frequency \n",
    "    'B-DURATION', 'I-DURATION',           # How long\n",
    "    'B-FREQUENCY', 'I-FREQUENCY',         # How often\n",
    "    \n",
    "    # Family history \n",
    "    'B-DPF', 'I-DPF',                     # Family history\n",
    "]\n",
    "\n",
    "# Create label mappings\n",
    "# Convert labels to numbers for model training\n",
    "label2id = {label: i for i, label in enumerate(ENTITY_LABELS)}\n",
    "id2label = {i: label for label, i in label2id.items()}\n",
    "\n",
    "# =============================================================================\n",
    "# 2. COMPREHENSIVE ENTITY PATTERNS\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "# Negation words to detect absence of symptoms\n",
    "NEGATION_WORDS = [\n",
    "    'no', 'not', 'none', 'never', 'without',\n",
    "    \"didn't\", \"didnt\", \"don't\", \"dont\", \"wasn't\", \"wasnt\",\n",
    "    \"weren't\", \"werent\", \"isn't\", \"isnt\", \"aren't\", \"arent\",\n",
    "    'absent', 'free', 'clear'\n",
    "]\n",
    "\n",
    "ENTITY_PATTERNS = {\n",
    "    # ==========================================================================\n",
    "    # LOCATION (Unilateral vs Bilateral)\n",
    "    # ==========================================================================\n",
    "    'LOCATION': [\n",
    "        # Unilateral patterns\n",
    "        r'\\b(left|right)\\s+(side|temple|hemisphere|forehead|half|eye)\\b',\n",
    "        r'\\b(one|single)\\s+side\\b',\n",
    "        r'\\bhemicranial\\b',\n",
    "        r'\\btemporal\\b',\n",
    "        r'\\bunilateral\\b',\n",
    "        r'\\bbehind\\s+(left|right)\\s+eye\\b',\n",
    "        r'\\b(left|right)\\s+half\\s+of\\s+head\\b',\n",
    "        \n",
    "        # Bilateral patterns\n",
    "        r'\\bboth\\s+(sides|temples)\\b',\n",
    "        r'\\b(whole|entire)\\s+head\\b',\n",
    "        r'\\ball\\s+over\\b',\n",
    "        r'\\beverywhere\\b',\n",
    "        r'\\bbilateral\\b',\n",
    "        r'\\bgeneralized\\b',\n",
    "        r'\\bdiffuse\\b',\n",
    "    ],\n",
    "    \n",
    "    # ==========================================================================\n",
    "    # CHARACTER (Throbbing vs Pressing)\n",
    "    # ==========================================================================\n",
    "    'CHARACTER': [\n",
    "        # Throbbing/pulsating patterns\n",
    "        r'\\b(throbbing|pulsating|pulsing|pounding)\\b',\n",
    "        r'\\b(beating|hammering|drumming|banging|thumping)\\b',\n",
    "        r'\\bpulse[-\\s]?like\\b',\n",
    "        r'\\brhythmic\\b',\n",
    "        r'\\blike\\s+a\\s+heartbeat\\b',\n",
    "        r'\\blike\\s+drumming\\b',\n",
    "        \n",
    "        # Pressing/constant patterns\n",
    "        r'\\b(pressing|squeezing|tight|tightness)\\b',\n",
    "        r'\\b(pressure|vice[-\\s]?like|band[-\\s]?like)\\b',\n",
    "        r'\\b(constricting|crushing|heavy|weighted)\\b',\n",
    "        r'\\bdull\\s+ache\\b',\n",
    "        r'\\bsteady\\s+ache\\b',\n",
    "        r'\\b(constant|continuous|persistent)\\b',\n",
    "    ],\n",
    "    \n",
    "    # ==========================================================================\n",
    "    # INTENSITY (Mild, Moderate, Severe)\n",
    "    # ==========================================================================\n",
    "    'INTENSITY': [\n",
    "        # Pain scale ratings \n",
    "        r'\\b[1-9]\\s*/\\s*10\\b',  # 1/10 to 9/10 (must have /10)\n",
    "        r'\\b10\\s*/\\s*10\\b',     # 10/10\n",
    "        r'\\b\\d+\\s+out\\s+of\\s+10\\b',  # X out of 10\n",
    "        \n",
    "        # Mild \n",
    "        r'\\b(mild|minor|slight)\\b',\n",
    "        r'\\blight(?!\\s+sensitivity)\\b',  # 'light' but NOT 'light sensitivity'\n",
    "        r'\\b(low|weak|bearable|tolerable)\\b',\n",
    "        r'\\b(manageable|annoying|bothersome)\\b',\n",
    "        \n",
    "        # Moderate\n",
    "        r'\\b(moderate|medium|average)\\b',\n",
    "        r'\\b(noticeable|significant)\\b',\n",
    "        r'\\b(uncomfortable|distressing)\\b',\n",
    "        \n",
    "        # Severe\n",
    "        r'\\b(severe|terrible|awful|horrible)\\b',\n",
    "        r'\\b(intense|extreme|unbearable|excruciating)\\b',\n",
    "        r'\\b(worst\\s+ever|agonizing|debilitating)\\b',\n",
    "        r'\\b(really\\s+)?bad\\b',\n",
    "        r'\\bvery\\s+bad\\b',\n",
    "        r'\\bsuper\\s+bad\\b',\n",
    "        r'\\bcouldn\\'?t\\s+function\\b',\n",
    "        r'\\bincapacitating\\b',\n",
    "    ],\n",
    "    \n",
    "    # ==========================================================================\n",
    "    # NAUSEA\n",
    "    # ==========================================================================\n",
    "    'NAUSEA': [\n",
    "        r'\\bnausea\\b',\n",
    "        r'\\bnauseous\\b',\n",
    "        r'\\bqueasy\\b',\n",
    "        r'\\b(sick|feeling\\s+sick|feel\\s+sick)\\b',\n",
    "        r'\\bsick\\s+to\\s+(my\\s+)?stomach\\b',\n",
    "        r'\\bnauseated\\b',\n",
    "        r'\\bupset\\s+stomach\\b',\n",
    "        r'\\bwant(ed)?\\s+to\\s+throw\\s+up\\b',\n",
    "        r'\\bfeel\\s+like\\s+throwing\\s+up\\b',\n",
    "        r'\\bgonna\\s+be\\s+sick\\b',\n",
    "    ],\n",
    "    \n",
    "    # ==========================================================================\n",
    "    # VOMITING\n",
    "    # ==========================================================================\n",
    "    'VOMIT': [\n",
    "        r'\\bvomit(ing|ed)?\\b',\n",
    "        r'\\bthrew\\s+up\\b',\n",
    "        r'\\bthrow\\s+up\\b',\n",
    "        r'\\bpuk(ed|ing)\\b',\n",
    "        r'\\bbeing\\s+sick\\b',\n",
    "        r'\\bgot\\s+sick\\b',\n",
    "        r'\\bretching\\b',\n",
    "        r'\\bheaving\\b',\n",
    "    ],\n",
    "    \n",
    "    # ==========================================================================\n",
    "    # PHOTOPHOBIA (Light sensitivity)\n",
    "    # ==========================================================================\n",
    "    'PHOTOPHOBIA': [\n",
    "        r'\\bphotophobia\\b',\n",
    "        r'\\blight\\s+sensitivity\\b',\n",
    "        r'\\bsensitive\\s+to\\s+light\\b',\n",
    "        r'\\b(lights?|bright\\s+lights?)\\s+hurt\\b',\n",
    "        r'\\bcan\\'?t\\s+stand\\s+(the\\s+)?light\\b',\n",
    "        r'\\beverything\\s+(too\\s+)?bright\\b',\n",
    "        r'\\b(too\\s+)?bright\\b',\n",
    "        r'\\b(had\\s+to\\s+)?close\\s+(the\\s+)?(curtains|blinds)\\b',\n",
    "        r'\\bwore\\s+sunglasses\\s+inside\\b',\n",
    "        r'\\beyes\\s+hurt\\s+from\\s+light\\b',\n",
    "    ],\n",
    "    \n",
    "    # ==========================================================================\n",
    "    # PHONOPHOBIA (Sound sensitivity)\n",
    "    # ==========================================================================\n",
    "    'PHONOPHOBIA': [\n",
    "        r'\\bphonophobia\\b',\n",
    "        r'\\b(sound|noise)\\s+sensitivity\\b',\n",
    "        r'\\bsensitive\\s+to\\s+(sound|noise)\\b',\n",
    "        r'\\b(sounds?|noise)\\s+hurt\\b',\n",
    "        r'\\bcan\\'?t\\s+stand\\s+(the\\s+)?(sound|noise)\\b',\n",
    "        r'\\beverything\\s+(too\\s+)?loud\\b',\n",
    "        r'\\b(sounds?|noise)\\s+(too\\s+)?loud\\b',\n",
    "        r'\\b(cover|plug)(ed)?\\s+(my\\s+)?ears\\b',\n",
    "        r'\\b(had\\s+to\\s+)?plug\\s+ears\\b',\n",
    "    ],\n",
    "    \n",
    "    # ==========================================================================\n",
    "    # VISUAL AURA\n",
    "    # ==========================================================================\n",
    "    'VISUAL': [\n",
    "        r'\\bvisual\\s+aura\\b',\n",
    "        r'\\bzigzag\\s+lines?\\b',\n",
    "        r'\\bzig[-\\s]?zag\\s+lines?\\b',\n",
    "        r'\\bjagged\\s+lines?\\b',\n",
    "        r'\\bflashing\\s+(lights?)?\\b',\n",
    "        r'\\bsparkles?\\b',\n",
    "        r'\\bblind\\s+spots?\\b',\n",
    "        r'\\bscotoma\\b',\n",
    "        r'\\bblurr?y\\s+vision\\b',\n",
    "        r'\\bshimmer(ing)?\\b',\n",
    "        r'\\bspots\\b',\n",
    "        r'\\bhalos?\\b',\n",
    "        r'\\bfortification\\s+spectra\\b',\n",
    "        r'\\bscintillat(ions|ing)\\b',\n",
    "        r'\\bwavy\\s+lines?\\b',\n",
    "        r'\\bgeometric\\s+patterns?\\b',\n",
    "        r'\\bvisual\\s+disturb(ances|ing)\\b',\n",
    "        r'\\bsaw\\s+(zigzag|lights?|sparkles?|flashing)\\b',\n",
    "    ],\n",
    "    \n",
    "    # ==========================================================================\n",
    "    # DIPLOPIA (Double vision)\n",
    "    # ==========================================================================\n",
    "    'DIPLOPIA': [\n",
    "        r'\\bdiplopia\\b',\n",
    "        r'\\bdouble\\s+vision\\b',\n",
    "        r'\\bseeing\\s+double\\b',\n",
    "        r'\\btwo\\s+of\\s+everything\\b',\n",
    "        r'\\bblurr?y\\s+double\\b',\n",
    "        r'\\bsplit\\s+vision\\b',\n",
    "        r'\\beyes\\s+not\\s+working\\s+together\\b',\n",
    "    ],\n",
    "    \n",
    "    # ==========================================================================\n",
    "    # VISUAL DEFECT\n",
    "    # ==========================================================================\n",
    "    'VISUAL_DEFECT': [\n",
    "        r'\\bvisual\\s+field\\s+defect\\b',\n",
    "        r'\\bfrontal\\s+(and\\s+)?nasal\\s+defect\\b',\n",
    "        r'\\bfield\\s+defect\\b',\n",
    "        r'\\btunnel\\s+vision\\b',\n",
    "        r'\\bperipheral\\s+vision\\s+(gone|loss)\\b',\n",
    "        r'\\bpart\\s+of\\s+vision\\s+missing\\b',\n",
    "    ],\n",
    "    \n",
    "    # ==========================================================================\n",
    "    # SENSORY (Tingling, numbness)\n",
    "    # ==========================================================================\n",
    "    'SENSORY': [\n",
    "        r'\\bsensory\\s+(aura|symptoms?)\\b',\n",
    "        r'\\btingling\\b',\n",
    "        r'\\bnumbness\\b',\n",
    "        r'\\bpins\\s+and\\s+needles\\b',\n",
    "        r'\\bprickling\\b',\n",
    "        r'\\bgoing\\s+numb\\b',\n",
    "        r'\\b(arm|hand|leg|foot)\\s+(went\\s+)?numb\\b',\n",
    "        r'\\bnumbness\\s+spreading\\b',\n",
    "        r'\\btingling\\s+spreading\\b',\n",
    "        r'\\bparesthesia\\b',\n",
    "        r'\\bweird\\s+sensation\\b',\n",
    "        r'\\bloss\\s+of\\s+feeling\\b',\n",
    "    ],\n",
    "    \n",
    "    # ==========================================================================\n",
    "    # PARESTHESIA (Bilateral)\n",
    "    # ==========================================================================\n",
    "    'PARESTHESIA': [\n",
    "        r'\\bbilateral\\s+paresthesia\\b',\n",
    "        r'\\bboth\\s+sides\\s+(tingling|numb)\\b',\n",
    "        r'\\btingling\\s+on\\s+both\\s+sides\\b',\n",
    "        r'\\bnumbness\\s+both\\s+sides\\b',\n",
    "        r'\\bpins\\s+and\\s+needles\\s+both\\s+sides\\b',\n",
    "        r'\\bsimultaneous\\s+(tingling|numbness)\\b',\n",
    "        r'\\bboth\\s+(arms|hands)\\s+(tingling|numb)\\b',\n",
    "    ],\n",
    "    \n",
    "    # ==========================================================================\n",
    "    # DYSPHASIA (Word-finding difficulty)\n",
    "    # ==========================================================================\n",
    "    'DYSPHASIA': [\n",
    "        r'\\bdysphasia\\b',\n",
    "        r'\\btrouble\\s+finding\\s+words\\b',\n",
    "        r'\\bcan\\'?t\\s+find\\s+words\\b',\n",
    "        r'\\bword[-\\s]?finding\\s+difficult(y|ies)\\b',\n",
    "        r'\\bdifficult(y|ies)\\s+with\\s+words\\b',\n",
    "        r'\\bcouldn\\'?t\\s+speak\\s+properly\\b',\n",
    "        r'\\bspeech\\s+problems?\\b',\n",
    "        r'\\bstruggling\\s+to\\s+talk\\b',\n",
    "        r'\\bcouldn\\'?t\\s+get\\s+words\\s+out\\b',\n",
    "        r'\\bwords\\s+wouldn\\'?t\\s+come\\b',\n",
    "    ],\n",
    "    \n",
    "    # ==========================================================================\n",
    "    # DYSARTHRIA (Slurred speech)\n",
    "    # ==========================================================================\n",
    "    'DYSARTHRIA': [\n",
    "        r'\\bdysarthria\\b',\n",
    "        r'\\bslurr(ed|ing)\\s+(speech|words)\\b',\n",
    "        r'\\bdifficult(y|ies)\\s+speaking\\b',\n",
    "        r'\\bcan\\'?t\\s+speak\\s+clearly\\b',\n",
    "        r'\\bmumbling\\b',\n",
    "        r'\\bgarbled\\s+speech\\b',\n",
    "        r'\\btongue\\s+felt\\s+thick\\b',\n",
    "        r'\\bmouth\\s+didn\\'?t\\s+work\\s+right\\b',\n",
    "        r'\\bspeech\\s+sounded\\s+drunk\\b',\n",
    "    ],\n",
    "    \n",
    "    # ==========================================================================\n",
    "    # VERTIGO (Dizziness)\n",
    "    # ==========================================================================\n",
    "    'VERTIGO': [\n",
    "        r'\\bvertigo\\b',\n",
    "        r'\\bdizz(y|iness)\\b',\n",
    "        r'\\broom\\s+spinning\\b',\n",
    "        r'\\bspinning\\b',\n",
    "        r'\\beverything\\s+spinning\\b',\n",
    "        r'\\blightheaded\\b',\n",
    "        r'\\blight[-\\s]?headed\\b',\n",
    "        r'\\boff\\s+balance\\b',\n",
    "        r'\\blost\\s+balance\\b',\n",
    "        r'\\bfelt\\s+like\\s+falling\\b',\n",
    "        r'\\bunsteady\\b',\n",
    "        r'\\bwobbly\\b',\n",
    "    ],\n",
    "    \n",
    "    # ==========================================================================\n",
    "    # ATAXIA (Coordination problems)\n",
    "    # ==========================================================================\n",
    "    'ATAXIA': [\n",
    "        r'\\bataxia\\b',\n",
    "        r'\\bcoordination\\s+problems?\\b',\n",
    "        r'\\black\\s+of\\s+coordination\\b',\n",
    "        r'\\bclumsy\\b',\n",
    "        r'\\buncoordinated\\b',\n",
    "        r'\\bbalance\\s+issues?\\b',\n",
    "        r'\\bcan\\'?t\\s+walk\\s+straight\\b',\n",
    "        r'\\bstumbling\\b',\n",
    "        r'\\btripping\\b',\n",
    "        r'\\bmovements?\\s+jerky\\b',\n",
    "        r'\\bmuscle\\s+control\\s+problems?\\b',\n",
    "    ],\n",
    "    \n",
    "    # ==========================================================================\n",
    "    # TINNITUS (Ringing in ears)\n",
    "    # ==========================================================================\n",
    "    'TINNITUS': [\n",
    "        r'\\btinnitus\\b',\n",
    "        r'\\bringing(ing)?\\s+in\\s+(the\\s+)?ears?\\b',\n",
    "        r'\\bears?\\s+ringing\\b',\n",
    "        r'\\bbuzzing\\s+in\\s+ears?\\b',\n",
    "        r'\\bhigh[-\\s]?pitched\\s+sound\\b',\n",
    "        r'\\bwhistling\\s+in\\s+ears?\\b',\n",
    "        r'\\bhumming\\s+in\\s+ears?\\b',\n",
    "        r'\\bnoise\\s+in\\s+ears?\\b',\n",
    "    ],\n",
    "    \n",
    "    # ==========================================================================\n",
    "    # HYPOACUSIS (Hearing loss)\n",
    "    # ==========================================================================\n",
    "    'HYPOACUSIS': [\n",
    "        r'\\bhypoacusis\\b',\n",
    "        r'\\bhearing\\s+loss\\b',\n",
    "        r'\\bcan\\'?t\\s+hear(\\s+well)?\\b',\n",
    "        r'\\bmuffled\\s+hearing\\b',\n",
    "        r'\\bsounds?\\s+muffled\\b',\n",
    "        r'\\bhard\\s+to\\s+hear\\b',\n",
    "        r'\\bears?\\s+felt\\s+(blocked|full)\\b',\n",
    "        r'\\blike\\s+underwater\\b',\n",
    "        r'\\bhearing\\s+underwater\\b',\n",
    "    ],\n",
    "    \n",
    "    # ==========================================================================\n",
    "    # CONSCIENCE (Altered consciousness)\n",
    "    # ==========================================================================\n",
    "    'CONSCIENCE': [\n",
    "        r'\\baltered\\s+consciousness\\b',\n",
    "        r'\\bconfused\\b',\n",
    "        r'\\bconfusion\\b',\n",
    "        r'\\bdisoriented\\b',\n",
    "        r'\\bout\\s+of\\s+it\\b',\n",
    "        r'\\bnot\\s+with\\s+it\\b',\n",
    "        r'\\bspaced\\s+out\\b',\n",
    "        r'\\b(brain\\s+)?fog(gy)?\\b',\n",
    "        r'\\bmental\\s+fog\\b',\n",
    "        r'\\bcan\\'?t\\s+think\\s+(straight|clearly)\\b',\n",
    "        r'\\bdazed\\b',\n",
    "        r'\\bin\\s+a\\s+daze\\b',\n",
    "        r'\\bfelt\\s+(weird|strange)\\b',\n",
    "        r'\\b(disconnected|detached)\\b',\n",
    "        r'\\bjeopardized\\s+conscience\\b',\n",
    "    ],\n",
    "    \n",
    "    # ==========================================================================\n",
    "    # DURATION (How long episodes last)\n",
    "    # ==========================================================================\n",
    "    'DURATION': [\n",
    "        r'\\blasted\\s+\\d+\\s+(hour|hr|day)s?\\b',\n",
    "        r'\\bduration\\s*:?\\s*\\d+\\s+(hour|hr|day)s?\\b',\n",
    "        r'\\b\\d+\\s+(hour|hr|day)s?\\s+(long|episode)\\b',\n",
    "        r'\\ball\\s+day\\b',\n",
    "        r'\\bentire\\s+day\\b',\n",
    "        r'\\bfor\\s+\\d+\\s+(hour|hr|day)s?\\b',\n",
    "        r'\\babout\\s+\\d+\\s+(hour|hr|day)s?\\b',\n",
    "        r'\\b(been|being)\\s+(like\\s+this\\s+)?for\\s+\\d+\\s+(hour|hr|day)s?\\b',  #\"been like this for 3 hours\"\n",
    "        r'\\bgoing\\s+on\\s+\\d+\\s+(hour|hr|day)s?\\b',  # \"going on 3 hours\"\n",
    "        r'\\b\\d+\\s+(hour|hr|day)s?\\s+now\\b',  # \"3 hours now\"\n",
    "    ],\n",
    "    \n",
    "    # ==========================================================================\n",
    "    # FREQUENCY (How often episodes occur)\n",
    "    # ==========================================================================\n",
    "    'FREQUENCY': [\n",
    "        r'\\b\\d+\\s+(times?|episodes?)\\s+(per|a)\\s+(month|week)\\b',\n",
    "        r'\\bfrequency\\s*:?\\s*\\d+\\b',\n",
    "        r'\\bget\\s+them\\s+\\d+\\s+times?\\b',\n",
    "        r'\\b\\d+x\\s+(per|a)\\s+(month|week)\\b',\n",
    "        r'\\bevery\\s+(few\\s+)?(day|week)s?\\b',\n",
    "    ],\n",
    "    \n",
    "    # ==========================================================================\n",
    "    # DPF (Family history)\n",
    "    # ==========================================================================\n",
    "    'DPF': [\n",
    "        r'\\bfamily\\s+history\\b',\n",
    "        r'\\bruns\\s+in\\s+(the\\s+)?family\\b',\n",
    "        r'\\b(mom|mother|dad|father|parent)s?\\s+(gets?|has|had)\\s+(migraines?|headaches?|them)\\b',\n",
    "        r'\\bfamilial\\b',\n",
    "        r'\\bgenetic\\b',\n",
    "    ],\n",
    "}\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 3. NEGATION-AWARE ENTITY EXTRACTION\n",
    "# =============================================================================\n",
    "\n",
    "def is_negated(text, match_start, match_end):\n",
    "    \n",
    "    # Get text before the entity (up to 30 characters)\n",
    "    context_window = text[max(0, match_start - 30):match_start].lower()\n",
    "    \n",
    "    # Check for negation words\n",
    "    for neg_word in NEGATION_WORDS:\n",
    "        # Look for negation word followed by optional words then the entity up to 5 words apart\n",
    "        pattern = rf'\\b{neg_word}\\b.{{0,30}}'  # Within 30 characters\n",
    "        if re.search(pattern, context_window):\n",
    "            return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "\n",
    "def extract_entities_from_text(text):\n",
    "\n",
    "    text_lower = text.lower()\n",
    "    tokens = text.split()  # Keep original case for tokens\n",
    "    labels = ['O'] * len(tokens)  # Initialize all as Outside\n",
    "    \n",
    "    # Track token positions in original text\n",
    "    token_positions = []\n",
    "    current_pos = 0\n",
    "    for token in tokens:\n",
    "        start = text_lower.find(token.lower(), current_pos)\n",
    "        end = start + len(token)\n",
    "        token_positions.append((start, end))\n",
    "        current_pos = end\n",
    "    \n",
    "    # For each entity type, find matches and label tokens\n",
    "    for entity_type, patterns in ENTITY_PATTERNS.items():\n",
    "        for pattern in patterns:\n",
    "            for match in re.finditer(pattern, text_lower, re.IGNORECASE):\n",
    "                match_start = match.start()\n",
    "                match_end = match.end()\n",
    "                \n",
    "                # CHECK FOR NEGATION\n",
    "                if is_negated(text_lower, match_start, match_end):\n",
    "                    continue\n",
    "                \n",
    "                # Find which tokens this match covers\n",
    "                is_first = True\n",
    "                for i, (tok_start, tok_end) in enumerate(token_positions):\n",
    "                    # Token overlaps with match\n",
    "                    if tok_start < match_end and tok_end > match_start:\n",
    "                        # Only label if not already labeled\n",
    "                        if labels[i] == 'O':\n",
    "                            if is_first:\n",
    "                                labels[i] = f'B-{entity_type}'\n",
    "                                is_first = False\n",
    "                            else:\n",
    "                                labels[i] = f'I-{entity_type}'\n",
    "    \n",
    "    return list(zip(tokens, labels))\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 4. DATASET CLASS\n",
    "# =============================================================================\n",
    "\n",
    "class NERDataset(Dataset):\n",
    "\n",
    "    \n",
    "    def __init__(self, texts, tokenizer, max_length=128):\n",
    "        self.texts = texts\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        \n",
    "        # Extract entities using patterns \n",
    "        token_label_pairs = extract_entities_from_text(text)\n",
    "        \n",
    "        if not token_label_pairs:\n",
    "            # Empty text, return dummy example\n",
    "            return {\n",
    "                'input_ids': torch.zeros(self.max_length, dtype=torch.long),\n",
    "                'attention_mask': torch.zeros(self.max_length, dtype=torch.long),\n",
    "                'labels': torch.full((self.max_length,), -100, dtype=torch.long),\n",
    "            }\n",
    "        \n",
    "        tokens, labels = zip(*token_label_pairs)\n",
    "        \n",
    "        # Tokenize with alignment\n",
    "        encoding = self.tokenizer(\n",
    "            list(tokens),\n",
    "            is_split_into_words=True,  # We already split into words\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # Align labels with subword tokens\n",
    "        word_ids = encoding.word_ids(batch_index=0)\n",
    "        aligned_labels = []\n",
    "        \n",
    "        previous_word_idx = None\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                # Special tokens ([CLS], [SEP], [PAD])\n",
    "                aligned_labels.append(-100)  # -100 = ignore in loss\n",
    "            elif word_idx != previous_word_idx:\n",
    "                # First subword of a word, use original label\n",
    "                aligned_labels.append(label2id[labels[word_idx]])\n",
    "            else:\n",
    "                # Continuation subword \n",
    "                label = labels[word_idx]\n",
    "                if label.startswith('B-'):\n",
    "                    label = label.replace('B-', 'I-')\n",
    "                aligned_labels.append(label2id[label])\n",
    "            previous_word_idx = word_idx\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(0),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "            'labels': torch.tensor(aligned_labels, dtype=torch.long),\n",
    "        }\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 5. LOAD DATA\n",
    "# =============================================================================\n",
    "\n",
    "def load_preprocessed_data(data_dir, sample_ratio=1.0):\n",
    "    \n",
    "    data_dir = Path(data_dir)\n",
    "    all_texts = []\n",
    "    \n",
    "    # Load from nested structure\n",
    "    for model_folder in ['llama3', 'qwen']:\n",
    "        json_file = data_dir / f'{model_folder}.json'\n",
    "        \n",
    "        if not json_file.exists():\n",
    "            print(f\" File not found: {json_file}\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"Loading {json_file}...\")\n",
    "        \n",
    "        with open(json_file, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        # Extract clean texts\n",
    "        for entry in data:\n",
    "            text = entry.get('clean_text', '').strip()\n",
    "            all_texts.append(text)\n",
    "        \n",
    "        print(f\"  ✓ Loaded {len(data)} entries\")\n",
    "    \n",
    "    # Sample data if requested\n",
    "    original_count = len(all_texts)\n",
    "    \n",
    "    if sample_ratio < 1.0:\n",
    "        random.seed(42)  # For reproducibility\n",
    "        sample_size = int(len(all_texts) * sample_ratio)\n",
    "        all_texts = random.sample(all_texts, sample_size)\n",
    "        print(f\"\\nSampled {len(all_texts)}/{original_count} texts ({sample_ratio*100:.0f}%)\")\n",
    "    else:\n",
    "        print(f\"\\n✓ Total texts: {len(all_texts)}\")\n",
    "    \n",
    "    return all_texts\n",
    "\n",
    "def extract_patient_id(filename):\n",
    "    match = re.search(r'patient[_\\s]*(\\d+)', str(filename), re.IGNORECASE)\n",
    "    if match:\n",
    "        return int(match.group(1)) \n",
    "    return 0\n",
    "# =============================================================================\n",
    "# 6. TRAINING\n",
    "# =============================================================================\n",
    "\n",
    "def train_ner_model():\n",
    "  \n",
    "    print(\"=\"*80)\n",
    "    print(\"ENHANCED MEDICAL DIARY NER TRAINING\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"\\nNEW FEATURES:\")\n",
    "    print(\"  Comprehensive vocabulary patterns\")\n",
    "    print(\"  Negation detection (no, not, didn't)\")\n",
    "    print(\"  Better BIO tagging\")\n",
    "    print(\"  More training epochs\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # 1. Load data\n",
    "    print(\"\\n1. Loading data...\")\n",
    "    texts = load_preprocessed_data(\n",
    "        CONFIG['data_dir'],\n",
    "        sample_ratio=CONFIG.get('sample_ratio', 1.0)\n",
    "    )\n",
    "    \n",
    " \n",
    "    # 2. Split data (80% train, 20% validation)\n",
    "    print(\"\\n2. Splitting data...\")\n",
    "    train_texts, val_texts = train_test_split(texts, test_size=0.2, random_state=42)\n",
    "    print(f\"  Train: {len(train_texts)}\")\n",
    "    print(f\"  Val: {len(val_texts)}\")\n",
    "    \n",
    "    # 3. Load tokenizer and model\n",
    "    print(\"\\n3. Loading ClinicalBERT...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(CONFIG['model_name'])\n",
    "    model = AutoModelForTokenClassification.from_pretrained(\n",
    "        CONFIG['model_name'],\n",
    "        num_labels=len(ENTITY_LABELS),\n",
    "        id2label=id2label,\n",
    "        label2id=label2id\n",
    "    )\n",
    "    print(f\"  Model loaded: {CONFIG['model_name']}\")\n",
    "    print(f\"  Entity labels: {len(ENTITY_LABELS)}\")\n",
    "    \n",
    "    # 4. Create datasets\n",
    "    print(\"\\n4. Creating datasets...\")\n",
    "    train_dataset = NERDataset(train_texts, tokenizer, CONFIG['max_length'])\n",
    "    val_dataset = NERDataset(val_texts, tokenizer, CONFIG['max_length'])\n",
    "    print(f\"  Train dataset: {len(train_dataset)} samples\")\n",
    "    print(f\"  Val dataset: {len(val_dataset)} samples\")\n",
    "    \n",
    "    # 5. Training arguments\n",
    "    print(\"\\n5. Setting up training...\")\n",
    "    \n",
    "    # Clean output directory before training\n",
    "    import shutil\n",
    "    output_dir = Path(CONFIG['output_dir'])\n",
    "    if output_dir.exists():\n",
    "        print(f\"  Cleaning old training artifacts from {output_dir}...\")\n",
    "        shutil.rmtree(output_dir)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=str(output_dir),\n",
    "        num_train_epochs=CONFIG['num_epochs'],\n",
    "        per_device_train_batch_size=CONFIG['batch_size'],\n",
    "        per_device_eval_batch_size=CONFIG['batch_size'],\n",
    "        learning_rate=CONFIG['learning_rate'],\n",
    "        weight_decay=0.01,\n",
    "        eval_strategy='epoch',\n",
    "        save_strategy='epoch',  \n",
    "        save_total_limit=2,     \n",
    "        load_best_model_at_end=True,\n",
    "        push_to_hub=False,\n",
    "        logging_steps=10,\n",
    "        logging_dir=f\"{CONFIG['output_dir']}/logs\",\n",
    "    )\n",
    "    \n",
    "    # Data collator (handles batching)\n",
    "    data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "    \n",
    "    # 6. Create trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        data_collator=data_collator,\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "    \n",
    "    # 7. Train!\n",
    "    print(\"\\n6. Training...\")\n",
    "    print(\"=\"*80)\n",
    "    trainer.train()\n",
    "    \n",
    "    # 8. Save final model\n",
    "    print(\"\\n7. Saving final model...\")\n",
    "    final_model_path = Path(CONFIG['output_dir']) / 'final_model'\n",
    "    \n",
    "    # Remove old final_model if exists\n",
    "    if final_model_path.exists():\n",
    "        shutil.rmtree(final_model_path)\n",
    "    \n",
    "    # Save model and tokenizer\n",
    "    trainer.save_model(final_model_path)\n",
    "    tokenizer.save_pretrained(final_model_path)\n",
    "    \n",
    "    print(f\"  ✓ Model saved to {final_model_path}\")\n",
    "    \n",
    "    # Clean up checkpoints (optional - keep only final model)\n",
    "    print(\"\\n8. Cleaning up checkpoints...\")\n",
    "    for checkpoint_dir in output_dir.glob('checkpoint-*'):\n",
    "        print(f\"  Removing {checkpoint_dir.name}...\")\n",
    "        shutil.rmtree(checkpoint_dir)\n",
    "    \n",
    "    print(f\"\\n  ✓ Final directory structure:\")\n",
    "    print(f\"     {output_dir}/\")\n",
    "    print(f\"     └── final_model/  ← Your trained model is here!\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"TRAINING COMPLETE!\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    return trainer, tokenizer\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 7. INFERENCE / ENTITY EXTRACTION\n",
    "# =============================================================================\n",
    "\n",
    "def extract_entities(text, model, tokenizer, device='cpu'):\n",
    "\n",
    "    # Tokenize\n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        return_tensors='pt',\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=128\n",
    "    ).to(device)\n",
    "    \n",
    "    # Predict\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "    \n",
    "    # Convert predictions to labels\n",
    "    tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
    "    predicted_labels = [id2label[p.item()] for p in predictions[0]]\n",
    "    \n",
    "    # Extract entities\n",
    "    entities = []\n",
    "    current_entity = []\n",
    "    current_label = None\n",
    "    \n",
    "    for token, label in zip(tokens, predicted_labels):\n",
    "        if token in ['[CLS]', '[SEP]', '[PAD]']:\n",
    "            continue\n",
    "        \n",
    "        if label.startswith('B-'):\n",
    "            # Save previous entity\n",
    "            if current_entity:\n",
    "                entities.append({\n",
    "                    'text': tokenizer.convert_tokens_to_string(current_entity),\n",
    "                    'label': current_label\n",
    "                })\n",
    "            # Start new entity\n",
    "            current_entity = [token]\n",
    "            current_label = label[2:]  # Remove 'B-'\n",
    "        \n",
    "        elif label.startswith('I-') and current_label == label[2:]:\n",
    "            # Continue current entity\n",
    "            current_entity.append(token)\n",
    "        \n",
    "        else:\n",
    "            # End current entity\n",
    "            if current_entity:\n",
    "                entities.append({\n",
    "                    'text': tokenizer.convert_tokens_to_string(current_entity),\n",
    "                    'label': current_label\n",
    "                })\n",
    "            current_entity = []\n",
    "            current_label = None\n",
    "    \n",
    "    # Add last entity\n",
    "    if current_entity:\n",
    "        entities.append({\n",
    "            'text': tokenizer.convert_tokens_to_string(current_entity),\n",
    "            'label': current_label\n",
    "        })\n",
    "    \n",
    "    return entities\n",
    "\n",
    "\n",
    "def extract_entities_structured(text, model, tokenizer, device='cpu'):\n",
    "\n",
    "    \n",
    "    # Get raw entities\n",
    "    entities = extract_entities(text, model, tokenizer, device)\n",
    "    \n",
    "    # Initialize all entity fields\n",
    "    structured = {\n",
    "        # Pain characteristics\n",
    "        'location': None,\n",
    "        'character': None,\n",
    "        'intensity': None,\n",
    "        \n",
    "        # Primary symptoms\n",
    "        'nausea': 'Not found',\n",
    "        'vomit': 'Not found',\n",
    "        'phonophobia': 'Not found',\n",
    "        'photophobia': 'Not found',\n",
    "        \n",
    "        # Visual\n",
    "        'visual': None,\n",
    "        'diplopia': 'Not found',\n",
    "        'visual_defect': 'Not found',\n",
    "        \n",
    "        # Sensory\n",
    "        'sensory': None,\n",
    "        'paresthesia': 'Not found',\n",
    "        \n",
    "        # Speech\n",
    "        'dysphasia': 'Not found',\n",
    "        'dysarthria': 'Not found',\n",
    "        \n",
    "        # Balance\n",
    "        'vertigo': 'Not found',\n",
    "        'ataxia': 'Not found',\n",
    "        \n",
    "        # Auditory\n",
    "        'tinnitus': 'Not found',\n",
    "        'hypoacusis': 'Not found',\n",
    "        \n",
    "        # Consciousness\n",
    "        'conscience': 'Not found',\n",
    "        \n",
    "        # Temporal\n",
    "        'duration': None,\n",
    "        'frequency': None,\n",
    "        \n",
    "        # Family\n",
    "        'dpf': 'Not found',\n",
    "    }\n",
    "    \n",
    "    # Fill in found entities\n",
    "    for ent in entities:\n",
    "        label = ent['label'].lower()\n",
    "        text_val = ent['text'].strip()\n",
    "        \n",
    "        if label in structured:\n",
    "            structured[label] = text_val\n",
    "    \n",
    "    return structured\n",
    "\n",
    "\n",
    "def format_entity_output(structured_entities):\n",
    "    \n",
    "    output = []\n",
    "    \n",
    "    output.append(\"=== PAIN CHARACTERISTICS ===\")\n",
    "    output.append(f\"  Location:     {structured_entities['location'] or 'Not specified'}\")\n",
    "    output.append(f\"  Character:    {structured_entities['character'] or 'Not specified'}\")\n",
    "    output.append(f\"  Intensity:    {structured_entities['intensity'] or 'Not specified'}\")\n",
    "    \n",
    "    output.append(\"\\n=== PRIMARY SYMPTOMS ===\")\n",
    "    output.append(f\"  Nausea:       {structured_entities['nausea']}\")\n",
    "    output.append(f\"  Vomiting:     {structured_entities['vomit']}\")\n",
    "    output.append(f\"  Sound sens:   {structured_entities['phonophobia']}\")\n",
    "    output.append(f\"  Light sens:   {structured_entities['photophobia']}\")\n",
    "    \n",
    "    output.append(\"\\n=== VISUAL SYMPTOMS ===\")\n",
    "    output.append(f\"  Visual aura:  {structured_entities['visual'] or 'Not found'}\")\n",
    "    output.append(f\"  Diplopia:     {structured_entities['diplopia']}\")\n",
    "    output.append(f\"  Visual def:   {structured_entities['visual_defect']}\")\n",
    "    \n",
    "    output.append(\"\\n=== SENSORY SYMPTOMS ===\")\n",
    "    output.append(f\"  Sensory:      {structured_entities['sensory'] or 'Not found'}\")\n",
    "    output.append(f\"  Paresthesia:  {structured_entities['paresthesia']}\")\n",
    "    \n",
    "    output.append(\"\\n=== SPEECH/LANGUAGE ===\")\n",
    "    output.append(f\"  Dysphasia:    {structured_entities['dysphasia']}\")\n",
    "    output.append(f\"  Dysarthria:   {structured_entities['dysarthria']}\")\n",
    "    \n",
    "    output.append(\"\\n=== BALANCE & AUDITORY ===\")\n",
    "    output.append(f\"  Vertigo:      {structured_entities['vertigo']}\")\n",
    "    output.append(f\"  Ataxia:       {structured_entities['ataxia']}\")\n",
    "    output.append(f\"  Tinnitus:     {structured_entities['tinnitus']}\")\n",
    "    output.append(f\"  Hypoacusis:   {structured_entities['hypoacusis']}\")\n",
    "    \n",
    "    output.append(\"\\n=== OTHER ===\")\n",
    "    output.append(f\"  Consciousness: {structured_entities['conscience']}\")\n",
    "    output.append(f\"  Duration:      {structured_entities['duration'] or 'Not specified'}\")\n",
    "    output.append(f\"  Frequency:     {structured_entities['frequency'] or 'Not specified'}\")\n",
    "    output.append(f\"  Family hist:   {structured_entities['dpf']}\")\n",
    "    \n",
    "    return '\\n'.join(output)\n",
    "\n",
    "\n",
    "def test_model_on_samples(model_path, num_samples=5):\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"TESTING MODEL ON SAMPLE TEXTS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Load model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    model = AutoModelForTokenClassification.from_pretrained(model_path)\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    model.to(device)\n",
    "    \n",
    "    # Sample texts (including negations!)\n",
    "    samples = [\n",
    "        \"Woke up this morning w/ a weird feeling in my head. Started w/ zigzag lines all over my vision, and then I got a blind spot in the corner of my eye. Felt nauseous, so I grabbed a granola bar and took a short walk outside. After that, the auras went away, but I still had a headache. Ugh, my head feels like its being squeezed in a vice. Took some ibuprofen and drank water, which helped a bit. Pain is severe, and I can feel it throbbing in my left temple. Been like this for 3 hours now.\",\n",
    "        \"Bilateral headache, constant pressure. Moderate 6/10. No nausea, no light sensitivity.\",  # NEGATION TEST\n",
    "        \"Aura started - saw zigzag lines and felt tingling. Then severe right-sided pounding 9/10. Vomiting and phonophobia.\",\n",
    "        \"Dizzy and room spinning. Double vision. Slurred speech. No vomiting though.\",  # NEGATION TEST\n",
    "        \"@ 9:45am, w/ out no warnin, the throbbing started @ the back of my head & kinda moved to the left side @ 10am. nausea & sound sensitivity kicked in @ 10:15am. took some meds & rested @ 10:30am. after 2 hrs, pain went away.\"\n",
    "    ]\n",
    "    \n",
    "    for i, text in enumerate(samples[:num_samples], 1):\n",
    "        print(f\"\\n{'─'*80}\")\n",
    "        print(f\"SAMPLE {i}\")\n",
    "        print(f\"{'─'*80}\")\n",
    "        print(f\"Text: {text}\\n\")\n",
    "        \n",
    "        # Get structured entities\n",
    "        structured = extract_entities_structured(text, model, tokenizer, device)\n",
    "        \n",
    "        # Format and print\n",
    "        formatted = format_entity_output(structured)\n",
    "        print(formatted)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 8. BATCH PROCESSING\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "def process_all_diaries(model_path, data_dir, output_file):\n",
    "\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"BATCH PROCESSING ALL DIARIES\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # =========================================================================\n",
    "    # 1. LOAD MODEL\n",
    "    # =========================================================================\n",
    "    print(\"\\n1. Loading model...\")\n",
    "    print(f\"   Model path: {model_path}\")\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    model = AutoModelForTokenClassification.from_pretrained(model_path)\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    model.to(device)\n",
    "    \n",
    "    print(f\"   Model loaded on {device}\")\n",
    "    \n",
    "    # =========================================================================\n",
    "    # 2. FIND DIARY FILES\n",
    "    # =========================================================================\n",
    "    print(f\"\\n2. Finding diary files in {data_dir}...\")\n",
    "    data_path = Path(data_dir)\n",
    "    \n",
    "    if not data_path.exists():\n",
    "        print(f\"   Directory not found: {data_path}\")\n",
    "        return None\n",
    "    \n",
    "    # Only process llama3.json and qwen.json\n",
    "    target_files = ['llama3.json', 'qwen.json']\n",
    "    diary_files = [data_path / f for f in target_files if (data_path / f).exists()]\n",
    "    \n",
    "    if not diary_files:\n",
    "        print(f\"   No diary files found!\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"   ✓ Found {len(diary_files)} files:\")\n",
    "    for f in diary_files:\n",
    "        print(f\"      - {f.name}\")\n",
    "    \n",
    "    # =========================================================================\n",
    "    # 3. PROCESS EACH FILE - NO SKIPS!\n",
    "    # =========================================================================\n",
    "    print(\"\\n3. Processing diary entries (100% GUARANTEE MODE)...\")\n",
    "    \n",
    "    results = []\n",
    "    total_loaded = 0\n",
    "    missing_filename_count = 0\n",
    "    \n",
    "    for json_file in diary_files:\n",
    "        print(f\"\\n   📁 Processing: {json_file.name}\")\n",
    "        \n",
    "        # Load file\n",
    "        with open(json_file, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        if not isinstance(data, list):\n",
    "            print(f\"      Unexpected format: {type(data)}\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"      Entries in file: {len(data)}\")\n",
    "        total_loaded += len(data)\n",
    "        \n",
    "\n",
    "        for i, entry in enumerate(data):\n",
    "            \n",
    "            if 'filename' in entry:\n",
    "                patient_id = extract_patient_id(entry['filename'])\n",
    "                filename = entry['filename']\n",
    "            else:\n",
    "                patient_id = 0\n",
    "                filename = f\"unknown_entry_{len(results)}\"\n",
    "                missing_filename_count += 1\n",
    "            \n",
    "            text = entry.get('clean_text', entry.get('text', ''))\n",
    "            if not text:\n",
    "                text = \"\"  \n",
    "            \n",
    "            structured = extract_entities_structured(text, model, tokenizer, device)\n",
    "            \n",
    "            results.append({\n",
    "                'text_id': len(results),\n",
    "                'patient_id': patient_id,\n",
    "                'filename': filename,\n",
    "                'text': text[:200] if text else \"\",\n",
    "                'entities': structured\n",
    "            })\n",
    "            \n",
    "            # Progress indicator\n",
    "            if (i + 1) % 100 == 0:\n",
    "                print(f\"         Processed {i+1}/{len(data)} entries...\")\n",
    "        \n",
    "        print(f\"      ✓ Completed {json_file.name}\")\n",
    "        print(f\"      ✓ Processed ALL {len(data)} entries\")\n",
    "    \n",
    "    # =========================================================================\n",
    "    # 4. VERIFICATION\n",
    "    # =========================================================================\n",
    "    print(f\"\\n   Processing Summary:\")\n",
    "    print(f\"      Total entries loaded:    {total_loaded}\")\n",
    "    print(f\"      Total entries processed: {len(results)}\")\n",
    "    \n",
    "    if len(results) == total_loaded:\n",
    "        print(f\"      VERIFIED: All {total_loaded} entries saved!\")\n",
    "    else:\n",
    "        print(f\"      ERROR: Mismatch!\")\n",
    "        print(f\"         Expected: {total_loaded}\")\n",
    "        print(f\"         Got: {len(results)}\")\n",
    "        print(f\"         Missing: {total_loaded - len(results)}\")\n",
    "    \n",
    "    if missing_filename_count > 0:\n",
    "        print(f\"       Entries with missing filename: {missing_filename_count}\")\n",
    "        print(f\"         (These were saved with default values)\")\n",
    "    \n",
    "    if not results:\n",
    "        print(\"\\n   No results to save!\")\n",
    "        return None\n",
    "    \n",
    "    # =========================================================================\n",
    "    # 5. SAVE JSON\n",
    "    # =========================================================================\n",
    "    print(\"\\n4. Saving results...\")\n",
    "    output_path = Path(output_file)\n",
    "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(results, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    file_size_kb = output_path.stat().st_size / 1024\n",
    "    print(f\"   ✓ JSON saved: {output_path}\")\n",
    "    print(f\"      Size: {file_size_kb:.1f} KB\")\n",
    "    print(f\"      Entries: {len(results)}\")\n",
    "    \n",
    "    # =========================================================================\n",
    "    # 6. SAVE CSV\n",
    "    # =========================================================================\n",
    "    csv_path = output_path.with_suffix('.csv')\n",
    "    \n",
    "    csv_data = []\n",
    "    for r in results:\n",
    "        row = {\n",
    "            'text_id': r['text_id'],\n",
    "            'patient_id': r['patient_id'],\n",
    "            'filename': r['filename'],\n",
    "            **r['entities']\n",
    "        }\n",
    "        csv_data.append(row)\n",
    "    \n",
    "    df = pd.DataFrame(csv_data)\n",
    "    df.to_csv(csv_path, index=False, encoding='utf-8')\n",
    "    \n",
    "    csv_size_kb = csv_path.stat().st_size / 1024\n",
    "    print(f\"   ✓ CSV saved: {csv_path}\")\n",
    "    print(f\"      Size: {csv_size_kb:.1f} KB\")\n",
    "    print(f\"      Rows: {len(df)}\")\n",
    "    \n",
    "    # =========================================================================\n",
    "    # 7. FINAL VERIFICATION\n",
    "    # =========================================================================\n",
    "    print(\"\\n5. Final Verification:\")\n",
    "    \n",
    "    # Re-read files to verify\n",
    "    with open(output_path, 'r') as f:\n",
    "        saved_json = json.load(f)\n",
    "    \n",
    "    saved_csv = pd.read_csv(csv_path)\n",
    "    \n",
    "    print(f\"   JSON entries:        {len(saved_json)}\")\n",
    "    print(f\"   CSV rows:            {len(saved_csv)}\")\n",
    "    print(f\"   Expected:            {total_loaded}\")\n",
    "    \n",
    "    # TRIPLE VERIFICATION\n",
    "    all_match = (\n",
    "        len(results) == total_loaded and\n",
    "        len(saved_json) == total_loaded and\n",
    "        len(saved_csv) == total_loaded\n",
    "    )\n",
    "    \n",
    "    if all_match:\n",
    "        print(f\"\\n   100% VERIFIED!\")\n",
    "        print(f\"   All {total_loaded} entries saved correctly!\")\n",
    "        print(f\"   JSON, CSV, and memory all match!\")\n",
    "    else:\n",
    "        print(f\"\\n   VERIFICATION FAILED!\")\n",
    "        print(f\"      In memory:  {len(results)}\")\n",
    "        print(f\"      JSON file:  {len(saved_json)}\")\n",
    "        print(f\"      CSV file:   {len(saved_csv)}\")\n",
    "        print(f\"      Expected:   {total_loaded}\")\n",
    "    \n",
    "    # Statistics\n",
    "    print(\"\\n6. Statistics:\")\n",
    "    print(f\"   Files processed:     {len(diary_files)}\")\n",
    "    print(f\"   Total entries:       {total_loaded}\")\n",
    "    print(f\"   Entries saved:       {len(results)}\")\n",
    "    \n",
    "    if len(results) > 0:\n",
    "        print(f\"   Unique patients:     {df['patient_id'].nunique()}\")\n",
    "        print(f\"   Entries per patient: {len(results) / df['patient_id'].nunique():.1f}\")\n",
    "        \n",
    "        # Count blank days\n",
    "        blank_count = sum(1 for r in results if not r['text'] or r['text'].lower() in ['blank', ''])\n",
    "        if blank_count > 0:\n",
    "            print(f\"   Blank days kept:     {blank_count} \")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"BATCH PROCESSING COMPLETE!\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\nGUARANTEE FULFILLED: {len(results)}/{total_loaded} entries saved\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN EXECUTION\n",
    "# =============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)    \n",
    "    # Train model\n",
    "    print(\"\\nSTEP 1: Training NER model...\")\n",
    "    trainer, tokenizer = train_ner_model()\n",
    "    \n",
    "    # Test on samples\n",
    "    print(\"\\nSTEP 2: Testing on sample texts...\")\n",
    "    model_path = Path(CONFIG['output_dir']) / 'final_model'\n",
    "    test_model_on_samples(model_path, num_samples=5)\n",
    "    \n",
    "    # Process all diaries\n",
    "    print(\"\\nSTEP 3: Processing all diaries ...\")\n",
    "    results = process_all_diaries(\n",
    "        model_path=model_path,\n",
    "        data_dir=CONFIG['data_dir'],\n",
    "        output_file='data/ner_results/extracted_entities.json'\n",
    "    )\n",
    "    \n",
    "    # Final summary\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"PIPELINE COMPLETE!\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    if results:\n",
    "        print(f\"\\nSuccess!\")\n",
    "        print(f\"   Model: {model_path}\")\n",
    "        print(f\"   Results: data/ner_results/extracted_entities.json\")\n",
    "        print(f\"   Total entries: {len(results)}\")\n",
    "        print(f\"\\n All entries saved - verified!\")\n",
    "    else:\n",
    "        print(f\"\\n No results generated!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a6592c-259b-48de-aa08-56b8dd27d09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "# =============================================================================\n",
    "# 1. DURATION EXTRACTION & CALCULATION\n",
    "# =============================================================================\n",
    "\n",
    "def parse_time(time_str):\n",
    "\n",
    "    time_str = time_str.strip().lower().replace('@', '').replace(' ', '')\n",
    "    \n",
    "    formats = [\n",
    "        '%I:%M%p',    # 10:45am\n",
    "        '%I%p',       # 11pm\n",
    "        '%H:%M',      # 13:30\n",
    "    ]\n",
    "    \n",
    "    for fmt in formats:\n",
    "        try:\n",
    "            time_obj = datetime.strptime(time_str, fmt).time()\n",
    "            return datetime.combine(datetime.today(), time_obj)\n",
    "        except ValueError:\n",
    "            continue\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "def calculate_duration_from_times(start_str, end_str):\n",
    "\n",
    "    \n",
    "    start = parse_time(start_str)\n",
    "    end = parse_time(end_str)\n",
    "    \n",
    "    if not start or not end:\n",
    "        return None\n",
    "    \n",
    "    duration = end - start\n",
    "    \n",
    "    # Handle overnight (end < start)\n",
    "    if duration.total_seconds() < 0:\n",
    "        duration += timedelta(days=1)\n",
    "    \n",
    "    hours = duration.total_seconds() / 3600\n",
    "    return round(hours, 2)\n",
    "\n",
    "\n",
    "def extract_duration_from_text(text):\n",
    "\n",
    "    \n",
    "    text_lower = text.lower()\n",
    "    \n",
    "    # STRATEGY 1: Explicit duration\n",
    "    explicit_patterns = [\n",
    "        r'lasted\\s+(?:for\\s+)?(\\d+(?:\\.\\d+)?)\\s+hours?',\n",
    "        r'duration\\s*:?\\s*(\\d+(?:\\.\\d+)?)\\s+hours?',\n",
    "        r'for\\s+(\\d+(?:\\.\\d+)?)\\s+hours?',\n",
    "        r'(\\d+(?:\\.\\d+)?)\\s+hours?\\s+(?:long|total)',\n",
    "        r'attack\\s+lasted\\s+(?:for\\s+)?(\\d+(?:\\.\\d+)?)\\s+hours?',\n",
    "    ]\n",
    "    \n",
    "    for pattern in explicit_patterns:\n",
    "        match = re.search(pattern, text_lower)\n",
    "        if match:\n",
    "            hours = float(match.group(1))\n",
    "            return {\n",
    "                'duration_hours': hours,\n",
    "                'duration_text': match.group(0),\n",
    "                'method': 'explicit'\n",
    "            }\n",
    "    \n",
    "    # STRATEGY 2: Time range\n",
    "    # More flexible patterns for time extraction\n",
    "    start_patterns = [\n",
    "        r'started?\\s+@?\\s*(\\d{1,2}:?\\d{0,2}\\s*(?:am|pm))',\n",
    "        r'began\\s+@?\\s*(\\d{1,2}:?\\d{0,2}\\s*(?:am|pm))',\n",
    "    ]\n",
    "    \n",
    "    end_patterns = [\n",
    "        r'ended?\\s+@?\\s*(\\d{1,2}:?\\d{0,2}\\s*(?:am|pm))',\n",
    "        r'finished\\s+@?\\s*(\\d{1,2}:?\\d{0,2}\\s*(?:am|pm))',\n",
    "        r'stopped\\s+@?\\s*(\\d{1,2}:?\\d{0,2}\\s*(?:am|pm))',\n",
    "    ]\n",
    "    \n",
    "    start_time = None\n",
    "    end_time = None\n",
    "    \n",
    "    for pattern in start_patterns:\n",
    "        match = re.search(pattern, text_lower)\n",
    "        if match:\n",
    "            start_time = match.group(1)\n",
    "            break\n",
    "    \n",
    "    for pattern in end_patterns:\n",
    "        match = re.search(pattern, text_lower)\n",
    "        if match:\n",
    "            end_time = match.group(1)\n",
    "            break\n",
    "    \n",
    "    if start_time and end_time:\n",
    "        hours = calculate_duration_from_times(start_time, end_time)\n",
    "        if hours:\n",
    "            return {\n",
    "                'duration_hours': hours,\n",
    "                'duration_text': f\"{start_time} to {end_time}\",\n",
    "                'method': 'time_range'\n",
    "            }\n",
    "    \n",
    "    # STRATEGY 3: Approximate\n",
    "    approximate_patterns = {\n",
    "        r'\\ball\\s+day\\b': 12.0,\n",
    "        r'\\bentire\\s+day\\b': 12.0,\n",
    "        r'\\bfew\\s+hours\\b': 3.0,\n",
    "        r'\\bcouple\\s+(?:of\\s+)?hours\\b': 2.0,\n",
    "        r'\\bseveral\\s+hours\\b': 4.0,\n",
    "    }\n",
    "    \n",
    "    for pattern, hours in approximate_patterns.items():\n",
    "        if re.search(pattern, text_lower):\n",
    "            return {\n",
    "                'duration_hours': hours,\n",
    "                'duration_text': re.search(pattern, text_lower).group(0),\n",
    "                'method': 'approximate'\n",
    "            }\n",
    "    \n",
    "    return {\n",
    "        'duration_hours': None,\n",
    "        'duration_text': None,\n",
    "        'method': None\n",
    "    }\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 2. FREQUENCY CALCULATION\n",
    "# =============================================================================\n",
    "\n",
    "def calculate_frequency_from_diary(diary_text):\n",
    "    \"\"\"\n",
    "    Count headache attack days in 30-day diary.\n",
    "    \n",
    "    Logic:\n",
    "    1. Split into days\n",
    "    2. Count days with headache indicators\n",
    "    3. Return attacks/month\n",
    "    \"\"\"\n",
    "    \n",
    "    # Split by \"Day X:\"\n",
    "    day_pattern = r'Day\\s+(\\d+):\\s*([^\\n]+(?:\\n(?!Day\\s+\\d+:)[^\\n]+)*)'\n",
    "    days = re.findall(day_pattern, diary_text, re.MULTILINE | re.IGNORECASE)\n",
    "    \n",
    "    # Headache indicators\n",
    "    headache_keywords = [\n",
    "        r'\\bha\\b',\n",
    "        r'\\bheadache\\b',\n",
    "        r'\\bmigraine\\b',\n",
    "        r'\\bpain\\b',\n",
    "        r'\\bthrobbing\\b',\n",
    "        r'\\bpounding\\b',\n",
    "        r'\\bpulsating\\b',\n",
    "        r'\\baura\\b',\n",
    "    ]\n",
    "    \n",
    "    attack_days = []\n",
    "    \n",
    "    for day_num, day_text in days:\n",
    "        text_lower = day_text.lower().strip()\n",
    "        \n",
    "        # Skip blank days\n",
    "        if not text_lower or text_lower in ['blank', '']:\n",
    "            continue\n",
    "        \n",
    "        # Check for headache\n",
    "        has_headache = any(\n",
    "            re.search(kw, text_lower)\n",
    "            for kw in headache_keywords\n",
    "        )\n",
    "        \n",
    "        if has_headache:\n",
    "            attack_days.append(int(day_num))\n",
    "    \n",
    "    return {\n",
    "        'attacks_per_month': len(attack_days),\n",
    "        'attack_days': attack_days,\n",
    "        'total_days': len(days),\n",
    "        'method': 'diary_count'\n",
    "    }\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 3. ENHANCED SYMPTOM PATTERNS\n",
    "# =============================================================================\n",
    "\n",
    "ENHANCED_PATTERNS = {\n",
    "    'TINNITUS': [\n",
    "        # All your original patterns\n",
    "        r'\\btinnitus\\b',\n",
    "        r'\\bringing(?:ing)?\\s+in\\s+(?:the\\s+)?ears?\\b',\n",
    "        r'\\bears?\\s+(?:were\\s+)?ringing\\b',\n",
    "        r'\\bbuzzing\\s+(?:in\\s+)?(?:the\\s+)?ears?\\b',\n",
    "        r'\\bhigh[-\\s]?pitched\\s+(?:sound|noise|ringing)\\b',\n",
    "        r'\\bwhistling\\s+(?:in\\s+)?(?:the\\s+)?ears?\\b',\n",
    "        r'\\bhumming\\s+(?:in\\s+)?(?:the\\s+)?ears?\\b',\n",
    "        r'\\bnoise\\s+in\\s+(?:my\\s+)?ears?\\b',\n",
    "        \n",
    "        # ADDED: Common variations\n",
    "        r'\\bears?\\s+(?:making|had)\\s+(?:a\\s+)?(?:ringing|buzzing|humming)\\b',\n",
    "        r'\\bheard?\\s+(?:ringing|buzzing|humming)\\b',\n",
    "        r'\\bears?\\s+(?:kept|started)\\s+(?:ringing|buzzing)\\b',\n",
    "        r'\\bringing\\s+(?:sound|noise)\\b',\n",
    "        r'\\bbuzzing\\s+(?:sound|noise)\\b',\n",
    "        r'\\bstrange\\s+(?:sound|noise)\\s+in\\s+ears?\\b',\n",
    "    ],\n",
    "    \n",
    "    'VISUAL': [\n",
    "        # Original patterns\n",
    "        r'\\bvisual\\s+aura\\b',\n",
    "        r'\\bzigzag\\s+lines?\\b',\n",
    "        r'\\bzig[-\\s]?zag\\s+lines?\\b',\n",
    "        r'\\bjagged\\s+lines?\\b',\n",
    "        r'\\bflashing\\s+(?:lights?|vision)?\\b',\n",
    "        r'\\bsparkles?\\b',\n",
    "        r'\\bblind\\s+spots?\\b',\n",
    "        r'\\bscotoma\\b',\n",
    "        \n",
    "        r'\\bgot\\s+(?:the\\s+)?(?:typical|usual)\\s+aura\\b',\n",
    "        r'\\bzigzag\\s+lines?\\s+&\\s+blind\\s+spots?\\b',  # \"zigzag lines & blind spots\"\n",
    "        r'\\bsaw\\s+zigzag\\b',\n",
    "        r'\\bvision\\s+(?:got\\s+)?(?:weird|strange|blurry)\\b',\n",
    "        r'\\bseeing\\s+(?:zigzag|lights?|sparkles?|spots?)\\b',\n",
    "        r'\\bblind\\s+spots?\\s+in\\s+(?:my\\s+)?vision\\b',\n",
    "    ],\n",
    "    \n",
    "    'SENSORY': [\n",
    "        # Original\n",
    "        r'\\bsensory\\s+(?:aura|symptoms?)\\b',\n",
    "        r'\\btingling\\b',\n",
    "        r'\\bnumbness\\b',\n",
    "        r'\\bpins\\s+and\\s+needles\\b',\n",
    "        \n",
    "        r'\\btingling\\s+&\\s+numbness\\b',\n",
    "        r'\\btingling\\s+(?:in\\s+)?(?:my\\s+)?(?:fingers?|toes?)\\b',\n",
    "        r'\\bnumbness\\s+(?:in\\s+)?(?:my\\s+)?(?:fingers?|toes?)\\b',\n",
    "        r'\\b(?:fingers?|toes?)\\s+(?:were\\s+)?(?:tingling|numb)\\b',\n",
    "        r'\\bfelt\\s+tingling\\b',\n",
    "    ],\n",
    "    \n",
    "    'DYSPHASIA': [\n",
    "        r'\\bdysphasia\\b',\n",
    "        r'\\btrouble\\s+finding\\s+words\\b',\n",
    "        r'\\bcan\\'?t\\s+find\\s+words\\b',\n",
    "        r'\\bword[-\\s]?finding\\s+difficult',\n",
    "        r'\\bspeech\\s+problems?\\b',\n",
    "        r'\\bstruggling\\s+to\\s+talk\\b',\n",
    "    ],\n",
    "    \n",
    "    'DYSARTHRIA': [\n",
    "        r'\\bdysarthria\\b',\n",
    "        r'\\bslurr(?:ed|ing)\\s+(?:speech|words)\\b',\n",
    "        r'\\bcan\\'?t\\s+speak\\s+clearly\\b',\n",
    "        r'\\bmumbling\\b',\n",
    "        r'\\bgarbled\\s+speech\\b',\n",
    "    ],\n",
    "    \n",
    "    'DIPLOPIA': [\n",
    "        r'\\bdiplopia\\b',\n",
    "        r'\\bdouble\\s+vision\\b',\n",
    "        r'\\bseeing\\s+double\\b',\n",
    "        r'\\btwo\\s+of\\s+everything\\b',\n",
    "    ],\n",
    "}\n",
    "\n",
    "\n",
    "def enhanced_symptom_detection(text):\n",
    "    \"\"\"\n",
    "    Detect symptoms using enhanced patterns.\n",
    "    This runs AFTER NER to catch anything missed.\n",
    "    \"\"\"\n",
    "    \n",
    "    text_lower = text.lower()\n",
    "    detected = {}\n",
    "    \n",
    "    for symptom, patterns in ENHANCED_PATTERNS.items():\n",
    "        matches = []\n",
    "        for pattern in patterns:\n",
    "            for match in re.finditer(pattern, text_lower):\n",
    "                matches.append(match.group(0))\n",
    "        \n",
    "        if matches:\n",
    "            detected[symptom] = list(set(matches))  # Unique matches\n",
    "    \n",
    "    return detected\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 4. COMPLETE POST-PROCESSING PIPELINE\n",
    "# =============================================================================\n",
    "\n",
    "def post_process_ner_results(\n",
    "    ner_results_json,\n",
    "    output_file='data/ner_results/enhanced_entities.json'\n",
    "):\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"POST-PROCESSING NER RESULTS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Load results\n",
    "    print(\"\\n1. Loading NER results...\")\n",
    "    with open(ner_results_json, 'r') as f:\n",
    "        results = json.load(f)\n",
    "    print(f\"  ✓ Loaded {len(results)} entries\")\n",
    "    \n",
    "    # Process each\n",
    "    print(\"\\n2. Enhancing extraction...\")\n",
    "    enhanced = []\n",
    "    \n",
    "    for i, entry in enumerate(results):\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(f\"  Processed {i+1}/{len(results)}...\")\n",
    "        \n",
    "        text = entry['text']\n",
    "        entities = entry['entities']\n",
    "        \n",
    "        # Extract duration\n",
    "        duration_info = extract_duration_from_text(text)\n",
    "        if duration_info['duration_hours']:\n",
    "            entities['duration'] = duration_info['duration_hours']\n",
    "            entities['duration_text'] = duration_info['duration_text']\n",
    "            entities['duration_method'] = duration_info['method']\n",
    "        \n",
    "        # Enhanced symptoms\n",
    "        enhanced_symptoms = enhanced_symptom_detection(text)\n",
    "        patient_id = entry.get('patient_id', 0)\n",
    "        \n",
    "        # Merge with NER results\n",
    "        for symptom, matches in enhanced_symptoms.items():\n",
    "            key = symptom.lower()\n",
    "            \n",
    "            # If NER missed it, add from post-processing\n",
    "            if not entities.get(key) or entities[key] == 'Not found':\n",
    "                entities[key] = matches[0] if matches else 'Not found'\n",
    "                entities[f'{key}_enhanced'] = True\n",
    "        \n",
    "        enhanced.append({\n",
    "            'text_id': entry['text_id'],\n",
    "            'patient_id': patient_id,\n",
    "            'text': text,\n",
    "            'entities': entities,\n",
    "        })\n",
    "    \n",
    "    # Save\n",
    "    print(\"\\n3. Saving enhanced results...\")\n",
    "    output_path = Path(output_file)\n",
    "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump(enhanced, f, indent=2)\n",
    "    \n",
    "    csv_path = output_path.with_suffix('.csv')\n",
    "    # Create list of rows\n",
    "    csv_data = []\n",
    "    for r in enhanced:\n",
    "        row = {\n",
    "            'text_id': r['text_id'],\n",
    "            'patient_id': r['patient_id'],\n",
    "            **r['entities']\n",
    "        }\n",
    "        csv_data.append(row)\n",
    "\n",
    "    # Create DataFrame and save\n",
    "    df = pd.DataFrame(csv_data)\n",
    "    df.to_csv(csv_path, index=False, encoding='utf-8')\n",
    "    \n",
    "    print(f\"  ✓ JSON: {output_path}\")\n",
    "    print(f\"  ✓ CSV: {csv_path}\")\n",
    "    \n",
    "    print(\"\\nPOST-PROCESSING COMPLETE!\")\n",
    "    return enhanced\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 5. TESTING\n",
    "# =============================================================================\n",
    "\n",
    "def test_on_your_example():\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"TESTING ON PATIENT 12 EXAMPLE\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Day 2 from your example\n",
    "    day2_text = \"\"\"HA!  Started @ 10:45am while at the office. Pain was severe & throbbing on the L side of my head. Dizziness & nausea set in @ 11:30am. Couldn't tolerate any light or sound. Went home & took ibuprofen @ 1pm, which helped a bit. Attack lasted for 3 hours. Tho, still feeling a bit queasy rn.\"\"\"\n",
    "    \n",
    "    print(\"\\nText:\", day2_text)\n",
    "    \n",
    "    # Test duration extraction\n",
    "    print(\"\\n\" + \"─\"*80)\n",
    "    print(\"DURATION EXTRACTION:\")\n",
    "    print(\"─\"*80)\n",
    "    duration = extract_duration_from_text(day2_text)\n",
    "    print(f\"Duration: {duration['duration_hours']} hours\")\n",
    "    print(f\"Text: {duration['duration_text']}\")\n",
    "    print(f\"Method: {duration['method']}\")\n",
    "    \n",
    "    # Test symptom detection\n",
    "    print(\"\\n\" + \"─\"*80)\n",
    "    print(\"ENHANCED SYMPTOM DETECTION:\")\n",
    "    print(\"─\"*80)\n",
    "    symptoms = enhanced_symptom_detection(day2_text)\n",
    "    for symptom, matches in symptoms.items():\n",
    "        print(f\"{symptom}: {matches}\")\n",
    "    \n",
    "    # Day 8 with visual aura\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"DAY 8 - VISUAL AURA TEST\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    day8_text = \"\"\"Had a w/ the most severe HA this yr. Started @ 8:45am while getting ready for work. The pain was like a jackhammer, throbbing & pulsating on the L side. Got the typical aura tho: zigzag lines & blind spots in my vision. Had to cancel my day & just rest. Took ibuprofen & acetaminophen @ 10am, but it didn't help much. Lasted for 3 hours tho.\"\"\"\n",
    "    \n",
    "    print(\"\\nText:\", day8_text)\n",
    "    \n",
    "    duration = extract_duration_from_text(day8_text)\n",
    "    print(f\"\\nDuration: {duration['duration_hours']} hours\")\n",
    "    \n",
    "    symptoms = enhanced_symptom_detection(day8_text)\n",
    "    print(f\"\\nDetected symptoms:\")\n",
    "    for symptom, matches in symptoms.items():\n",
    "        print(f\"  {symptom}: {matches}\")\n",
    "    \n",
    "    # Full diary frequency test\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"FREQUENCY CALCULATION TEST\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    full_diary = \"\"\"\n",
    "Patient 35\n",
    "\n",
    "Day 1: \n",
    "blank\n",
    "\n",
    "Day 2: \n",
    "today was a pretty chill day tho, didnt do much rn. woke up @ 8am, had breakfast w/ my sis @ 10am. had a big project @ work due today, so I was pretty stressed out. felt a weird kinda pressure in my head, like its just gonna burst any sec now. thats when the throbbing started @ 2pm. this time its on my left side, HA. nausea hit @ 2:30pm. took some meds @ 3pm & rested for a bit. after 2 hrs the pain subsided.\n",
    "\n",
    "Day 3: \n",
    "blank\n",
    "\n",
    "Day 4: \n",
    "tired rn. woke up @ 7am. didnt have any plans 4 today, so I just relaxed @ home. didnt feel like doin much tho. felt a slight headache @ 12pm. pain's not too intense rn, just a dull ache. prob just stress from work.\n",
    "\n",
    "Day 5: \n",
    "@ 9:45am, w/ out no warnin, the throbbing started @ the back of my head & kinda moved to the left side @ 10am. nausea & sound sensitivity kicked in @ 10:15am. took some meds & rested @ 10:30am. after 2 hrs, pain went away.\n",
    "\n",
    "Day 6: \n",
    "blank\n",
    "\n",
    "Day 7: \n",
    "today was crazy busy @ work, had to finish 2 projects @ the same time. was pretty stressed out, but w/ the help of my coworkers, I got it all done. no headache tho. yay!\n",
    "\n",
    "Day 8: \n",
    "@ 12:20pm, the pulsating started @ the front of my head & kinda moved to the left side @ 12:30pm. nausea hit @ 12:35pm. took some meds @ 1pm & rested for a bit. after 2 hrs, pain went away.\n",
    "\n",
    "Day 9: \n",
    "blank\n",
    "\n",
    "Day 10: \n",
    "today was a pretty relaxed day @ home. watched some movies & played video games. felt a slight headache @ 3pm. prob just stress from work.\n",
    "\n",
    "Day 11: \n",
    "@ 9:20am, w/ out no warnin, the throbbing started @ the front of my head & kinda moved to the left side @ 9:30am. nausea & light sensitivity kicked in @ 9:35am. took some meds & rested @ 9:45am. after 2 hrs, pain went away.\n",
    "\n",
    "Day 12: \n",
    "blank\n",
    "\n",
    "Day 13: \n",
    "today was a pretty chill day, didnt do much rn. went 4 a walk @ 2pm & felt a slight headache @ 2:30pm. prob just stress from work.\n",
    "\n",
    "Day 14: \n",
    "@ 12:40pm, the pulsating started @ the back of my head & kinda moved to the left side @ 12:50pm. nausea & sound sensitivity kicked in @ 12:55pm. took some meds @ 1pm & rested for a bit. after 2 hrs, pain went away.\n",
    "\n",
    "Day 15: \n",
    "blank\n",
    "\n",
    "Day 16: \n",
    "today was crazy busy @ work, had to finish 3 projects @ the same time. was pretty stressed out, but w/ the help of my coworkers, I got it all done. no headache tho. yay!\n",
    "\n",
    "Day 17: \n",
    "@ 9:45am, w/ out no warnin, the throbbing started @ the front of my head & kinda moved to the left side @ 10am. nausea & light sensitivity kicked in @ 10:05am. took some meds & rested @ 10:15am. after 2 hrs, pain went away.\n",
    "\n",
    "Day 18: \n",
    "blank\n",
    "\n",
    "Day 19: \n",
    "today was a pretty relaxed day @ home. watched some movies & played video games. felt a slight headache @ 2pm. prob just stress from work.\n",
    "\n",
    "Day 20: \n",
    "@ 12:15pm, the pulsating started @ the back of my head & kinda moved to the left side @ 12:25pm. nausea & sound sensitivity kicked in @ 12:30pm. took some meds @ 12:40pm & rested for a bit. after 2 hrs, pain went away.\n",
    "\n",
    "Day 21: \n",
    "blank\n",
    "\n",
    "Day 22: \n",
    "today was crazy busy @ work, had to finish 2 projects @ the same time. was pretty stressed out, but w/ the help of my coworkers, I got it all done. no headache tho. yay!\n",
    "\n",
    "Day 23: \n",
    "@ 9:30am, w/ out no warnin, the throbbing started @ the front of my head & kinda moved to the left side @ 9:40am. nausea & light sensitivity kicked in @ 9:45am. took some meds & rested @ 9:55am. after 2 hrs, pain went away.\n",
    "\n",
    "Day 24: \n",
    "blank\n",
    "\n",
    "Day 25: \n",
    "today was a pretty relaxed day @ home. watched some movies & played video games. felt a slight headache @ 3pm. prob just stress from work.\n",
    "\n",
    "Day 26: \n",
    "@ 12:10pm, the pulsating started @ the back of my head & kinda moved to the left side @ 12:20pm. nausea & sound sensitivity kicked in @ 12:25pm. took some meds @ 12:35pm & rested for a bit. after 2 hrs, pain went away.\n",
    "\n",
    "Day 27: \n",
    "blank\n",
    "\n",
    "Day 28: \n",
    "today was a pretty chill day, didnt do much rn. went 4 a walk @ 2pm & felt a slight headache @ 2:30pm. prob just stress from work.\n",
    "\n",
    "Day 29: \n",
    "@ 9:50am, w/ out no warnin, the throbbing started @ the front of my head & kinda moved to the left side @ 10am. nausea & light sensitivity kicked in @ 10:05am. took some meds & rested @ 10:15am. after 2 hrs, pain went away.\n",
    "\n",
    "Day 30: \n",
    "today was a pretty relaxed day @ home. watched some movies & played video games. felt a slight headache @ 4pm. prob just stress from work.\n",
    "\"\"\"\n",
    "    \n",
    "    freq = calculate_frequency_from_diary(full_diary)\n",
    "    print(f\"\\nAttacks per month: {freq['attacks_per_month']}\")\n",
    "    print(f\"Attack days: {freq['attack_days']}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Run tests\n",
    "    test_on_your_example()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ALL TESTS COMPLETE!\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # To use on actual NER results:\n",
    "    post_process_ner_results(\n",
    "         ner_results_json='data/ner_results/extracted_entities.json',\n",
    "         output_file='data/ner_results/postenhanced_entities.json'\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b97929d-8ad0-4eb6-838d-eb180a6b9594",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "# =============================================================================\n",
    "# 1. ADD DPF TO PATIENT SUMMARIES\n",
    "# =============================================================================\n",
    "\n",
    "def add_dpf_to_summaries(\n",
    "    summaries_file='data/ner_results/patient_summaries_fixed.json',\n",
    "    original_data='data/migraine_with_id.csv',\n",
    "    output_file='data/ner_results/patient_summaries_with_dpf.json'\n",
    "):\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ADDING DPF TO PATIENT SUMMARIES\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Load patient summaries\n",
    "    print(\"\\n1. Loading patient summaries...\")\n",
    "    with open(summaries_file, 'r', encoding='utf-8') as f:\n",
    "        summaries = json.load(f)\n",
    "    \n",
    "    print(f\"   ✓ Loaded {len(summaries)} patient summaries\")\n",
    "    \n",
    "    # Load original data with DPF\n",
    "    print(\"\\n2. Loading original data with DPF...\")\n",
    "    original_df = pd.read_csv(original_data)\n",
    "    print(f\"   ✓ Loaded {len(original_df)} rows\")\n",
    "    print(f\"   ✓ Columns: {list(original_df.columns)}\")\n",
    "    \n",
    "    # Check if DPF column exists\n",
    "    if 'DPF' not in original_df.columns and 'dpf' not in original_df.columns:\n",
    "        print(\"\\n    DPF column not found! Available columns:\")\n",
    "        for col in original_df.columns:\n",
    "            print(f\"      - {col}\")\n",
    "        return None\n",
    "    \n",
    "    dpf_col = 'DPF' if 'DPF' in original_df.columns else 'dpf'\n",
    "    \n",
    "    # Map patient_id to DPF value\n",
    "    print(\"\\n3. Mapping DPF values to patients...\")\n",
    "    dpf_map = {}\n",
    "    \n",
    "    for idx, row in original_df.iterrows():\n",
    "        patient_id = idx + 1  # Assuming 1-indexed\n",
    "        dpf_value = row[dpf_col]\n",
    "        dpf_map[patient_id] = dpf_value\n",
    "    \n",
    "    print(f\"   ✓ Mapped {len(dpf_map)} DPF values\")\n",
    "    \n",
    "    # Add DPF to each summary\n",
    "    print(\"\\n4. Adding DPF to summaries...\")\n",
    "    added_count = 0\n",
    "    missing_count = 0\n",
    "    \n",
    "    for summary in summaries:\n",
    "        patient_id = summary.get('patient_id')\n",
    "        \n",
    "        if patient_id in dpf_map:\n",
    "            summary['dpf'] = dpf_map[patient_id]\n",
    "            added_count += 1\n",
    "        else:\n",
    "            summary['dpf'] = 0  # Default: no family history\n",
    "            missing_count += 1\n",
    "    \n",
    "    print(f\"   ✓ Added DPF to {added_count} summaries\")\n",
    "    if missing_count > 0:\n",
    "        print(f\"   {missing_count} patients missing DPF (set to 0)\")\n",
    "    \n",
    "    # Save updated summaries\n",
    "    print(\"\\n5. Saving updated summaries...\")\n",
    "    output_path = Path(output_file)\n",
    "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(summaries, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"   ✓ Saved to: {output_path}\")\n",
    "    \n",
    "    # Statistics\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"DPF STATISTICS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    dpf_values = [s.get('dpf', 0) for s in summaries]\n",
    "    dpf_counts = Counter(dpf_values)\n",
    "    \n",
    "    print(f\"\\n DPF Distribution:\")\n",
    "    for dpf, count in sorted(dpf_counts.items()):\n",
    "        pct = count / len(summaries) * 100\n",
    "        print(f\"   DPF={dpf}: {count:4d} ({pct:5.1f}%)\")\n",
    "    \n",
    "    print(\"\\nDPF ADDED TO SUMMARIES!\")\n",
    "    \n",
    "    return summaries\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 2. ANALYZE ENTITY EXTRACTION QUALITY\n",
    "# =============================================================================\n",
    "\n",
    "def analyze_entity_quality(summaries_file='data/ner_results/patient_summaries.json'):\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ANALYZING ENTITY EXTRACTION QUALITY\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Load summaries\n",
    "    print(\"\\n1. Loading patient summaries...\")\n",
    "    with open(summaries_file, 'r', encoding='utf-8') as f:\n",
    "        summaries = json.load(f)\n",
    "    \n",
    "    print(f\"   ✓ Loaded {len(summaries)} patients\")\n",
    "    \n",
    "    # Define entity fields\n",
    "    entity_fields = [\n",
    "        'duration', 'intensity', 'location', 'character', 'frequency',\n",
    "        'nausea', 'vomit', 'photophobia', 'phonophobia',\n",
    "        'visual', 'sensory', 'dysphasia', 'dysarthria',\n",
    "        'vertigo', 'tinnitus', 'hypoacusis', 'diplopia', 'ataxia', 'conscience',\n",
    "        'dpf'\n",
    "    ]\n",
    "    \n",
    "    # Count missing/present for each entity\n",
    "    print(\"\\n2. Analyzing entity extraction rates...\")\n",
    "    \n",
    "    entity_stats = {}\n",
    "    \n",
    "    for field in entity_fields:\n",
    "        present_count = 0\n",
    "        missing_count = 0\n",
    "        values = []\n",
    "        \n",
    "        for summary in summaries:\n",
    "            value = summary.get(field)\n",
    "            \n",
    "            # Check if present\n",
    "            if value is None or value in ['Not found', 'Not specified', 'None', '', 0]:\n",
    "                missing_count += 1\n",
    "            else:\n",
    "                present_count += 1\n",
    "                values.append(value)\n",
    "        \n",
    "        entity_stats[field] = {\n",
    "            'present': present_count,\n",
    "            'missing': missing_count,\n",
    "            'rate': present_count / len(summaries) * 100,\n",
    "            'sample_values': values[:5] if values else []\n",
    "        }\n",
    "    \n",
    "    # Print statistics\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ENTITY EXTRACTION RATES\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(\"\\nCritical Entities (needed for diagnosis):\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    critical_entities = ['duration', 'intensity', 'location', 'character', \n",
    "                         'nausea', 'photophobia', 'phonophobia']\n",
    "    \n",
    "    for field in critical_entities:\n",
    "        stats = entity_stats[field]\n",
    "        print(f\"   {field:20s}: {stats['present']:4d}/{len(summaries)} ({stats['rate']:5.1f}%)\")\n",
    "        if stats['sample_values']:\n",
    "            print(f\"      Sample values: {stats['sample_values'][:3]}\")\n",
    "    \n",
    "    print(\"\\nAura Symptoms:\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    aura_entities = ['visual', 'sensory', 'dysphasia', 'dysarthria']\n",
    "    \n",
    "    for field in aura_entities:\n",
    "        stats = entity_stats[field]\n",
    "        print(f\"   {field:20s}: {stats['present']:4d}/{len(summaries)} ({stats['rate']:5.1f}%)\")\n",
    "    \n",
    "    print(\"\\nBrainstem Symptoms:\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    brainstem_entities = ['vertigo', 'tinnitus', 'hypoacusis', 'diplopia', 'ataxia', 'conscience']\n",
    "    \n",
    "    for field in brainstem_entities:\n",
    "        stats = entity_stats[field]\n",
    "        print(f\"   {field:20s}: {stats['present']:4d}/{len(summaries)} ({stats['rate']:5.1f}%)\")\n",
    "    \n",
    "    # ==========================================================================\n",
    "    # 3. ANALYZE WHY \"NO DIAGNOSIS\" HAPPENS\n",
    "    # ==========================================================================\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ANALYZING 'NO DIAGNOSIS' CASES\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(\"\\nChecking ICHD-3 1.1 criteria for each patient...\")\n",
    "    \n",
    "    criteria_failures = defaultdict(int)\n",
    "    \n",
    "    for summary in summaries:\n",
    "        patient_id = summary.get('patient_id')\n",
    "        \n",
    "        # Check Criterion B: Duration 4-72 hours\n",
    "        duration = summary.get('duration')\n",
    "        duration_ok = False\n",
    "        if duration:\n",
    "            try:\n",
    "                dur_val = float(duration)\n",
    "                duration_ok = 4 <= dur_val <= 72\n",
    "            except:\n",
    "                duration_ok = True  # Assume OK if can't parse\n",
    "        else:\n",
    "            duration_ok = True  # Assume OK if missing\n",
    "        \n",
    "        if not duration_ok:\n",
    "            criteria_failures['Duration not 4-72 hours'] += 1\n",
    "            continue\n",
    "        \n",
    "        # Check Criterion C: At least 2 pain characteristics\n",
    "        pain_count = 0\n",
    "        \n",
    "        location = summary.get('location')\n",
    "        if location and str(location).lower() not in ['not found', 'none', '']:\n",
    "            if any(term in str(location).lower() for term in ['left', 'right', 'side', 'temple', 'unilateral']):\n",
    "                pain_count += 1\n",
    "        \n",
    "        character = summary.get('character')\n",
    "        if character and str(character).lower() not in ['not found', 'none', '']:\n",
    "            if any(term in str(character).lower() for term in ['throb', 'puls', 'pound', 'beat']):\n",
    "                pain_count += 1\n",
    "        \n",
    "        intensity = summary.get('intensity')\n",
    "        if intensity:\n",
    "            try:\n",
    "                int_val = float(intensity)\n",
    "                if int_val >= 2:  # Moderate or severe\n",
    "                    pain_count += 1\n",
    "            except:\n",
    "                if any(term in str(intensity).lower() for term in ['moderate', 'severe', 'bad']):\n",
    "                    pain_count += 1\n",
    "        \n",
    "        if pain_count < 2:\n",
    "            criteria_failures[f'Only {pain_count} pain characteristics'] += 1\n",
    "            continue\n",
    "        \n",
    "        # Check Criterion D: Migraine symptoms\n",
    "        nausea = summary.get('nausea')\n",
    "        vomit = summary.get('vomit')\n",
    "        photophobia = summary.get('photophobia')\n",
    "        phonophobia = summary.get('phonophobia')\n",
    "        \n",
    "        has_nausea_vomit = False\n",
    "        if nausea and str(nausea) not in ['0', 'Not found', 'None', '']:\n",
    "            has_nausea_vomit = True\n",
    "        if vomit and str(vomit) not in ['0', 'Not found', 'None', '']:\n",
    "            has_nausea_vomit = True\n",
    "        \n",
    "        has_both_phobias = False\n",
    "        if (photophobia and str(photophobia) not in ['0', 'Not found', 'None', ''] and\n",
    "            phonophobia and str(phonophobia) not in ['0', 'Not found', 'None', '']):\n",
    "            has_both_phobias = True\n",
    "        \n",
    "        if not (has_nausea_vomit or has_both_phobias):\n",
    "            criteria_failures['Missing migraine symptoms (D)'] += 1\n",
    "            continue\n",
    "    \n",
    "    print(\"\\n Criteria Failure Reasons:\")\n",
    "    print(\"-\" * 80)\n",
    "    for reason, count in sorted(criteria_failures.items(), key=lambda x: x[1], reverse=True):\n",
    "        pct = count / len(summaries) * 100\n",
    "        print(f\"   {reason:50s}: {count:4d} ({pct:5.1f}%)\")\n",
    "    \n",
    "    # ==========================================================================\n",
    "    # 4. SAMPLE PATIENT ANALYSIS\n",
    "    # ==========================================================================\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"SAMPLE PATIENT DETAILS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(\"\\n Patient 1 (Full Details):\")\n",
    "    print(\"-\" * 80)\n",
    "    patient1 = summaries[0]\n",
    "    for key, value in patient1.items():\n",
    "        print(f\"   {key:20s}: {value}\")\n",
    "    \n",
    "    print(\"\\n Patient 2 (Full Details):\")\n",
    "    print(\"-\" * 80)\n",
    "    patient2 = summaries[1]\n",
    "    for key, value in patient2.items():\n",
    "        print(f\"   {key:20s}: {value}\")\n",
    "    \n",
    "    return entity_stats, criteria_failures\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN\n",
    "# =============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Step 1: Add DPF to summaries\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"STEP 1: ADD DPF TO PATIENT SUMMARIES\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    summaries_with_dpf = add_dpf_to_summaries(\n",
    "        summaries_file='data/ner_results/patient_summaries.json',\n",
    "        original_data='data/migraine_with_id.csv',\n",
    "        output_file='data/ner_results/patient_summaries_with_dpf.json'\n",
    "    )\n",
    "    \n",
    "    # Step 2: Analyze entity quality\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"STEP 2: ANALYZE ENTITY QUALITY\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    entity_stats, criteria_failures = analyze_entity_quality(\n",
    "        summaries_file='data/ner_results/patient_summaries_with_dpf.json'\n",
    "    )\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ANALYSIS COMPLETE!\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(\"\\nRECOMMENDATIONS:\")\n",
    "    print(\"-\" * 80)\n",
    "    print(\"1. Check entity extraction quality for:\")\n",
    "    for field, stats in entity_stats.items():\n",
    "        if stats['rate'] < 50:\n",
    "            print(f\"   - {field}: Only {stats['rate']:.1f}% extracted\")\n",
    "    \n",
    "    print(\"\\n2. Main diagnosis failure reasons:\")\n",
    "    for reason, count in list(criteria_failures.items())[:3]:\n",
    "        print(f\"   - {reason}: {count} patients\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eccaf89b-75fe-449b-a56a-43f682c461af",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from collections import Counter, defaultdict\n",
    "import numpy as np\n",
    "\n",
    "# =============================================================================\n",
    "# INTENSITY NORMALIZATION\n",
    "# =============================================================================\n",
    "\n",
    "def normalize_intensity(intensity_text):\n",
    " \n",
    "    if not intensity_text or intensity_text in ['Not found', 'Not specified', 'None', '']:\n",
    "        return 0\n",
    "    \n",
    "    text = str(intensity_text).lower()\n",
    "    \n",
    "    # CRITICAL: Exclude duration patterns\n",
    "    if re.search(r'\\b(?:last|lasted|been|for)\\s+\\d+[-\\s]*\\d*\\s*(?:hour|hr)', text):\n",
    "        return 0  # This is duration, not intensity!\n",
    "    \n",
    "    # Extract numeric rating\n",
    "    rating_match = re.search(r'(\\d+)\\s*/\\s*10', text)\n",
    "    if rating_match:\n",
    "        rating = int(rating_match.group(1))\n",
    "        if 1 <= rating <= 4:\n",
    "            return 1\n",
    "        elif 5 <= rating <= 6:\n",
    "            return 2\n",
    "        elif 7 <= rating <= 10:\n",
    "            return 3\n",
    "    \n",
    "    # Severe keywords\n",
    "    if any(kw in text for kw in ['severe', 'terrible', 'awful', 'horrible', 'intense',\n",
    "                                   'extreme', 'unbearable', 'excruciating', 'worst']):\n",
    "        return 3\n",
    "    \n",
    "    # Moderate keywords\n",
    "    if any(kw in text for kw in ['moderate', 'medium', 'average', 'bad', 'uncomfortable']):\n",
    "        return 2\n",
    "    \n",
    "    # Mild keywords  \n",
    "    if any(kw in text for kw in ['mild', 'light', 'minor', 'slight', 'tolerable']):\n",
    "        return 1\n",
    "    \n",
    "    return 0\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# DURATION EXTRACTION\n",
    "# =============================================================================\n",
    "\n",
    "def extract_duration_with_patterns(text):\n",
    "    \n",
    "    if not text or text in ['Not found', 'Not specified', 'None', '']:\n",
    "        return None\n",
    "    \n",
    "    text_lower = str(text).lower()\n",
    "    \n",
    "    patterns = [\n",
    "        r'\\blasted\\s+(\\d+(?:\\.\\d+)?)\\s*[-–]\\s*(\\d+(?:\\.\\d+)?)\\s*(?:hour|hr)s?\\b',\n",
    "        r'\\blasted\\s+(?:for\\s+)?(\\d+(?:\\.\\d+)?)\\s*(?:hour|hr)s?\\b',\n",
    "        r'\\bduration\\s*:?\\s*(\\d+(?:\\.\\d+)?)\\s*(?:hour|hr)s?\\b',\n",
    "        r'\\b(\\d+(?:\\.\\d+)?)\\s*(?:hour|hr)s?\\s*(?:long|episode)\\b',\n",
    "        r'\\bfor\\s+(\\d+(?:\\.\\d+)?)\\s*(?:hour|hr)s?\\b',\n",
    "        r'\\babout\\s+(\\d+(?:\\.\\d+)?)\\s*(?:hour|hr)s?\\b',\n",
    "        r'\\b(been|being)\\s+(?:like\\s+this\\s+)?for\\s+(\\d+(?:\\.\\d+)?)\\s*(?:hour|hr)s?\\b',\n",
    "        r'\\bgoing\\s+on\\s+(\\d+(?:\\.\\d+)?)\\s*(?:hour|hr)s?\\b',\n",
    "        r'\\b(\\d+(?:\\.\\d+)?)\\s*(?:hour|hr)s?\\s+now\\b',\n",
    "    ]\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, text_lower)\n",
    "        if match:\n",
    "            if len(match.groups()) == 2 and match.group(2):\n",
    "                start = float(match.group(1))\n",
    "                end = float(match.group(2))\n",
    "                return (start + end) / 2\n",
    "            else:\n",
    "                for g in match.groups():\n",
    "                    if g and g.replace('.', '').isdigit():\n",
    "                        return float(g)\n",
    "    \n",
    "    if re.search(r'\\ball\\s+day\\b', text_lower):\n",
    "        return 12.0\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# FREQUENCY CALCULATION\n",
    "# =============================================================================\n",
    "\n",
    "def calculate_frequency_from_extractions(day_entities_list):\n",
    "    \n",
    "    attack_count = 0\n",
    "    \n",
    "    for day_entities in day_entities_list:\n",
    "        if not day_entities:\n",
    "            continue\n",
    "        \n",
    "        has_headache = False\n",
    "        \n",
    "        # Check for pain characteristics\n",
    "        if day_entities.get('intensity') not in ['Not found', 'Not specified', None, '']:\n",
    "            has_headache = True\n",
    "        \n",
    "        if day_entities.get('character') not in ['Not found', 'Not specified', None, '']:\n",
    "            has_headache = True\n",
    "        \n",
    "        if day_entities.get('location') not in ['Not found', 'Not specified', None, '']:\n",
    "            has_headache = True\n",
    "        \n",
    "        # Check for symptoms\n",
    "        symptoms = ['nausea', 'vomit', 'photophobia', 'phonophobia', 'visual', 'sensory']\n",
    "        for symptom in symptoms:\n",
    "            val = day_entities.get(symptom)\n",
    "            if val and val not in ['Not found', 'Not specified', None, '', '0']:\n",
    "                has_headache = True\n",
    "                break\n",
    "        \n",
    "        if has_headache:\n",
    "            attack_count += 1\n",
    "    \n",
    "    return attack_count\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# AGGREGATION HELPERS\n",
    "# =============================================================================\n",
    "\n",
    "def get_most_frequent(values):\n",
    "    \n",
    "    valid_values = [\n",
    "        v for v in values \n",
    "        if v and v not in ['Not found', 'Not specified', 'None', '', 'variable']\n",
    "    ]\n",
    "    \n",
    "    if not valid_values:\n",
    "        return None\n",
    "    \n",
    "    counter = Counter(valid_values)\n",
    "    return counter.most_common(1)[0][0]\n",
    "\n",
    "\n",
    "def aggregate_symptom_binary(values):\n",
    "    \n",
    "    for v in values:\n",
    "        if v and v not in ['Not found', 'Not specified', 'None', '', '0']:\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# PATIENT-LEVEL AGGREGATION\n",
    "# =============================================================================\n",
    "\n",
    "def aggregate_patient_entities(patient_id, day_entities_list):\n",
    "\n",
    "    \n",
    "    # Collect values\n",
    "    collected = defaultdict(list)\n",
    "    \n",
    "    for day_entities in day_entities_list:\n",
    "        if not day_entities:\n",
    "            continue\n",
    "        \n",
    "        for key, value in day_entities.items():\n",
    "            collected[key].append(value)\n",
    "    \n",
    "    aggregated = {'patient_id': patient_id}\n",
    "    \n",
    "    # 1. DURATION - Extract with patterns, then average\n",
    "    durations = []\n",
    "    for val in collected.get('duration', []):\n",
    "        dur = extract_duration_with_patterns(val)\n",
    "        if dur and dur > 0:\n",
    "            durations.append(dur)\n",
    "    \n",
    "    if durations:\n",
    "        aggregated['duration'] = round(np.mean(durations), 2)\n",
    "    else:\n",
    "        aggregated['duration'] = None\n",
    "    \n",
    "    # 2. INTENSITY - Normalize then average\n",
    "    intensities = []\n",
    "    for val in collected.get('intensity', []):\n",
    "        norm = normalize_intensity(val)\n",
    "        if norm > 0:\n",
    "            intensities.append(norm)\n",
    "    \n",
    "    if intensities:\n",
    "        avg_intensity = round(np.mean(intensities), 1)\n",
    "        aggregated['intensity'] = avg_intensity\n",
    "        \n",
    "        # Convert to text\n",
    "        if avg_intensity <= 1.3:\n",
    "            aggregated['intensity_text'] = 'mild'\n",
    "        elif avg_intensity <= 2.3:\n",
    "            aggregated['intensity_text'] = 'moderate'\n",
    "        else:\n",
    "            aggregated['intensity_text'] = 'severe'\n",
    "    else:\n",
    "        aggregated['intensity'] = 0\n",
    "        aggregated['intensity_text'] = 'variable'\n",
    "    \n",
    "    # 3. LOCATION - Most frequent\n",
    "    aggregated['location'] = get_most_frequent(collected.get('location', []))\n",
    "    \n",
    "    # 4. CHARACTER - Most frequent\n",
    "    aggregated['character'] = get_most_frequent(collected.get('character', []))\n",
    "    \n",
    "    # 5. FREQUENCY - Count days with headache\n",
    "    aggregated['frequency'] = calculate_frequency_from_extractions(day_entities_list)\n",
    "    \n",
    "    # 6. SYMPTOMS - Binary (1 if ever present, 0 otherwise)\n",
    "    symptom_fields = [\n",
    "        'nausea', 'vomit', 'photophobia', 'phonophobia',\n",
    "        'visual', 'sensory', 'dysphasia', 'dysarthria',\n",
    "        'vertigo', 'tinnitus', 'hypoacusis', 'diplopia',\n",
    "        'ataxia', 'conscience', 'visual_defect', 'paresthesia'\n",
    "    ]\n",
    "    \n",
    "    for symptom in symptom_fields:\n",
    "        aggregated[symptom] = aggregate_symptom_binary(collected.get(symptom, []))\n",
    "    \n",
    "    # 7. DPF (family history)\n",
    "    aggregated['dpf'] = aggregate_symptom_binary(collected.get('dpf', []))\n",
    "    \n",
    "    return aggregated\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# BATCH PROCESSING\n",
    "# =============================================================================\n",
    "\n",
    "def aggregate_all_patients(\n",
    "    ner_results_file='data/ner_results/postenhanced_entities.json',\n",
    "    output_file='data/ner_results/patient_summaries.json',\n",
    "    expected_patients=400\n",
    "):\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"PATIENT-LEVEL ENTITY AGGREGATION\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Load day-level extractions\n",
    "    print(f\"\\n1. Loading NER results from {ner_results_file}...\")\n",
    "    with open(ner_results_file, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    # Handle both list and dict formats\n",
    "    if isinstance(data, list):\n",
    "        day_results = data\n",
    "    elif isinstance(data, dict):\n",
    "        day_results = data.get('results', data.get('extractions', []))\n",
    "    else:\n",
    "        raise ValueError(f\"Unexpected data format: {type(data)}\")\n",
    "    \n",
    "    print(f\"  Loaded {len(day_results)} day-level extractions\")\n",
    "    \n",
    "    # Check if patient_id exists\n",
    "    if day_results and 'patient_id' not in day_results[0]:\n",
    "        print(f\"  ERROR: 'patient_id' field not found in entries!\")\n",
    "        print(f\"     Available fields: {list(day_results[0].keys())}\")\n",
    "        return None\n",
    "    \n",
    "    # Group by patient ID\n",
    "    print(\"\\n2. Grouping days by patient_id...\")\n",
    "    \n",
    "    patient_groups = defaultdict(list)\n",
    "    missing_patient_id = 0\n",
    "    \n",
    "    for entry in day_results:\n",
    "        # Get patient_id directly from entry\n",
    "        patient_id = entry.get('patient_id')\n",
    "        \n",
    "        if patient_id is None or patient_id == 0:\n",
    "            missing_patient_id += 1\n",
    "            continue\n",
    "        \n",
    "        # Get entities\n",
    "        entities = entry.get('entities', {})\n",
    "        \n",
    "        # Add to patient group\n",
    "        patient_groups[patient_id].append(entities)\n",
    "    \n",
    "    num_patients = len(patient_groups)\n",
    "    \n",
    "    if num_patients == 0:\n",
    "        print(f\"   ERROR: No valid patients found!\")\n",
    "        print(f\"     Entries with missing/invalid patient_id: {missing_patient_id}\")\n",
    "        return None\n",
    "    \n",
    "    # Calculate statistics\n",
    "    days_per_patient = [len(days) for days in patient_groups.values()]\n",
    "    avg_days = np.mean(days_per_patient)\n",
    "    min_days = np.min(days_per_patient)\n",
    "    max_days = np.max(days_per_patient)\n",
    "    \n",
    "    print(f\"  ✓ Grouped into {num_patients} patients\")\n",
    "    print(f\"  ℹ️  Days per patient: avg={avg_days:.1f}, min={min_days}, max={max_days}\")\n",
    "    print(f\"  ℹ️  Expected: {expected_patients} patients\")\n",
    "    \n",
    "    if missing_patient_id > 0:\n",
    "        print(f\"    Skipped {missing_patient_id} entries (missing/invalid patient_id)\")\n",
    "    \n",
    "    # Aggregate each patient\n",
    "    print(\"\\n3. Aggregating entities for each patient...\")\n",
    "    \n",
    "    patient_summaries = []\n",
    "    \n",
    "    for patient_id in sorted(patient_groups.keys()):\n",
    "        day_entities = patient_groups[patient_id]\n",
    "        \n",
    "        summary = aggregate_patient_entities(\n",
    "            patient_id=patient_id,\n",
    "            day_entities_list=day_entities\n",
    "        )\n",
    "        \n",
    "        patient_summaries.append(summary)\n",
    "        \n",
    "        # Progress\n",
    "        if len(patient_summaries) % 50 == 0:\n",
    "            print(f\"     Processed {len(patient_summaries)}/{num_patients} patients...\")\n",
    "    \n",
    "    print(f\"  ✓ Aggregated {len(patient_summaries)} patient summaries\")\n",
    "    \n",
    "    # Save results\n",
    "    print(\"\\n4. Saving patient-level summaries...\")\n",
    "    \n",
    "    output_path = Path(output_file)\n",
    "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # JSON\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(patient_summaries, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"  ✓ JSON: {output_path}\")\n",
    "    print(f\"     Entries: {len(patient_summaries)}\")\n",
    "    \n",
    "    # CSV\n",
    "    csv_path = output_path.with_suffix('.csv')\n",
    "    df = pd.DataFrame(patient_summaries)\n",
    "    df.to_csv(csv_path, index=False, encoding='utf-8')\n",
    "    \n",
    "    print(f\"  ✓ CSV: {csv_path}\")\n",
    "    print(f\"     Rows: {len(df)}\")\n",
    "    \n",
    "    # Summary statistics\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"AGGREGATION SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Day-level extractions:  {len(day_results)}\")\n",
    "    print(f\"Patient summaries:      {len(patient_summaries)}\")\n",
    "    print(f\"Days per patient (avg): {len(day_results) / len(patient_summaries):.1f}\")\n",
    "    \n",
    "    # Show sample\n",
    "    if patient_summaries:\n",
    "        print(\"\\n📊 Sample Patient Summary:\")\n",
    "        print(\"-\" * 80)\n",
    "        sample = patient_summaries[0]\n",
    "        for key, value in sample.items():\n",
    "            print(f\"  {key:20s}: {value}\")\n",
    "    \n",
    "    # Statistics\n",
    "    print(\"\\n📊 Aggregated Statistics:\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    print(f\"Duration:\")\n",
    "    print(f\"  Mean:    {df['duration'].mean():.2f} hours\")\n",
    "    print(f\"  Std:     {df['duration'].std():.2f} hours\")\n",
    "    print(f\"  Missing: {df['duration'].isna().sum()} patients\")\n",
    "    \n",
    "    print(f\"\\nIntensity:\")\n",
    "    print(f\"  Mean:         {df['intensity'].mean():.2f}\")\n",
    "    print(f\"  Distribution: {df['intensity_text'].value_counts().to_dict()}\")\n",
    "    \n",
    "    print(f\"\\nFrequency:\")\n",
    "    print(f\"  Mean: {df['frequency'].mean():.1f} attacks/month\")\n",
    "    print(f\"  Min:  {df['frequency'].min()}\")\n",
    "    print(f\"  Max:  {df['frequency'].max()}\")\n",
    "    \n",
    "    print(f\"\\nLocation:\")\n",
    "    location_counts = df['location'].value_counts()\n",
    "    for loc, count in location_counts.head(5).items():\n",
    "        print(f\"  {loc}: {count}\")\n",
    "    \n",
    "    print(\"\\n✅ PATIENT-LEVEL AGGREGATION COMPLETE!\")\n",
    "    \n",
    "    return patient_summaries\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN\n",
    "# =============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    aggregate_all_patients(\n",
    "        ner_results_file='data/ner_results/postenhanced_entities.json',\n",
    "        output_file='data/ner_results/patient_summaries.json',\n",
    "        expected_patients=400\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
