{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6561ef6-4795-4c5f-a622-77f8eb7c1c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"/Users/M1HR/Desktop/MIGRAINE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5b7a3f-351a-4d97-baef-8ab8b9950ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install frozendict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1317740-9010-43b4-adf2-38d436ec0626",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install experta pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44dd95c8-d00e-4371-ac1a-0342e99b616e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade jsonpickle pyyaml nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93380e35-25ea-42e8-8310-0934dbc5950e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from collections import Counter, defaultdict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# HELPER FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "def check_present(value):\n",
    "  \n",
    "    if not value:\n",
    "        return False\n",
    "    \n",
    "    # Numeric: 0 = absent, >0 = present\n",
    "    if isinstance(value, (int, float)):\n",
    "        return value > 0\n",
    "    \n",
    "    # String: check for common \"absent\" values\n",
    "    value_str = str(value).lower()\n",
    "    return value_str not in ['not found', 'not specified', 'none', '', '0']\n",
    "\n",
    "\n",
    "def check_base_migraine_criteria(patient, verbose=False):\n",
    "\n",
    "    pain_count = 0\n",
    "    pain_details = []\n",
    "  \n",
    "    # ------------------------------------------------------\n",
    "    # 1. Unilateral location\n",
    "    # ------------------------------------------------------\n",
    "    location = patient.get('location')\n",
    "    unilateral = False\n",
    "    \n",
    "    if location:\n",
    "        loc_str = str(location).lower()\n",
    "        # terms that imply \"one side\"\n",
    "        unilateral_terms = ['left', 'right', 'side', 'temple', 'unilateral']\n",
    "        if any(term in loc_str for term in unilateral_terms):\n",
    "            unilateral = True\n",
    "            pain_count += 1\n",
    "            pain_details.append('unilateral')\n",
    "    \n",
    "    # ------------------------------------------------------\n",
    "    # 2. Pulsating quality\n",
    "    # ------------------------------------------------------\n",
    "    character = patient.get('character')\n",
    "    pulsating = False\n",
    "    \n",
    "    if character:\n",
    "        char_str = str(character).lower()\n",
    "        puls_terms = ['throb', 'puls', 'pound', 'beat']\n",
    "        if any(term in char_str for term in puls_terms):\n",
    "            pulsating = True\n",
    "            pain_count += 1\n",
    "            pain_details.append('pulsating')\n",
    "    \n",
    "    # ------------------------------------------------------\n",
    "    # 3. Intensity: moderate or severe\n",
    "    # ------------------------------------------------------\n",
    "    moderate_severe = False\n",
    "    \n",
    "    # Text intensity (preferred)\n",
    "    int_text = patient.get('intensity_text')\n",
    "    if int_text:\n",
    "        it = str(int_text).lower()\n",
    "        if 'moderate' in it or 'severe' in it:\n",
    "            moderate_severe = True\n",
    "            pain_details.append(f'moderate_severe_text({int_text})')\n",
    "    \n",
    "    # Numeric intensity (fallback)\n",
    "    if not moderate_severe:\n",
    "        int_val = patient.get('intensity')\n",
    "        try:\n",
    "            if int_val is not None and float(int_val) >= 2:  # 2=moderate, 3=severe\n",
    "                moderate_severe = True\n",
    "                pain_details.append(f'moderate_severe_numeric({int_val})')\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    if moderate_severe:\n",
    "        pain_count += 1\n",
    "    \n",
    "    # ------------------------------------------------------\n",
    "    # Final evaluation\n",
    "    # Need >= 2 of 3 characteristics\n",
    "    # ------------------------------------------------------\n",
    "    meets_criterion = pain_count >= 2\n",
    "    \n",
    "    if verbose:\n",
    "        if meets_criterion:\n",
    "            print(f\"  ‚úì Pain characteristics: {pain_count}/3 {pain_details}\")\n",
    "        else:\n",
    "            print(f\"  ‚úó Pain characteristics: {pain_count}/3 {pain_details}\")\n",
    "    \n",
    "\n",
    "    # =========================================================================\n",
    "    # CRITERION 2: Frequency >= 5 attacks (CHANGED from >=5)\n",
    "    # =========================================================================\n",
    "    frequency = patient.get('frequency')\n",
    "    \n",
    "    if frequency:\n",
    "        try:\n",
    "            freq_val = int(frequency)\n",
    "            if freq_val < 5:\n",
    "                reason = f\"Frequency: {freq_val} (need >=4)\"\n",
    "                if verbose:\n",
    "                    print(f\"  ‚úó {reason}\")\n",
    "                return False, reason\n",
    "            \n",
    "            if verbose:\n",
    "                print(f\"  ‚úì Frequency: {freq_val}\")\n",
    "        except:\n",
    "            if verbose:\n",
    "                print(f\"    Frequency parse failed, assuming valid\")\n",
    "    else:\n",
    "        if verbose:\n",
    "            print(f\"    Frequency not specified, assuming valid\")\n",
    "    \n",
    "    # =========================================================================\n",
    "    # CRITERION 3: Accompanying symptoms (need >= 1)\n",
    "    # =========================================================================\n",
    "    nausea = patient.get('nausea')\n",
    "    vomit = patient.get('vomit')\n",
    "    photophobia = patient.get('photophobia')\n",
    "    phonophobia = patient.get('phonophobia')\n",
    "    \n",
    "    has_symptoms = (\n",
    "        check_present(nausea) or\n",
    "        check_present(vomit) or\n",
    "        check_present(photophobia) or\n",
    "        check_present(phonophobia)\n",
    "    )\n",
    "    \n",
    "    if not has_symptoms:\n",
    "        reason = \"No accompanying symptoms\"\n",
    "        if verbose:\n",
    "            print(f\"  ‚úó {reason}\")\n",
    "        return False, reason\n",
    "    \n",
    "    symptom_list = []\n",
    "    if check_present(nausea): symptom_list.append('nausea')\n",
    "    if check_present(vomit): symptom_list.append('vomit')\n",
    "    if check_present(photophobia): symptom_list.append('photophobia')\n",
    "    if check_present(phonophobia): symptom_list.append('phonophobia')\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"  ‚úì Accompanying symptoms: {symptom_list}\")\n",
    "    \n",
    "    return True, \"Base criteria met\"\n",
    "\n",
    "def count_ha_symptoms(patient):\n",
    "    \n",
    "    count = 0\n",
    "    types = []\n",
    "    \n",
    "    ha = [\n",
    "        'location', 'intensity'\n",
    "    ]\n",
    "    \n",
    "    for field in ha:\n",
    "        if check_present(patient.get(field)):\n",
    "            count += 1\n",
    "            types.append(field)\n",
    "    \n",
    "    return count, types\n",
    "    \n",
    "def count_aura_symptoms(patient):\n",
    "    \n",
    "    count = 0\n",
    "    types = []\n",
    "    \n",
    "    aura_fields = [\n",
    "        'visual', 'sensory', 'dysphasia', 'dysarthria',\n",
    "        'vertigo', 'tinnitus', 'hypoacusis', 'diplopia',\n",
    "        'ataxia', 'conscience', 'visual_defect', 'paresthesia'\n",
    "    ]\n",
    "    \n",
    "    for field in aura_fields:\n",
    "        if check_present(patient.get(field)):\n",
    "            count += 1\n",
    "            types.append(field)\n",
    "    \n",
    "    return count, types\n",
    "\n",
    "\n",
    "def count_brainstem_symptoms(patient):\n",
    "   \n",
    "    count = 0\n",
    "    types = []\n",
    "    \n",
    "    brainstem_fields = [\n",
    "        'dysarthria', 'vertigo', 'tinnitus', 'hypoacusis',\n",
    "        'diplopia', 'ataxia', 'conscience'\n",
    "    ]\n",
    "    \n",
    "    for field in brainstem_fields:\n",
    "        if check_present(patient.get(field)):\n",
    "            count += 1\n",
    "            types.append(field)\n",
    "    \n",
    "    return count, types\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN DIAGNOSTIC FUNCTION\n",
    "# =============================================================================\n",
    "\n",
    "def diagnose_patient(patient, verbose=False):\n",
    "   \n",
    "    \n",
    "    patient_id = patient.get('patient_id')\n",
    "    reasoning = []\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"DIAGNOSING PATIENT {patient_id}\")\n",
    "        print(f\"{'='*80}\")\n",
    "    \n",
    "    # =========================================================================\n",
    "    # CHECK BASE CRITERIA\n",
    "    # =========================================================================\n",
    "    meets_base, base_reason = check_base_migraine_criteria(patient, verbose=verbose)\n",
    "    \n",
    "    if verbose:\n",
    "        if meets_base:\n",
    "            reasoning.append(\"‚úì Base migraine criteria met\")\n",
    "        else:\n",
    "            reasoning.append(f\"‚úó Base migraine criteria failed: {base_reason}\")\n",
    "    \n",
    "    # =========================================================================\n",
    "    # COUNT SYMPTOMS\n",
    "    # =========================================================================\n",
    "    aura_count, aura_types = count_aura_symptoms(patient)\n",
    "    brainstem_count, brainstem_types = count_brainstem_symptoms(patient)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"  Aura symptoms: {aura_count} ({aura_types[:5]}...)\" if aura_count > 5 else f\"  Aura symptoms: {aura_count} {aura_types}\")\n",
    "        print(f\"  Brainstem symptoms: {brainstem_count} {brainstem_types}\")\n",
    "    \n",
    "    reasoning.append(f\"Aura: {aura_count}, Brainstem: {brainstem_count}\")\n",
    "    \n",
    "    # Get other relevant fields\n",
    "    frequency = patient.get('frequency', 0)\n",
    "    dpf = patient.get('dpf', 0)\n",
    "\n",
    "    #count HA\n",
    "    ha_count, ha_types = count_ha_symptoms(patient)\n",
    "    # =========================================================================\n",
    "    # DIAGNOSTIC RULES (NON-OVERLAPPING - in priority order)\n",
    "    # =========================================================================\n",
    "    \n",
    "    # Get dysphasia once for all rules\n",
    "    dysphasia = patient.get('dysphasia')\n",
    "    has_dysphasia = check_present(dysphasia)\n",
    "    \n",
    "    # Rule 1: Familial hemiplegic migraine\n",
    "    # - Base criteria met\n",
    "    # - dysphasia >= 1 (ONLY dysphasia)\n",
    "    # - DPF = 1 (family history present)\n",
    "    if has_dysphasia and (check_present(dpf) or int(dpf) == 1) and brainstem_count <2:\n",
    "        diagnosis = \"Familial hemiplegic migraine\"\n",
    "        code = \"ICHD-3 1.2.3.1\"\n",
    "        confidence = \"high\"\n",
    "        reasoning.append(f\"‚úì {diagnosis}: dysphasia + DPF({int(dpf)})=1 + brainstem({brainstem_count})<2\")\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"\\n  ‚Üí DIAGNOSIS: {diagnosis}\")\n",
    "        \n",
    "        return {\n",
    "            'patient_id': patient_id,\n",
    "            'diagnosis': diagnosis,\n",
    "            'code': code,\n",
    "            'confidence': confidence,\n",
    "            'reasoning': reasoning\n",
    "        }\n",
    "    \n",
    "    # Rule 2: Sporadic hemiplegic migraine\n",
    "    # - Base criteria met\n",
    "    # - dysphasia >0 (ONLY dysphasia)\n",
    "    # - DPF = 0 (no family history)\n",
    "    dpf_zero = not check_present(dpf) or int(dpf) == 0\n",
    "    \n",
    "    if has_dysphasia and dpf_zero and brainstem_count <2:\n",
    "        diagnosis = \"Sporadic hemiplegic migraine\"\n",
    "        code = \"ICHD-3 1.2.3.2\"\n",
    "        confidence = \"high\"\n",
    "        reasoning.append(f\"‚úì {diagnosis}: dysphasia + DPF({int(dpf)})=0 +brainstem({brainstem_count})<2\")\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"\\n  ‚Üí DIAGNOSIS: {diagnosis}\")\n",
    "        \n",
    "        return {\n",
    "            'patient_id': patient_id,\n",
    "            'diagnosis': diagnosis,\n",
    "            'code': code,\n",
    "            'confidence': confidence,\n",
    "            'reasoning': reasoning\n",
    "        }\n",
    "    \n",
    "    # Rule 3: Basilar-type aura\n",
    "    # - Base criteria met\n",
    "    # - dysphasia == 0 (NO dysphasia)\n",
    "    if not has_dysphasia and brainstem_count > 1:\n",
    "        diagnosis = \"Basilar-type aura\"\n",
    "        code = \"ICHD-3 1.2.2\"\n",
    "        confidence = \"high\"\n",
    "        reasoning.append(f\"‚úì {diagnosis}: brainstem({brainstem_count})>1\")\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"\\n  ‚Üí DIAGNOSIS: {diagnosis}\")\n",
    "        \n",
    "        return {\n",
    "            'patient_id': patient_id,\n",
    "            'diagnosis': diagnosis,\n",
    "            'code': code,\n",
    "            'confidence': confidence,\n",
    "            'reasoning': reasoning\n",
    "        }\n",
    "    \n",
    "    # Rule 4: Typical aura with headache\n",
    "    # - Base criteria met\n",
    "    # - dysphasia == 0 (NO dysphasia)\n",
    "    # - brainstem ==0 (not basilar)\n",
    "    if ha_count >0 and aura_count >= 1 and not has_dysphasia and brainstem_count <=1:\n",
    "        diagnosis = \"Typical aura with migraine\"\n",
    "        code = \"ICHD-3 1.2.1.1\"\n",
    "        confidence = \"high\"\n",
    "        reasoning.append(f\"‚úì {diagnosis}: aura({aura_count})>0 + headache({ha_count})>0 + no dysphasia + brainstem({brainstem_count})<=1\")\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"\\n  ‚Üí DIAGNOSIS: {diagnosis}\")\n",
    "        \n",
    "        return {\n",
    "            'patient_id': patient_id,\n",
    "            'diagnosis': diagnosis,\n",
    "            'code': code,\n",
    "            'confidence': confidence,\n",
    "            'reasoning': reasoning\n",
    "        }\n",
    "    \n",
    "    # Rule 5: Typical aura without migraine\n",
    "    # - Does NOT meet base criteria\n",
    "    # - aura >= 1\n",
    "    # - frequency = 0 (no headache attacks)\n",
    "    # - dysphasia == 0 (NO dysphasia)\n",
    "    # - brainstem ==0 (not basilar)\n",
    "    if ha_count ==0 and aura_count >= 1 and not has_dysphasia and brainstem_count <=1:\n",
    "        try:\n",
    "            freq_val = int(frequency)\n",
    "            if freq_val <= 3:\n",
    "                diagnosis = \"Typical aura without migraine\"\n",
    "                code = \"ICHD-3 1.2.1.2\"\n",
    "                confidence = \"high\"\n",
    "                reasoning.append(f\"‚úì {diagnosis}: aura({aura_count})>0 + headache({ha_count})=0 + no dysphasia + brainstem({brainstem_count})<=1\")\n",
    "                \n",
    "                if verbose:\n",
    "                    print(f\"\\n  ‚Üí DIAGNOSIS: {diagnosis}\")\n",
    "                \n",
    "                return {\n",
    "                    'patient_id': patient_id,\n",
    "                    'diagnosis': diagnosis,\n",
    "                    'code': code,\n",
    "                    'confidence': confidence,\n",
    "                    'reasoning': reasoning\n",
    "                }\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # Rule 6: Migraine without aura\n",
    "    # - Base criteria met\n",
    "    # - aura ==0\n",
    "    # - brainstem ==0 (not basilar)\n",
    "    # - dysphasia ==0\n",
    "    \n",
    "    if meets_base and aura_count == 0:\n",
    "        diagnosis = \"Migraine without aura\"\n",
    "        code = \"ICHD-3 1.1\"\n",
    "        confidence = \"high\"\n",
    "        reasoning.append(f\"‚úì {diagnosis}: Base + aura({aura_count})=0 \")\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"\\n  ‚Üí DIAGNOSIS: {diagnosis}\")\n",
    "        \n",
    "        return {\n",
    "            'patient_id': patient_id,\n",
    "            'diagnosis': diagnosis,\n",
    "            'code': code,\n",
    "            'confidence': confidence,\n",
    "            'reasoning': reasoning\n",
    "        }\n",
    "    \n",
    "    # Rule 6: Other\n",
    "    # - All remainder cases\n",
    "    diagnosis = \"Other\"\n",
    "    code = \"N/A\"\n",
    "    confidence = \"low\"\n",
    "    \n",
    "    if not meets_base:\n",
    "        reasoning.append(f\"‚úó {diagnosis}: Base criteria not met\")\n",
    "    else:\n",
    "        reasoning.append(f\"‚úó {diagnosis}: No specific criteria matched\")\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\n  ‚Üí DIAGNOSIS: {diagnosis}\")\n",
    "    \n",
    "    return {\n",
    "        'patient_id': patient_id,\n",
    "        'diagnosis': diagnosis,\n",
    "        'code': code,\n",
    "        'confidence': confidence,\n",
    "        'reasoning': reasoning\n",
    "    }\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# BATCH PROCESSING\n",
    "# =============================================================================\n",
    "\n",
    "def batch_diagnose(\n",
    "    summaries_file='data/ner_results/patient_summaries_fixed.json',\n",
    "    output_file='data/diagnoses/ichd3_diagnoses_final.json',\n",
    "    ground_truth_file=None\n",
    "):\n",
    "    \n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ICHD-3 SYMBOLIC REASONING ENGINE - BATCH DIAGNOSIS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Load patient summaries\n",
    "    print(f\"\\n1. Loading patient summaries from {summaries_file}...\")\n",
    "    with open(summaries_file, 'r', encoding='utf-8') as f:\n",
    "        patients = json.load(f)\n",
    "    \n",
    "    print(f\"   ‚úì Loaded {len(patients)} patients\")\n",
    "    \n",
    "    # Diagnose all patients\n",
    "    print(f\"\\n2. Running diagnostic engine...\")\n",
    "    \n",
    "    all_results = []\n",
    "    diagnosis_counts = Counter()\n",
    "    \n",
    "    for i, patient in enumerate(patients):\n",
    "        if (i + 1) % 50 == 0:\n",
    "            print(f\"   Progress: {i+1}/{len(patients)}...\")\n",
    "        \n",
    "        result = diagnose_patient(patient, verbose=False)\n",
    "        all_results.append(result)\n",
    "        diagnosis_counts[result['diagnosis']] += 1\n",
    "    \n",
    "    print(f\"\\n   ‚úì Diagnosed {len(all_results)} patients\")\n",
    "    \n",
    "    # Save results\n",
    "    print(f\"\\n3. Saving results to {output_file}...\")\n",
    "    \n",
    "    output_path = Path(output_file)\n",
    "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Save JSON (full details)\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(all_results, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    # Save CSV (summary)\n",
    "    csv_data = [{\n",
    "        'patient_id': r['patient_id'],\n",
    "        'diagnosis': r['diagnosis'],\n",
    "        'code': r['code'],\n",
    "        'confidence': r['confidence']\n",
    "    } for r in all_results]\n",
    "    \n",
    "    df = pd.DataFrame(csv_data)\n",
    "    csv_path = output_path.with_suffix('.csv')\n",
    "    df.to_csv(csv_path, index=False, encoding='utf-8')\n",
    "    \n",
    "    print(f\"   ‚úì JSON: {output_path}\")\n",
    "    print(f\"   ‚úì CSV: {csv_path}\")\n",
    "    \n",
    "    # Display statistics\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"DIAGNOSIS DISTRIBUTION\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(f\"\\nResults:\")\n",
    "    for diagnosis, count in diagnosis_counts.most_common():\n",
    "        percentage = count / len(all_results) * 100\n",
    "        print(f\"   {diagnosis:40s}: {count:4d} ({percentage:5.1f}%)\")\n",
    "    \n",
    "    # Validate against ground truth if provided\n",
    "    if ground_truth_file:\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"VALIDATION AGAINST GROUND TRUTH\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        try:\n",
    "            gt_df = pd.read_csv(ground_truth_file)\n",
    "            print(f\"\\n‚úì Loaded ground truth: {len(gt_df)} patients\")\n",
    "            \n",
    "            # Create mapping\n",
    "            gt_map = {}\n",
    "            for idx, row in gt_df.iterrows():\n",
    "                patient_id = idx + 1\n",
    "                gt_map[patient_id] = row.get('Type', 'Unknown')\n",
    "            \n",
    "            # Compare\n",
    "            matches = 0\n",
    "            mismatches = 0\n",
    "            mismatch_details = defaultdict(int)\n",
    "            \n",
    "            for result in all_results:\n",
    "                pid = result['patient_id']\n",
    "                predicted = result['diagnosis']\n",
    "                actual = gt_map.get(pid, 'Unknown')\n",
    "                \n",
    "                # Normalize for comparison\n",
    "                predicted_norm = predicted.lower()\n",
    "                actual_norm = actual.lower()\n",
    "                \n",
    "                if predicted_norm == actual_norm:\n",
    "                    matches += 1\n",
    "                elif ('without aura' in predicted_norm and 'without aura' in actual_norm) or \\\n",
    "                     ('with aura' in predicted_norm and 'with aura' in actual_norm) or \\\n",
    "                     ('hemiplegic' in predicted_norm and 'hemiplegic' in actual_norm) or \\\n",
    "                     ('basilar' in predicted_norm and 'basilar' in actual_norm):\n",
    "                    matches += 1\n",
    "                else:\n",
    "                    mismatches += 1\n",
    "                    mismatch_details[f\"{actual} ‚Üí {predicted}\"] += 1\n",
    "            \n",
    "            total = matches + mismatches\n",
    "            accuracy = matches / total * 100 if total > 0 else 0\n",
    "            \n",
    "            print(f\"\\n Accuracy:\")\n",
    "            print(f\"   Total:      {total}\")\n",
    "            print(f\"   Matches:    {matches}\")\n",
    "            print(f\"   Mismatches: {mismatches}\")\n",
    "            print(f\"   Accuracy:   {accuracy:.2f}%\")\n",
    "            \n",
    "            if mismatches > 0:\n",
    "                print(f\"\\n Top mismatches:\")\n",
    "                for error, count in sorted(mismatch_details.items(), key=lambda x: x[1], reverse=True)[:10]:\n",
    "                    print(f\"   {error}: {count}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"\\n‚ö†Ô∏è  Validation error: {e}\")\n",
    "    \n",
    "    # Show sample diagnoses\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"SAMPLE DIAGNOSES\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    for i in range(min(3, len(patients))):\n",
    "        diagnose_patient(patients[i], verbose=True)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\" BATCH DIAGNOSIS COMPLETE!\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN ENTRY POINT\n",
    "# =============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import sys\n",
    "\n",
    "    \n",
    "    results = batch_diagnose(\n",
    "        summaries_file='data/ner_results/patient_summaries_fixed.json',\n",
    "        output_file='data/diagnoses/ichd3_diagnoses_final.json',\n",
    "        ground_truth_file='data/migraine_with_id.csv'\n",
    "    )\n",
    "        \n",
    "    print(f\"\\n Diagnosed {len(results)} patients successfully!\")\n",
    "    \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb243976-44f3-4c0b-b5e5-ffff5fb48817",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def run_engine_on_csv(csv_path, output_path=\"engine_output.csv\", verbose=False):\n",
    "    \"\"\"\n",
    "    Run your existing diagnose_patient() on every row in a CSV.\n",
    "    Includes ground truth comparison using the 'Type' column.\n",
    "    \"\"\"\n",
    "\n",
    "    df = pd.read_csv(csv_path)\n",
    "    print(f\"\\nLoaded {len(df)} patients from {csv_path}\")\n",
    "\n",
    "    # Check if Type column exists\n",
    "    if \"Type\" not in df.columns:\n",
    "        raise ValueError(\"CSV missing required ground truth column: 'Type'\")\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for i, row in df.iterrows():\n",
    "        patient_dict = row.to_dict()\n",
    "\n",
    "        # Normalize keys to lower-case since diagnose_patient uses lowercase\n",
    "        patient = {k.lower(): v for k, v in patient_dict.items()}\n",
    "\n",
    "        diag = diagnose_patient(patient, verbose=verbose)\n",
    "\n",
    "        gt = row[\"Type\"]  # Ground truth\n",
    "\n",
    "        # Compare engine diagnosis vs ground truth\n",
    "        match = (diag['diagnosis'] == gt)\n",
    "\n",
    "        results.append({\n",
    "            \"patient_id\": diag['patient_id'],\n",
    "            \"ground_truth\": gt,\n",
    "            \"diagnosis\": diag['diagnosis'],\n",
    "            \"match\": match,\n",
    "            \"code\": diag['code'],\n",
    "            \"confidence\": diag['confidence'],\n",
    "            \"reasoning\": \" | \".join(diag['reasoning']),\n",
    "        })\n",
    "\n",
    "        if (i+1) % 20 == 0:\n",
    "            print(f\"Processed {i+1}/{len(df)} patients...\")\n",
    "\n",
    "    out_df = pd.DataFrame(results)\n",
    "    out_df.to_csv(output_path, index=False)\n",
    "\n",
    "    # Quick stats\n",
    "    acc = out_df['match'].mean() * 100\n",
    "    print(f\"\\nüî• DONE! Saved output to {output_path}\")\n",
    "    print(f\"üéØ Accuracy vs ground truth: {acc:.2f}%\\n\")\n",
    "\n",
    "    return out_df\n",
    "\n",
    "\n",
    "run_engine_on_csv(\"data/migraine_with_id.csv\", verbose =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3bcff54-6404-48db-9e18-b8a53a7269b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Run ICHD-3 Diagnostic Engine on JSON Patient Summaries\n",
    "========================================================\n",
    "\n",
    "Runs diagnose_patient() on NER-extracted patient summaries (JSON format).\n",
    "Compares against ground truth from original CSV.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def run_engine_on_json(json_path, \n",
    "                       ground_truth_csv=\"data/migraine_with_id.csv\",\n",
    "                       output_path=\"engine_output_ner.csv\", \n",
    "                       verbose=False):\n",
    "    \"\"\"\n",
    "    Run diagnose_patient() on JSON patient summaries (NER-extracted data).\n",
    "    \n",
    "    Args:\n",
    "        json_path: Path to JSON file with patient summaries\n",
    "        ground_truth_csv: Path to CSV with ground truth diagnoses\n",
    "        output_path: Where to save results CSV\n",
    "        verbose: Whether to print detailed reasoning\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with results\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"RUNNING ENGINE ON NER-EXTRACTED JSON DATA\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Load JSON patient summaries\n",
    "    with open(json_path, 'r') as f:\n",
    "        patients_json = json.load(f)\n",
    "    \n",
    "    print(f\"\\n‚úì Loaded {len(patients_json)} patients from {json_path}\")\n",
    "    \n",
    "    # Load ground truth\n",
    "    gt_df = pd.read_csv(ground_truth_csv)\n",
    "    print(f\"‚úì Loaded ground truth from {ground_truth_csv}\")\n",
    "    \n",
    "    # Create ground truth mapping\n",
    "    gt_map = {}\n",
    "    for idx, row in gt_df.iterrows():\n",
    "        patient_id = idx + 1\n",
    "        gt_map[patient_id] = row.get('Type', 'Unknown')\n",
    "    \n",
    "    # Process each patient\n",
    "    results = []\n",
    "    \n",
    "    for i, patient_data in enumerate(patients_json):\n",
    "        \n",
    "        # Get patient ID\n",
    "        patient_id = patient_data.get('patient_id', i + 1)\n",
    "        \n",
    "        # Prepare patient dict for diagnose_patient()\n",
    "        # Convert JSON format to expected format (lowercase keys)\n",
    "        patient = {\n",
    "            'patient_id': patient_id,\n",
    "            'duration': patient_data.get('duration', 0),\n",
    "            'intensity': patient_data.get('intensity', 0),\n",
    "            'intensity_text': patient_data.get('intensity_text', ''),\n",
    "            'location': patient_data.get('location', ''),\n",
    "            'character': patient_data.get('character', ''),\n",
    "            'frequency': patient_data.get('frequency', 0),\n",
    "            'nausea': patient_data.get('nausea', 0),\n",
    "            'vomit': patient_data.get('vomit', 0),\n",
    "            'photophobia': patient_data.get('photophobia', 0),\n",
    "            'phonophobia': patient_data.get('phonophobia', 0),\n",
    "            'visual': patient_data.get('visual', 0),\n",
    "            'sensory': patient_data.get('sensory', 0),\n",
    "            'dysphasia': patient_data.get('dysphasia', 0),\n",
    "            'dysarthria': patient_data.get('dysarthria', 0),\n",
    "            'vertigo': patient_data.get('vertigo', 0),\n",
    "            'tinnitus': patient_data.get('tinnitus', 0),\n",
    "            'hypoacusis': patient_data.get('hypoacusis', 0),\n",
    "            'diplopia': patient_data.get('diplopia', 0),\n",
    "            'ataxia': patient_data.get('ataxia', 0),\n",
    "            'conscience': patient_data.get('conscience', 0),\n",
    "            'visual_defect': patient_data.get('visual_defect', 0),\n",
    "            'paresthesia': patient_data.get('paresthesia', 0),\n",
    "            'dpf': patient_data.get('dpf', 0),\n",
    "        }\n",
    "        \n",
    "        # Run diagnosis\n",
    "        diag = diagnose_patient(patient, verbose=verbose)\n",
    "        \n",
    "        # Get ground truth\n",
    "        gt = gt_map.get(patient_id, 'Unknown')\n",
    "        \n",
    "        # Compare\n",
    "        match = (diag['diagnosis'] == gt)\n",
    "        \n",
    "        results.append({\n",
    "            \"patient_id\": patient_id,\n",
    "            \"ground_truth\": gt,\n",
    "            \"diagnosis\": diag['diagnosis'],\n",
    "            \"match\": match,\n",
    "            \"code\": diag['code'],\n",
    "            \"confidence\": diag['confidence'],\n",
    "            \"reasoning\": \" | \".join(diag['reasoning']),\n",
    "        })\n",
    "        \n",
    "        if (i+1) % 20 == 0:\n",
    "            print(f\"Processed {i+1}/{len(patients_json)} patients...\")\n",
    "    \n",
    "    # Create DataFrame\n",
    "    out_df = pd.DataFrame(results)\n",
    "    \n",
    "    # Save to CSV\n",
    "    output_dir = Path(output_path).parent\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    out_df.to_csv(output_path, index=False)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    acc = out_df['match'].mean() * 100\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"RESULTS\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"\\n‚úì Saved output to: {output_path}\")\n",
    "    print(f\"üéØ Accuracy vs ground truth: {acc:.2f}%\")\n",
    "    print(f\"üìä Correct: {out_df['match'].sum()}/{len(out_df)}\")\n",
    "    \n",
    "    # Show diagnosis distribution\n",
    "    print(f\"\\n{'‚îÄ'*80}\")\n",
    "    print(\"Diagnosis Distribution:\")\n",
    "    print(f\"{'‚îÄ'*80}\")\n",
    "    print(\"\\nGround Truth:\")\n",
    "    print(out_df['ground_truth'].value_counts())\n",
    "    print(\"\\nPredicted:\")\n",
    "    print(out_df['diagnosis'].value_counts())\n",
    "    \n",
    "    return out_df\n",
    "\n",
    "\n",
    "def run_engine_on_csv(csv_path, output_path=\"engine_output.csv\", verbose=False):\n",
    "    \"\"\"\n",
    "    Run diagnose_patient() on CSV data (original structured format).\n",
    "    \n",
    "    Args:\n",
    "        csv_path: Path to CSV file\n",
    "        output_path: Where to save results\n",
    "        verbose: Whether to print detailed reasoning\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with results\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"RUNNING ENGINE ON ORIGINAL CSV DATA\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    df = pd.read_csv(csv_path)\n",
    "    print(f\"\\n‚úì Loaded {len(df)} patients from {csv_path}\")\n",
    "    \n",
    "    # Check if Type column exists\n",
    "    if \"Type\" not in df.columns:\n",
    "        raise ValueError(\"CSV missing required ground truth column: 'Type'\")\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for i, row in df.iterrows():\n",
    "        patient_dict = row.to_dict()\n",
    "        \n",
    "        # Normalize keys to lower-case since diagnose_patient uses lowercase\n",
    "        patient = {k.lower(): v for k, v in patient_dict.items()}\n",
    "        \n",
    "        diag = diagnose_patient(patient, verbose=verbose)\n",
    "        gt = row[\"Type\"]  # Ground truth\n",
    "        \n",
    "        # Compare engine diagnosis vs ground truth\n",
    "        match = (diag['diagnosis'] == gt)\n",
    "        \n",
    "        results.append({\n",
    "            \"patient_id\": diag['patient_id'],\n",
    "            \"ground_truth\": gt,\n",
    "            \"diagnosis\": diag['diagnosis'],\n",
    "            \"match\": match,\n",
    "            \"code\": diag['code'],\n",
    "            \"confidence\": diag['confidence'],\n",
    "            \"reasoning\": \" | \".join(diag['reasoning']),\n",
    "        })\n",
    "        \n",
    "        if (i+1) % 20 == 0:\n",
    "            print(f\"Processed {i+1}/{len(df)} patients...\")\n",
    "    \n",
    "    out_df = pd.DataFrame(results)\n",
    "    \n",
    "    # Save\n",
    "    output_dir = Path(output_path).parent\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    out_df.to_csv(output_path, index=False)\n",
    "    \n",
    "    # Quick stats\n",
    "    acc = out_df['match'].mean() * 100\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"RESULTS\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"\\n‚úì Saved output to: {output_path}\")\n",
    "    print(f\"üéØ Accuracy vs ground truth: {acc:.2f}%\")\n",
    "    print(f\"üìä Correct: {out_df['match'].sum()}/{len(out_df)}\")\n",
    "    \n",
    "    return out_df\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"EXAMPLE USAGE\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "\n",
    "    results_json = run_engine_on_json(\n",
    "        json_path=\"data/ner_results/patient_summaries_fixed.json\",\n",
    "        ground_truth_csv=\"data/migraine_with_id.csv\",\n",
    "        output_path=\"engine_output_ner.csv\",\n",
    "        verbose=False\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aad1e95-3891-434b-a29f-8e28e2b1b0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Systematic Debug - Trace Rule Failures\n",
    "=======================================\n",
    "\n",
    "Find EXACTLY why \"Typical aura with migraine\" patients fail all rules.\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Load everything\n",
    "with open('data/diagnoses/ichd3_diagnoses_final.json', 'r') as f:\n",
    "    results = json.load(f)\n",
    "\n",
    "with open('data/ner_results/patient_summaries_fixed.json', 'r') as f:\n",
    "    patients = json.load(f)\n",
    "\n",
    "gt_df = pd.read_csv('data/migraine_with_id.csv')\n",
    "\n",
    "# Create mappings\n",
    "gt_map = {}\n",
    "for idx, row in gt_df.iterrows():\n",
    "    patient_id = idx + 1\n",
    "    gt_map[patient_id] = row.get('Type', 'Unknown')\n",
    "\n",
    "patient_map = {p['patient_id']: p for p in patients}\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"SYSTEMATIC DEBUG - FINDING THE BUG\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Find \"Typical aura with migraine\" ‚Üí \"Other\" cases\n",
    "print(\"\\nFinding 'Typical aura with migraine' ‚Üí 'Other' patients...\")\n",
    "\n",
    "failing_patients = []\n",
    "for result in results:\n",
    "    pid = result['patient_id']\n",
    "    predicted = result['diagnosis']\n",
    "    actual = gt_map.get(pid, 'Unknown')\n",
    "    \n",
    "    if 'typical aura with' in actual.lower() and predicted == \"Other\":\n",
    "        failing_patients.append(pid)\n",
    "\n",
    "print(f\"Found {len(failing_patients)} failing patients\")\n",
    "print(f\"\\nAnalyzing first 3 patients in detail...\\n\")\n",
    "\n",
    "# Detailed analysis\n",
    "for i, pid in enumerate(failing_patients[:3]):\n",
    "    patient = patient_map[pid]\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(f\"PATIENT {pid} - TRACE THROUGH ALL RULES\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(f\"\\nGround truth: {gt_map[pid]}\")\n",
    "    print(f\"Our diagnosis: Other\")\n",
    "    \n",
    "    # Show all fields\n",
    "    print(\"\\nüìã PATIENT DATA:\")\n",
    "    print(\"-\"*80)\n",
    "    print(f\"  location:       {patient.get('location')}\")\n",
    "    print(f\"  character:      {patient.get('character')}\")\n",
    "    print(f\"  intensity:      {patient.get('intensity')}\")\n",
    "    print(f\"  intensity_text: {patient.get('intensity_text')}\")\n",
    "    print(f\"  frequency:      {patient.get('frequency')}\")\n",
    "    print(f\"  nausea:         {patient.get('nausea')}\")\n",
    "    print(f\"  vomit:          {patient.get('vomit')}\")\n",
    "    print(f\"  photophobia:    {patient.get('photophobia')}\")\n",
    "    print(f\"  phonophobia:    {patient.get('phonophobia')}\")\n",
    "    print(f\"  visual:         {patient.get('visual')}\")\n",
    "    print(f\"  sensory:        {patient.get('sensory')}\")\n",
    "    print(f\"  dysphasia:      {patient.get('dysphasia')}\")\n",
    "    print(f\"  dysarthria:     {patient.get('dysarthria')}\")\n",
    "    print(f\"  vertigo:        {patient.get('vertigo')}\")\n",
    "    print(f\"  tinnitus:       {patient.get('tinnitus')}\")\n",
    "    print(f\"  dpf:            {patient.get('dpf')}\")\n",
    "    \n",
    "    # Check base criteria manually\n",
    "    print(\"\\nüîç BASE CRITERIA CHECK:\")\n",
    "    print(\"-\"*80)\n",
    "    \n",
    "    # Pain characteristics\n",
    "    pain_count = 0\n",
    "    pain_details = []\n",
    "    \n",
    "    location = patient.get('location')\n",
    "    if location and any(term in str(location).lower() for term in ['left', 'right', 'side', 'temple']):\n",
    "        pain_count += 1\n",
    "        pain_details.append(f'unilateral({location})')\n",
    "    \n",
    "    character = patient.get('character')\n",
    "    if character and any(term in str(character).lower() for term in ['throb', 'puls', 'pound', 'beat']):\n",
    "        pain_count += 1\n",
    "        pain_details.append(f'pulsating({character})')\n",
    "    \n",
    "    intensity_text = patient.get('intensity_text')\n",
    "    intensity = patient.get('intensity')\n",
    "    if intensity_text and any(term in str(intensity_text).lower() for term in ['moderate', 'severe']):\n",
    "        pain_count += 1\n",
    "        pain_details.append(f'mod/sev_text({intensity_text})')\n",
    "    elif intensity:\n",
    "        try:\n",
    "            if float(intensity) >= 2:\n",
    "                pain_count += 1\n",
    "                pain_details.append(f'mod/sev_num({intensity})')\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    print(f\"  Pain characteristics: {pain_count}/3 {pain_details}\")\n",
    "    if pain_count < 2:\n",
    "        print(f\"  ‚ùå FAIL: Need >=2, got {pain_count}\")\n",
    "    else:\n",
    "        print(f\"  ‚úÖ PASS\")\n",
    "    \n",
    "    # Frequency\n",
    "    frequency = patient.get('frequency')\n",
    "    freq_pass = True\n",
    "    if frequency:\n",
    "        try:\n",
    "            if int(frequency) < 5:\n",
    "                print(f\"  Frequency: {frequency}\")\n",
    "                print(f\"  ‚ùå FAIL: Need >=5, got {frequency}\")\n",
    "                freq_pass = False\n",
    "            else:\n",
    "                print(f\"  Frequency: {frequency}\")\n",
    "                print(f\"  ‚úÖ PASS\")\n",
    "        except:\n",
    "            print(f\"  Frequency: {frequency} (parse error, assuming pass)\")\n",
    "    else:\n",
    "        print(f\"  Frequency: Not specified (assuming pass)\")\n",
    "    \n",
    "    # Symptoms\n",
    "    nausea = patient.get('nausea')\n",
    "    vomit = patient.get('vomit')\n",
    "    photophobia = patient.get('photophobia')\n",
    "    phonophobia = patient.get('phonophobia')\n",
    "    \n",
    "    def is_present(val):\n",
    "        if not val:\n",
    "            return False\n",
    "        if isinstance(val, (int, float)):\n",
    "            return val > 0\n",
    "        return str(val).lower() not in ['not found', 'none', '', '0']\n",
    "    \n",
    "    has_symptoms = (is_present(nausea) or is_present(vomit) or \n",
    "                   is_present(photophobia) or is_present(phonophobia))\n",
    "    \n",
    "    sx_list = []\n",
    "    if is_present(nausea): sx_list.append('nausea')\n",
    "    if is_present(vomit): sx_list.append('vomit')\n",
    "    if is_present(photophobia): sx_list.append('photophobia')\n",
    "    if is_present(phonophobia): sx_list.append('phonophobia')\n",
    "    \n",
    "    print(f\"  Symptoms: {sx_list}\")\n",
    "    if not has_symptoms:\n",
    "        print(f\"  ‚ùå FAIL: No symptoms\")\n",
    "    else:\n",
    "        print(f\"  ‚úÖ PASS\")\n",
    "    \n",
    "    base_pass = (pain_count >= 2 and freq_pass and has_symptoms)\n",
    "    \n",
    "    print(f\"\\n  üéØ BASE CRITERIA: {'‚úÖ PASS' if base_pass else '‚ùå FAIL'}\")\n",
    "    \n",
    "    # Count aura\n",
    "    print(\"\\nüîç AURA COUNT:\")\n",
    "    print(\"-\"*80)\n",
    "    \n",
    "    aura_fields = ['visual', 'sensory', 'dysphasia', 'dysarthria', \n",
    "                   'vertigo', 'tinnitus', 'hypoacusis', 'diplopia',\n",
    "                   'ataxia', 'conscience', 'visual_defect', 'paresthesia']\n",
    "    \n",
    "    aura_count = 0\n",
    "    aura_list = []\n",
    "    for field in aura_fields:\n",
    "        if is_present(patient.get(field)):\n",
    "            aura_count += 1\n",
    "            aura_list.append(field)\n",
    "    \n",
    "    print(f\"  Aura symptoms: {aura_count} {aura_list}\")\n",
    "    \n",
    "    # Count brainstem\n",
    "    brainstem_fields = ['dysarthria', 'vertigo', 'tinnitus', 'hypoacusis',\n",
    "                       'diplopia', 'ataxia', 'conscience']\n",
    "    \n",
    "    brainstem_count = 0\n",
    "    brainstem_list = []\n",
    "    for field in brainstem_fields:\n",
    "        if is_present(patient.get(field)):\n",
    "            brainstem_count += 1\n",
    "            brainstem_list.append(field)\n",
    "    \n",
    "    print(f\"  Brainstem symptoms: {brainstem_count} {brainstem_list}\")\n",
    "    \n",
    "    # DPF\n",
    "    dpf = patient.get('dpf')\n",
    "    print(f\"  DPF: {dpf}\")\n",
    "    \n",
    "    # Check each rule\n",
    "    print(\"\\nüîç RULE-BY-RULE CHECK:\")\n",
    "    print(\"-\"*80)\n",
    "    \n",
    "    # Rule 1: Familial hemiplegic\n",
    "    has_hemi_aura = (is_present(patient.get('dysphasia')) or \n",
    "                     is_present(patient.get('visual')) or \n",
    "                     is_present(patient.get('sensory')))\n",
    "    \n",
    "    print(\"\\n  Rule 1: Familial hemiplegic\")\n",
    "    print(f\"    Base: {base_pass}\")\n",
    "    print(f\"    Hemiplegic aura (dysphasia/visual/sensory): {has_hemi_aura}\")\n",
    "    print(f\"    DPF=1: {is_present(dpf) and int(dpf) == 1}\")\n",
    "    \n",
    "    if base_pass and has_hemi_aura and is_present(dpf) and int(dpf) == 1:\n",
    "        print(\"    ‚úÖ SHOULD MATCH\")\n",
    "    else:\n",
    "        print(\"    ‚ùå NO MATCH\")\n",
    "    \n",
    "    # Rule 2: Sporadic hemiplegic\n",
    "    dpf_zero = not is_present(dpf) or int(dpf) == 0\n",
    "    \n",
    "    print(\"\\n  Rule 2: Sporadic hemiplegic\")\n",
    "    print(f\"    Base: {base_pass}\")\n",
    "    print(f\"    Hemiplegic aura: {has_hemi_aura}\")\n",
    "    print(f\"    DPF=0: {dpf_zero}\")\n",
    "    \n",
    "    if base_pass and has_hemi_aura and dpf_zero:\n",
    "        print(\"    ‚úÖ SHOULD MATCH\")\n",
    "    else:\n",
    "        print(\"    ‚ùå NO MATCH\")\n",
    "    \n",
    "    # Rule 3: Typical aura with headache\n",
    "    print(\"\\n  Rule 3: Typical aura with headache\")\n",
    "    print(f\"    Base: {base_pass}\")\n",
    "    print(f\"    Aura>=1: {aura_count >= 1}\")\n",
    "    print(f\"    DPF=0: {dpf_zero}\")\n",
    "    print(f\"    Brainstem<2: {brainstem_count < 2}\")\n",
    "    \n",
    "    if base_pass and aura_count >= 1 and dpf_zero and brainstem_count < 2:\n",
    "        print(\"    ‚úÖ SHOULD MATCH\")\n",
    "    else:\n",
    "        print(\"    ‚ùå NO MATCH\")\n",
    "    \n",
    "    # Rule 4: Typical aura without migraine\n",
    "    print(\"\\n  Rule 4: Typical aura without migraine\")\n",
    "    print(f\"    NOT base: {not base_pass}\")\n",
    "    print(f\"    Aura>=1: {aura_count >= 1}\")\n",
    "    print(f\"    Freq=0: {frequency == 0 if frequency else 'N/A'}\")\n",
    "    print(f\"    DPF=0: {dpf_zero}\")\n",
    "    print(f\"    Brainstem<2: {brainstem_count < 2}\")\n",
    "    \n",
    "    # Rule 5: Basilar\n",
    "    print(\"\\n  Rule 5: Basilar-type aura\")\n",
    "    print(f\"    Base: {base_pass}\")\n",
    "    print(f\"    Brainstem>=2: {brainstem_count >= 2}\")\n",
    "    \n",
    "    if base_pass and brainstem_count >= 2:\n",
    "        print(\"    ‚úÖ SHOULD MATCH\")\n",
    "    else:\n",
    "        print(\"    ‚ùå NO MATCH\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üéØ WHY IT FAILED:\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    if not base_pass:\n",
    "        print(\"‚ùå BASE CRITERIA FAILED\")\n",
    "        if pain_count < 2:\n",
    "            print(f\"   - Pain characteristics: {pain_count}/3 (need >=2)\")\n",
    "        if not freq_pass:\n",
    "            print(f\"   - Frequency: {frequency} (need >=5)\")\n",
    "        if not has_symptoms:\n",
    "            print(f\"   - No accompanying symptoms\")\n",
    "    elif aura_count == 0:\n",
    "        print(\"‚ùå NO AURA SYMPTOMS\")\n",
    "    elif brainstem_count >= 2:\n",
    "        print(\"‚ùå HAS >=2 BRAINSTEM (should be basilar but base fails?)\")\n",
    "    else:\n",
    "        print(\"‚ùå UNKNOWN - should have matched a rule!\")\n",
    "    \n",
    "    print(\"\\n\")\n",
    "\n",
    "# Summary\n",
    "print(\"=\"*80)\n",
    "print(\"SUMMARY OF COMMON FAILURES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "fail_reasons = {\n",
    "    'base_fail': 0,\n",
    "    'no_aura': 0,\n",
    "    'brainstem_high': 0,\n",
    "    'dpf_issue': 0,\n",
    "    'other': 0\n",
    "}\n",
    "\n",
    "for pid in failing_patients[:50]:  # Check first 50\n",
    "    patient = patient_map[pid]\n",
    "    \n",
    "    # Check base\n",
    "    pain_count = 0\n",
    "    if patient.get('location') and any(term in str(patient.get('location')).lower() for term in ['left', 'right', 'side', 'temple']):\n",
    "        pain_count += 1\n",
    "    if patient.get('character') and any(term in str(patient.get('character')).lower() for term in ['throb', 'puls', 'pound', 'beat']):\n",
    "        pain_count += 1\n",
    "    if patient.get('intensity_text') and any(term in str(patient.get('intensity_text')).lower() for term in ['moderate', 'severe']):\n",
    "        pain_count += 1\n",
    "    elif patient.get('intensity'):\n",
    "        try:\n",
    "            if float(patient.get('intensity')) >= 2:\n",
    "                pain_count += 1\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    freq_ok = True\n",
    "    if patient.get('frequency'):\n",
    "        try:\n",
    "            if int(patient.get('frequency')) < 5:\n",
    "                freq_ok = False\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    def is_present(val):\n",
    "        if not val:\n",
    "            return False\n",
    "        if isinstance(val, (int, float)):\n",
    "            return val > 0\n",
    "        return str(val).lower() not in ['not found', 'none', '', '0']\n",
    "    \n",
    "    has_sx = (is_present(patient.get('nausea')) or is_present(patient.get('vomit')) or\n",
    "              is_present(patient.get('photophobia')) or is_present(patient.get('phonophobia')))\n",
    "    \n",
    "    base_ok = pain_count >= 2 and freq_ok and has_sx\n",
    "    \n",
    "    if not base_ok:\n",
    "        fail_reasons['base_fail'] += 1\n",
    "    else:\n",
    "        # Count aura\n",
    "        aura_count = sum(1 for f in ['visual', 'sensory', 'dysphasia', 'dysarthria', \n",
    "                                     'vertigo', 'tinnitus', 'hypoacusis', 'diplopia',\n",
    "                                     'ataxia', 'conscience', 'visual_defect', 'paresthesia']\n",
    "                        if is_present(patient.get(f)))\n",
    "        \n",
    "        if aura_count == 0:\n",
    "            fail_reasons['no_aura'] += 1\n",
    "        else:\n",
    "            fail_reasons['other'] += 1\n",
    "\n",
    "print(\"\\nOut of first 50 failing patients:\")\n",
    "print(f\"  Base criteria failed: {fail_reasons['base_fail']}\")\n",
    "print(f\"  No aura symptoms:     {fail_reasons['no_aura']}\")\n",
    "print(f\"  Other reasons:        {fail_reasons['other']}\")\n",
    "\n",
    "print(\"\\nüéØ MAIN ISSUE IDENTIFIED!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9673f4-eb5b-4060-989f-96f046381bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Plot Confusion Matrix from Engine Output CSV\n",
    "==============================================\n",
    "\n",
    "Load engine_output.csv and plot confusion matrix with inferno colormap.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "from pathlib import Path\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 10)\n",
    "\n",
    "\n",
    "def plot_confusion_matrix_from_csv(csv_path='engine_output.csv', \n",
    "                                   output_path='evaluation_results/symbolic_original/confusion_matrix_original_data.png'):\n",
    "    \"\"\"\n",
    "    Load engine output CSV and plot confusion matrix\n",
    "    \n",
    "    Args:\n",
    "        csv_path: Path to engine_output.csv\n",
    "        output_path: Where to save the confusion matrix plot\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"PLOTTING CONFUSION MATRIX FROM ENGINE OUTPUT\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Load results\n",
    "    df = pd.read_csv(csv_path)\n",
    "    print(f\"\\n‚úì Loaded {len(df)} patients from {csv_path}\")\n",
    "    \n",
    "    # Extract true and predicted labels\n",
    "    y_true = df['ground_truth']\n",
    "    y_pred = df['diagnosis']\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"PERFORMANCE METRICS\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"\\nOverall Accuracy: {accuracy:.4f} ({accuracy*100:.1f}%)\")\n",
    "    \n",
    "    print(f\"\\n{'‚îÄ'*80}\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(f\"{'‚îÄ'*80}\")\n",
    "    print(classification_report(y_true, y_pred, zero_division=0))\n",
    "    \n",
    "    # Compute confusion matrix\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"GENERATING CONFUSION MATRIX\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    labels = sorted(y_true.unique())\n",
    "    \n",
    "    print(f\"\\nUnique diagnoses found: {len(labels)}\")\n",
    "    print(f\"Labels: {labels}\")\n",
    "    \n",
    "    # Normalize\n",
    "    cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    \n",
    "    # Create figure\n",
    "    fig, ax = plt.subplots(figsize=(12, 10))\n",
    "    \n",
    "    # Plot with inferno colormap\n",
    "    sns.heatmap(cm_norm, annot=True, fmt='.2f', cmap='inferno',\n",
    "                xticklabels=labels, yticklabels=labels,\n",
    "                ax=ax, cbar_kws={'label': 'Proportion'})\n",
    "    \n",
    "    ax.set_title('Symbolic Reasoning on Original Data - Confusion Matrix', \n",
    "                 fontsize=14, fontweight='bold', pad=15)\n",
    "    ax.set_xlabel('Predicted Diagnosis', fontsize=12, fontweight='bold')\n",
    "    ax.set_ylabel('True Diagnosis', fontsize=12, fontweight='bold')\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "    plt.setp(ax.get_yticklabels(), rotation=0)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Create output directory\n",
    "    output_dir = Path(output_path).parent\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Save\n",
    "    plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"\\n‚úì Saved confusion matrix: {output_path}\")\n",
    "    plt.close()\n",
    "    \n",
    "    # Print diagnosis distribution\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"DIAGNOSIS DISTRIBUTION\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    print(\"\\nGround Truth Distribution:\")\n",
    "    print(y_true.value_counts().sort_index())\n",
    "    \n",
    "    print(\"\\nPredicted Distribution:\")\n",
    "    print(y_pred.value_counts().sort_index())\n",
    "    \n",
    "    # Calculate per-class accuracy\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"PER-CLASS ACCURACY\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    for label in labels:\n",
    "        mask = y_true == label\n",
    "        if mask.sum() > 0:\n",
    "            class_acc = (y_true[mask] == y_pred[mask]).mean()\n",
    "            correct = (y_true[mask] == y_pred[mask]).sum()\n",
    "            total = mask.sum()\n",
    "            print(f\"\\n{label}:\")\n",
    "            print(f\"  Accuracy: {class_acc:.2%} ({correct}/{total})\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"‚úÖ COMPLETE!\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\nGenerated file: {output_path}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üîç CHECKING FOR ENGINE OUTPUT...\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Check multiple possible locations\n",
    "    possible_paths = [\n",
    "        'engine_output.csv',\n",
    "        'data/engine_output.csv',\n",
    "        '/home/claude/engine_output.csv',\n",
    "    ]\n",
    "    \n",
    "    csv_path = None\n",
    "    for path in possible_paths:\n",
    "        if Path(path).exists():\n",
    "            csv_path = path\n",
    "            print(f\"‚úì Found: {path}\")\n",
    "            break\n",
    "    \n",
    "    if csv_path is None:\n",
    "        print(\"\\n‚ùå ERROR: engine_output.csv not found in any location!\")\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"üìã INSTRUCTIONS TO GENERATE engine_output.csv\")\n",
    "        print(\"=\"*80)\n",
    "        print(\"\"\"\n",
    "1. Make sure you have your diagnose_patient() function defined\n",
    "\n",
    "2. Run this code in Python:\n",
    "\n",
    "   import pandas as pd\n",
    "   \n",
    "   def run_engine_on_csv(csv_path, output_path=\"engine_output.csv\", verbose=False):\n",
    "       df = pd.read_csv(csv_path)\n",
    "       print(f\"\\\\nLoaded {len(df)} patients from {csv_path}\")\n",
    "       \n",
    "       if \"Type\" not in df.columns:\n",
    "           raise ValueError(\"CSV missing required column: 'Type'\")\n",
    "       \n",
    "       results = []\n",
    "       for i, row in df.iterrows():\n",
    "           patient_dict = row.to_dict()\n",
    "           patient = {k.lower(): v for k, v in patient_dict.items()}\n",
    "           diag = diagnose_patient(patient, verbose=verbose)\n",
    "           gt = row[\"Type\"]\n",
    "           \n",
    "           results.append({\n",
    "               \"patient_id\": diag['patient_id'],\n",
    "               \"ground_truth\": gt,\n",
    "               \"diagnosis\": diag['diagnosis'],\n",
    "               \"match\": (diag['diagnosis'] == gt),\n",
    "               \"code\": diag['code'],\n",
    "               \"confidence\": diag['confidence'],\n",
    "               \"reasoning\": \" | \".join(diag['reasoning']),\n",
    "           })\n",
    "           \n",
    "           if (i+1) % 20 == 0:\n",
    "               print(f\"Processed {i+1}/{len(df)} patients...\")\n",
    "       \n",
    "       out_df = pd.DataFrame(results)\n",
    "       out_df.to_csv(output_path, index=False)\n",
    "       \n",
    "       acc = out_df['match'].mean() * 100\n",
    "       print(f\"\\\\nüî• DONE! Saved to {output_path}\")\n",
    "       print(f\"üéØ Accuracy: {acc:.2f}%\\\\n\")\n",
    "       return out_df\n",
    "   \n",
    "   # Run it\n",
    "   results = run_engine_on_csv(\"data/migraine_with_id.csv\", verbose=False)\n",
    "\n",
    "3. Then run this script again:\n",
    "   python /mnt/user-data/outputs/plot_engine_confusion_matrix.py\n",
    "        \"\"\")\n",
    "        return\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    output_path = 'evaluation_results/symbolic_original/confusion_matrix_original_data.png'\n",
    "    plot_confusion_matrix_from_csv(csv_path, output_path)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0ddd7a-bc49-4474-87a1-2b4945912870",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Detailed Error Analysis for ICHD-3 Diagnostic Engine (original data)\n",
    "======================================================\n",
    "\n",
    "Analyzes which reasoning criteria fail for misclassified patients.\n",
    "Visualizes error patterns and rule failures.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from collections import defaultdict, Counter\n",
    "import re\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 10)\n",
    "\n",
    "# Okabe-Ito colorblind-safe palette\n",
    "COLORS = {\n",
    "    'orange': '#E69F00',\n",
    "    'sky_blue': '#56B4E9', \n",
    "    'bluish_green': '#009E73',\n",
    "    'yellow': '#F0E442',\n",
    "    'vermillion': '#D55E00',\n",
    "    'reddish_purple': '#CC79A7'\n",
    "}\n",
    "\n",
    "\n",
    "def parse_reasoning(reasoning_str):\n",
    "    \"\"\"\n",
    "    Extract specific criteria from reasoning string\n",
    "    \n",
    "    Returns dict with parsed components:\n",
    "    - met_criteria: list of criteria that passed\n",
    "    - failed_criteria: list of criteria that failed\n",
    "    - diagnosis_path: which rule was triggered\n",
    "    \"\"\"\n",
    "    if pd.isna(reasoning_str):\n",
    "        return {\n",
    "            'met_criteria': [],\n",
    "            'failed_criteria': [],\n",
    "            'diagnosis_path': 'Unknown'\n",
    "        }\n",
    "    \n",
    "    met = []\n",
    "    failed = []\n",
    "    diagnosis = 'Unknown'\n",
    "    \n",
    "    # Split by separator\n",
    "    parts = str(reasoning_str).split('|')\n",
    "    \n",
    "    for part in parts:\n",
    "        part = part.strip()\n",
    "        \n",
    "        # Check for diagnosis assignment\n",
    "        if 'diagnosed as' in part.lower() or 'diagnosis:' in part.lower():\n",
    "            diagnosis = part\n",
    "        \n",
    "        # Check for positive criteria\n",
    "        if any(word in part.lower() for word in ['meets', 'has', 'present', 'found', 'detected', '‚úì', 'pass']):\n",
    "            met.append(part)\n",
    "        \n",
    "        # Check for negative criteria\n",
    "        if any(word in part.lower() for word in ['fails', 'missing', 'absent', 'not found', 'insufficient', '‚úó', 'fail']):\n",
    "            failed.append(part)\n",
    "    \n",
    "    return {\n",
    "        'met_criteria': met,\n",
    "        'failed_criteria': failed,\n",
    "        'diagnosis_path': diagnosis\n",
    "    }\n",
    "\n",
    "\n",
    "def extract_criterion_type(criterion_text):\n",
    "    \"\"\"\n",
    "    Categorize a criterion into types\n",
    "    \"\"\"\n",
    "    text = criterion_text.lower()\n",
    "    \n",
    "    # Define criterion categories\n",
    "    if any(word in text for word in ['pain', 'character', 'location', 'intensity', 'unilateral', 'pulsating', 'throbbing']):\n",
    "        return 'Pain Characteristics'\n",
    "    elif any(word in text for word in ['duration', 'hours', '4-72']):\n",
    "        return 'Duration'\n",
    "    elif any(word in text for word in ['frequency', 'attacks', 'episodes', '>=5']):\n",
    "        return 'Frequency'\n",
    "    elif any(word in text for word in ['nausea', 'vomit', 'photophobia', 'phonophobia', 'accompanying']):\n",
    "        return 'Associated Symptoms'\n",
    "    elif any(word in text for word in ['visual', 'sensory', 'dysphasia', 'aura', 'scotoma', 'paresthesia']):\n",
    "        return 'Aura Symptoms'\n",
    "    elif any(word in text for word in ['dysarthria', 'vertigo', 'tinnitus', 'diplopia', 'ataxia', 'brainstem']):\n",
    "        return 'Brainstem Symptoms'\n",
    "    elif any(word in text for word in ['hemiplegic', 'motor', 'weakness', 'paralysis']):\n",
    "        return 'Motor Symptoms'\n",
    "    elif any(word in text for word in ['dpf', 'family', 'familial', 'hereditary']):\n",
    "        return 'Family History'\n",
    "    else:\n",
    "        return 'Other'\n",
    "\n",
    "\n",
    "def analyze_errors(csv_path='engine_output_ner.csv'):\n",
    "    \"\"\"\n",
    "    Comprehensive error analysis\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"DETAILED ERROR ANALYSIS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Load data\n",
    "    df = pd.read_csv(csv_path)\n",
    "    print(f\"\\n‚úì Loaded {len(df)} patients from {csv_path}\")\n",
    "    \n",
    "    # Split into correct and incorrect\n",
    "    correct_df = df[df['match'] == True]\n",
    "    incorrect_df = df[df['match'] == False]\n",
    "    \n",
    "    print(f\"\\nüìä Overall Statistics:\")\n",
    "    print(f\"   Correct: {len(correct_df)} ({len(correct_df)/len(df)*100:.1f}%)\")\n",
    "    print(f\"   Incorrect: {len(incorrect_df)} ({len(incorrect_df)/len(df)*100:.1f}%)\")\n",
    "    \n",
    "    # Analyze error patterns\n",
    "    error_patterns = analyze_error_patterns(incorrect_df)\n",
    "    \n",
    "    # Analyze reasoning failures\n",
    "    reasoning_failures = analyze_reasoning_failures(incorrect_df)\n",
    "    \n",
    "    # Analyze confusion patterns\n",
    "    confusion_patterns = analyze_confusion_patterns(incorrect_df)\n",
    "    \n",
    "    return {\n",
    "        'overall': df,\n",
    "        'correct': correct_df,\n",
    "        'incorrect': incorrect_df,\n",
    "        'error_patterns': error_patterns,\n",
    "        'reasoning_failures': reasoning_failures,\n",
    "        'confusion_patterns': confusion_patterns\n",
    "    }\n",
    "\n",
    "\n",
    "def analyze_error_patterns(incorrect_df):\n",
    "    \"\"\"\n",
    "    Analyze patterns in misclassifications\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"ERROR PATTERNS BY TRUE DIAGNOSIS\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    error_patterns = {}\n",
    "    \n",
    "    for true_diag in incorrect_df['ground_truth'].unique():\n",
    "        mask = incorrect_df['ground_truth'] == true_diag\n",
    "        subset = incorrect_df[mask]\n",
    "        \n",
    "        error_patterns[true_diag] = {\n",
    "            'count': len(subset),\n",
    "            'predicted_as': subset['diagnosis'].value_counts().to_dict(),\n",
    "            'sample_cases': []\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n{true_diag}:\")\n",
    "        print(f\"   Total errors: {len(subset)}\")\n",
    "        print(f\"   Predicted as:\")\n",
    "        for pred, count in subset['diagnosis'].value_counts().items():\n",
    "            pct = count / len(subset) * 100\n",
    "            print(f\"      - {pred}: {count} ({pct:.1f}%)\")\n",
    "        \n",
    "        # Sample cases\n",
    "        for idx, row in subset.head(3).iterrows():\n",
    "            error_patterns[true_diag]['sample_cases'].append({\n",
    "                'patient_id': row['patient_id'],\n",
    "                'predicted': row['diagnosis'],\n",
    "                'confidence': row.get('confidence', 'N/A'),\n",
    "                'reasoning': row.get('reasoning', 'N/A')\n",
    "            })\n",
    "    \n",
    "    return error_patterns\n",
    "\n",
    "\n",
    "def analyze_reasoning_failures(incorrect_df):\n",
    "    \"\"\"\n",
    "    Analyze which criteria fail most often\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"REASONING FAILURE ANALYSIS\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Parse all reasoning strings\n",
    "    failed_criteria_by_type = defaultdict(int)\n",
    "    failed_criteria_by_diagnosis = defaultdict(lambda: defaultdict(int))\n",
    "    \n",
    "    for idx, row in incorrect_df.iterrows():\n",
    "        parsed = parse_reasoning(row.get('reasoning', ''))\n",
    "        true_diag = row['ground_truth']\n",
    "        \n",
    "        for criterion in parsed['failed_criteria']:\n",
    "            criterion_type = extract_criterion_type(criterion)\n",
    "            failed_criteria_by_type[criterion_type] += 1\n",
    "            failed_criteria_by_diagnosis[true_diag][criterion_type] += 1\n",
    "    \n",
    "    # Print overall failure counts\n",
    "    print(\"\\nüìä Most Common Criterion Failures (Overall):\")\n",
    "    sorted_failures = sorted(failed_criteria_by_type.items(), key=lambda x: x[1], reverse=True)\n",
    "    for criterion_type, count in sorted_failures:\n",
    "        pct = count / len(incorrect_df) * 100\n",
    "        print(f\"   {criterion_type}: {count} ({pct:.1f}%)\")\n",
    "    \n",
    "    # Print by diagnosis\n",
    "    print(f\"\\n{'‚îÄ'*80}\")\n",
    "    print(\"Criterion Failures by True Diagnosis:\")\n",
    "    print(f\"{'‚îÄ'*80}\")\n",
    "    \n",
    "    for true_diag in sorted(failed_criteria_by_diagnosis.keys()):\n",
    "        print(f\"\\n{true_diag}:\")\n",
    "        failures = failed_criteria_by_diagnosis[true_diag]\n",
    "        sorted_diag_failures = sorted(failures.items(), key=lambda x: x[1], reverse=True)\n",
    "        for criterion_type, count in sorted_diag_failures[:5]:  # Top 5\n",
    "            print(f\"   - {criterion_type}: {count}\")\n",
    "    \n",
    "    return {\n",
    "        'by_type': dict(failed_criteria_by_type),\n",
    "        'by_diagnosis': dict(failed_criteria_by_diagnosis)\n",
    "    }\n",
    "\n",
    "\n",
    "def analyze_confusion_patterns(incorrect_df):\n",
    "    \"\"\"\n",
    "    Analyze which diagnoses are confused with each other\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"CONFUSION PATTERNS\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    confusion_pairs = []\n",
    "    \n",
    "    for idx, row in incorrect_df.iterrows():\n",
    "        confusion_pairs.append((row['ground_truth'], row['diagnosis']))\n",
    "    \n",
    "    confusion_counts = Counter(confusion_pairs)\n",
    "    \n",
    "    print(\"\\nMost Common Misclassifications:\")\n",
    "    for (true_diag, pred_diag), count in confusion_counts.most_common(10):\n",
    "        print(f\"   {true_diag} ‚Üí {pred_diag}: {count}\")\n",
    "    \n",
    "    return dict(confusion_counts)\n",
    "\n",
    "\n",
    "def visualize_error_analysis(analysis, output_dir='evaluation_results/symbolic_original'):\n",
    "    \"\"\"\n",
    "    Create visualizations for error analysis\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"GENERATING VISUALIZATIONS\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    output_dir = Path(output_dir)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # 1. Error rate by diagnosis\n",
    "    plot_error_rates(analysis, output_dir)\n",
    "    \n",
    "    # 2. Criterion failure heatmap\n",
    "    plot_criterion_failures(analysis, output_dir)\n",
    "    \n",
    "    # 3. Confusion flow diagram\n",
    "    plot_confusion_flow(analysis, output_dir)\n",
    "    \n",
    "    # 4. Detailed error breakdown\n",
    "    plot_error_breakdown(analysis, output_dir)\n",
    "\n",
    "\n",
    "def plot_error_rates(analysis, output_dir):\n",
    "    \"\"\"\n",
    "    Plot error rates for each diagnosis type\n",
    "    \"\"\"\n",
    "    \n",
    "    df = analysis['overall']\n",
    "    \n",
    "    # Calculate error rate per diagnosis\n",
    "    error_rates = {}\n",
    "    for diag in df['ground_truth'].unique():\n",
    "        mask = df['ground_truth'] == diag\n",
    "        total = mask.sum()\n",
    "        errors = ((df['ground_truth'] == diag) & (df['match'] == False)).sum()\n",
    "        error_rates[diag] = {\n",
    "            'error_rate': errors / total if total > 0 else 0,\n",
    "            'total': total,\n",
    "            'errors': errors,\n",
    "            'correct': total - errors\n",
    "        }\n",
    "    \n",
    "    # Sort by error rate\n",
    "    sorted_diags = sorted(error_rates.keys(), key=lambda x: error_rates[x]['error_rate'], reverse=True)\n",
    "    \n",
    "    # Create figure\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # Left: Error rates\n",
    "    error_pcts = [error_rates[d]['error_rate'] * 100 for d in sorted_diags]\n",
    "    colors = [COLORS['vermillion'] if pct > 50 else COLORS['orange'] if pct > 30 else COLORS['bluish_green'] for pct in error_pcts]\n",
    "    \n",
    "    bars1 = ax1.barh(range(len(sorted_diags)), error_pcts, color=colors, alpha=0.8)\n",
    "    ax1.set_yticks(range(len(sorted_diags)))\n",
    "    ax1.set_yticklabels(sorted_diags, fontsize=16)\n",
    "    ax1.set_xlabel('Error Rate (%)', fontsize=14, fontweight='bold')\n",
    "    ax1.set_title('Error Rate by Diagnosis Type', fontsize=14, fontweight='bold', pad=15)\n",
    "    ax1.axvline(50, color='red', linestyle='--', alpha=0.3, linewidth=1)\n",
    "    ax1.axvline(30, color='orange', linestyle='--', alpha=0.3, linewidth=1)\n",
    "    ax1.grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, (bar, pct) in enumerate(zip(bars1, error_pcts)):\n",
    "        ax1.text(pct + 1, i, f'{pct:.1f}%', va='center', fontsize=9)\n",
    "    \n",
    "    # Right: Correct vs Incorrect counts\n",
    "    diagnoses = sorted_diags\n",
    "    correct_counts = [error_rates[d]['correct'] for d in diagnoses]\n",
    "    error_counts = [error_rates[d]['errors'] for d in diagnoses]\n",
    "    \n",
    "    y_pos = np.arange(len(diagnoses))\n",
    "    \n",
    "    ax2.barh(y_pos, correct_counts, color=COLORS['bluish_green'], alpha=0.8, label='Correct')\n",
    "    ax2.barh(y_pos, error_counts, left=correct_counts, color=COLORS['vermillion'], alpha=0.8, label='Errors')\n",
    "    \n",
    "    ax2.set_yticks(y_pos)\n",
    "    ax2.set_yticklabels(diagnoses, fontsize=16)\n",
    "    ax2.set_xlabel('Number of Patients', fontsize=14, fontweight='bold')\n",
    "    ax2.set_title('Correct vs Incorrect Predictions', fontsize=14, fontweight='bold', pad=15)\n",
    "    ax2.legend(loc='lower right', fontsize=14)\n",
    "    ax2.grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    output_path = output_dir / 'error_rates_by_diagnosis.png'\n",
    "    plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"‚úì Saved: {output_path}\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_criterion_failures(analysis, output_dir):\n",
    "    \"\"\"\n",
    "    Plot heatmap of criterion failures by diagnosis\n",
    "    \"\"\"\n",
    "    \n",
    "    reasoning_failures = analysis['reasoning_failures']['by_diagnosis']\n",
    "    \n",
    "    if not reasoning_failures:\n",
    "        print(\"‚ö† No reasoning failure data available, skipping criterion failure plot\")\n",
    "        return\n",
    "    \n",
    "    # Create matrix\n",
    "    all_criterion_types = set()\n",
    "    for diag_failures in reasoning_failures.values():\n",
    "        all_criterion_types.update(diag_failures.keys())\n",
    "    \n",
    "    criterion_types = sorted(all_criterion_types)\n",
    "    diagnoses = sorted(reasoning_failures.keys())\n",
    "    \n",
    "    # Build matrix\n",
    "    matrix = np.zeros((len(diagnoses), len(criterion_types)))\n",
    "    \n",
    "    for i, diag in enumerate(diagnoses):\n",
    "        for j, criterion in enumerate(criterion_types):\n",
    "            matrix[i, j] = reasoning_failures[diag].get(criterion, 0)\n",
    "    \n",
    "    # Create figure\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    \n",
    "    # Plot heatmap with inferno colormap\n",
    "    sns.heatmap(matrix, annot=True, fmt='g', cmap='inferno',\n",
    "                xticklabels=criterion_types, yticklabels=diagnoses,\n",
    "                ax=ax, cbar_kws={'label': 'Failure Count'})\n",
    "    \n",
    "    ax.set_title('Criterion Failures by Diagnosis Type (Inferno)', \n",
    "                 fontsize=14, fontweight='bold', pad=15)\n",
    "    ax.set_xlabel('Criterion Type', fontsize=14, fontweight='bold')\n",
    "    ax.set_ylabel('True Diagnosis', fontsize=14, fontweight='bold')\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "    plt.setp(ax.get_yticklabels(), rotation=0)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    output_path = output_dir / 'criterion_failures_heatmap.png'\n",
    "    plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"‚úì Saved: {output_path}\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_confusion_flow(analysis, output_dir):\n",
    "    \"\"\"\n",
    "    Plot sankey-style confusion flow\n",
    "    \"\"\"\n",
    "    \n",
    "    confusion_patterns = analysis['confusion_patterns']\n",
    "    \n",
    "    if not confusion_patterns:\n",
    "        print(\"‚ö† No confusion pattern data, skipping flow plot\")\n",
    "        return\n",
    "    \n",
    "    # Get top 10 confusions\n",
    "    top_confusions = sorted(confusion_patterns.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "    \n",
    "    # Create bar chart\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    \n",
    "    labels = [f\"{true} ‚Üí {pred}\" for (true, pred), count in top_confusions]\n",
    "    counts = [count for (true, pred), count in top_confusions]\n",
    "    \n",
    "    bars = ax.barh(range(len(labels)), counts, color=COLORS['orange'], alpha=0.8)\n",
    "    ax.set_yticks(range(len(labels)))\n",
    "    ax.set_yticklabels(labels, fontsize=16)\n",
    "    ax.set_xlabel('Number of Misclassifications', fontsize=16, fontweight='bold')\n",
    "    ax.set_title('Top 10 Confusion Patterns', fontsize=14, fontweight='bold', pad=15)\n",
    "    ax.grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, (bar, count) in enumerate(zip(bars, counts)):\n",
    "        ax.text(count + 0.5, i, str(count), va='center', fontsize=14)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    output_path = output_dir / 'confusion_flow.png'\n",
    "    plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"‚úì Saved: {output_path}\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_error_breakdown(analysis, output_dir):\n",
    "    \"\"\"\n",
    "    Plot detailed breakdown of failure types\n",
    "    \"\"\"\n",
    "    \n",
    "    reasoning_failures = analysis['reasoning_failures']['by_type']\n",
    "    \n",
    "    if not reasoning_failures:\n",
    "        print(\"‚ö† No failure type data, skipping breakdown plot\")\n",
    "        return\n",
    "    \n",
    "    # Sort by count\n",
    "    sorted_failures = sorted(reasoning_failures.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    criterion_types = [item[0] for item in sorted_failures]\n",
    "    counts = [item[1] for item in sorted_failures]\n",
    "    \n",
    "    # Create figure\n",
    "    fig, ax = plt.subplots(figsize=(12, 7))\n",
    "    \n",
    "    # Color by severity\n",
    "    max_count = max(counts)\n",
    "    colors = []\n",
    "    for count in counts:\n",
    "        if count > max_count * 0.6:\n",
    "            colors.append(COLORS['vermillion'])\n",
    "        elif count > max_count * 0.3:\n",
    "            colors.append(COLORS['orange'])\n",
    "        else:\n",
    "            colors.append(COLORS['bluish_green'])\n",
    "    \n",
    "    bars = ax.barh(range(len(criterion_types)), counts, color=colors, alpha=0.8)\n",
    "    ax.set_yticks(range(len(criterion_types)))\n",
    "    ax.set_yticklabels(criterion_types, fontsize=14)\n",
    "    ax.set_xlabel('Number of Failures', fontsize=14, fontweight='bold')\n",
    "    ax.set_title('Criterion Failure Breakdown (All Misclassifications)', \n",
    "                 fontsize=14, fontweight='bold', pad=15)\n",
    "    ax.grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, (bar, count) in enumerate(zip(bars, counts)):\n",
    "        ax.text(count + max_count*0.01, i, str(count), va='center', fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    output_path = output_dir / 'failure_breakdown.png'\n",
    "    plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"‚úì Saved: {output_path}\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def print_detailed_examples(analysis, num_examples=5):\n",
    "    \"\"\"\n",
    "    Print detailed examples of errors\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"DETAILED ERROR EXAMPLES (First {num_examples} per diagnosis)\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    incorrect_df = analysis['incorrect']\n",
    "    \n",
    "    for true_diag in sorted(incorrect_df['ground_truth'].unique()):\n",
    "        subset = incorrect_df[incorrect_df['ground_truth'] == true_diag].head(num_examples)\n",
    "        \n",
    "        print(f\"\\n{'‚îÄ'*80}\")\n",
    "        print(f\"TRUE DIAGNOSIS: {true_diag}\")\n",
    "        print(f\"{'‚îÄ'*80}\")\n",
    "        \n",
    "        for idx, row in subset.iterrows():\n",
    "            print(f\"\\n  Patient {row['patient_id']}:\")\n",
    "            print(f\"    Predicted: {row['diagnosis']}\")\n",
    "            print(f\"    Confidence: {row.get('confidence', 'N/A')}\")\n",
    "            print(f\"    Reasoning:\")\n",
    "            \n",
    "            reasoning = str(row.get('reasoning', 'N/A'))\n",
    "            reasoning_parts = reasoning.split('|')\n",
    "            for part in reasoning_parts[:5]:  # Show first 5 parts\n",
    "                print(f\"      ‚Ä¢ {part.strip()}\")\n",
    "            if len(reasoning_parts) > 5:\n",
    "                print(f\"      ... ({len(reasoning_parts) - 5} more criteria)\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üîç CHECKING FOR ENGINE OUTPUT...\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Check for file\n",
    "    possible_paths = [\n",
    "        'engine_output.csv',\n",
    "       \n",
    "    ]\n",
    "    \n",
    "    csv_path = None\n",
    "    for path in possible_paths:\n",
    "        if Path(path).exists():\n",
    "            csv_path = path\n",
    "            print(f\"‚úì Found: {path}\")\n",
    "            break\n",
    "    \n",
    "    if csv_path is None:\n",
    "        print(\"\\n‚ùå ERROR: engine_output.csv not found!\")\n",
    "        print(\"\\nPlease run your engine first to generate engine_output.csv\")\n",
    "        print(\"See: /mnt/user-data/outputs/QUICK_START.txt\")\n",
    "        return\n",
    "    \n",
    "    # Run analysis\n",
    "    analysis = analyze_errors(csv_path)\n",
    "    \n",
    "    # Create visualizations\n",
    "    visualize_error_analysis(analysis)\n",
    "    \n",
    "    # Print detailed examples\n",
    "    print_detailed_examples(analysis, num_examples=3)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"‚úÖ ERROR ANALYSIS COMPLETE!\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"\\nGenerated files in: evaluation_results/symbolic_original/\")\n",
    "    print(\"  1. error_rates_by_diagnosis.png\")\n",
    "    print(\"  2. criterion_failures_heatmap.png (inferno colormap)\")\n",
    "    print(\"  3. confusion_flow.png\")\n",
    "    print(\"  4. failure_breakdown.png\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1acf73c-2fa3-4e48-9710-f9d42ea8fd10",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Detailed Error Analysis for ICHD-3 Diagnostic Engine (NER)\n",
    "======================================================\n",
    "\n",
    "Analyzes which reasoning criteria fail for misclassified patients.\n",
    "Visualizes error patterns and rule failures.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from collections import defaultdict, Counter\n",
    "import re\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 10)\n",
    "\n",
    "# Okabe-Ito colorblind-safe palette\n",
    "COLORS = {\n",
    "    'orange': '#E69F00',\n",
    "    'sky_blue': '#56B4E9', \n",
    "    'bluish_green': '#009E73',\n",
    "    'yellow': '#F0E442',\n",
    "    'vermillion': '#D55E00',\n",
    "    'reddish_purple': '#CC79A7'\n",
    "}\n",
    "\n",
    "\n",
    "def parse_reasoning(reasoning_str):\n",
    "    \"\"\"\n",
    "    Extract specific criteria from reasoning string\n",
    "    \n",
    "    Returns dict with parsed components:\n",
    "    - met_criteria: list of criteria that passed\n",
    "    - failed_criteria: list of criteria that failed\n",
    "    - diagnosis_path: which rule was triggered\n",
    "    \"\"\"\n",
    "    if pd.isna(reasoning_str):\n",
    "        return {\n",
    "            'met_criteria': [],\n",
    "            'failed_criteria': [],\n",
    "            'diagnosis_path': 'Unknown'\n",
    "        }\n",
    "    \n",
    "    met = []\n",
    "    failed = []\n",
    "    diagnosis = 'Unknown'\n",
    "    \n",
    "    # Split by separator\n",
    "    parts = str(reasoning_str).split('|')\n",
    "    \n",
    "    for part in parts:\n",
    "        part = part.strip()\n",
    "        \n",
    "        # Check for diagnosis assignment\n",
    "        if 'diagnosed as' in part.lower() or 'diagnosis:' in part.lower():\n",
    "            diagnosis = part\n",
    "        \n",
    "        # Check for positive criteria\n",
    "        if any(word in part.lower() for word in ['meets', 'has', 'present', 'found', 'detected', '‚úì', 'pass']):\n",
    "            met.append(part)\n",
    "        \n",
    "        # Check for negative criteria\n",
    "        if any(word in part.lower() for word in ['fails', 'missing', 'absent', 'not found', 'insufficient', '‚úó', 'fail']):\n",
    "            failed.append(part)\n",
    "    \n",
    "    return {\n",
    "        'met_criteria': met,\n",
    "        'failed_criteria': failed,\n",
    "        'diagnosis_path': diagnosis\n",
    "    }\n",
    "\n",
    "\n",
    "def extract_criterion_type(criterion_text):\n",
    "    \"\"\"\n",
    "    Categorize a criterion into types\n",
    "    \"\"\"\n",
    "    text = criterion_text.lower()\n",
    "    \n",
    "    # Define criterion categories\n",
    "    if any(word in text for word in ['pain', 'character', 'location', 'intensity', 'unilateral', 'pulsating', 'throbbing']):\n",
    "        return 'Pain Characteristics'\n",
    "    elif any(word in text for word in ['duration', 'hours', '4-72']):\n",
    "        return 'Duration'\n",
    "    elif any(word in text for word in ['frequency', 'attacks', 'episodes', '>=5']):\n",
    "        return 'Frequency'\n",
    "    elif any(word in text for word in ['nausea', 'vomit', 'photophobia', 'phonophobia', 'accompanying']):\n",
    "        return 'Associated Symptoms'\n",
    "    elif any(word in text for word in ['visual', 'sensory', 'dysphasia', 'aura', 'scotoma', 'paresthesia']):\n",
    "        return 'Aura Symptoms'\n",
    "    elif any(word in text for word in ['dysarthria', 'vertigo', 'tinnitus', 'diplopia', 'ataxia', 'brainstem']):\n",
    "        return 'Brainstem Symptoms'\n",
    "    elif any(word in text for word in ['hemiplegic', 'motor', 'weakness', 'paralysis']):\n",
    "        return 'Motor Symptoms'\n",
    "    elif any(word in text for word in ['dpf', 'family', 'familial', 'hereditary']):\n",
    "        return 'Family History'\n",
    "    else:\n",
    "        return 'Other'\n",
    "\n",
    "\n",
    "def analyze_errors(csv_path='engine_output_ner.csv'):\n",
    "    \"\"\"\n",
    "    Comprehensive error analysis\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"DETAILED ERROR ANALYSIS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Load data\n",
    "    df = pd.read_csv(csv_path)\n",
    "    print(f\"\\n‚úì Loaded {len(df)} patients from {csv_path}\")\n",
    "    \n",
    "    # Split into correct and incorrect\n",
    "    correct_df = df[df['match'] == True]\n",
    "    incorrect_df = df[df['match'] == False]\n",
    "    \n",
    "    print(f\"\\nüìä Overall Statistics:\")\n",
    "    print(f\"   Correct: {len(correct_df)} ({len(correct_df)/len(df)*100:.1f}%)\")\n",
    "    print(f\"   Incorrect: {len(incorrect_df)} ({len(incorrect_df)/len(df)*100:.1f}%)\")\n",
    "    \n",
    "    # Analyze error patterns\n",
    "    error_patterns = analyze_error_patterns(incorrect_df)\n",
    "    \n",
    "    # Analyze reasoning failures\n",
    "    reasoning_failures = analyze_reasoning_failures(incorrect_df)\n",
    "    \n",
    "    # Analyze confusion patterns\n",
    "    confusion_patterns = analyze_confusion_patterns(incorrect_df)\n",
    "    \n",
    "    return {\n",
    "        'overall': df,\n",
    "        'correct': correct_df,\n",
    "        'incorrect': incorrect_df,\n",
    "        'error_patterns': error_patterns,\n",
    "        'reasoning_failures': reasoning_failures,\n",
    "        'confusion_patterns': confusion_patterns\n",
    "    }\n",
    "\n",
    "\n",
    "def analyze_error_patterns(incorrect_df):\n",
    "    \"\"\"\n",
    "    Analyze patterns in misclassifications\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"ERROR PATTERNS BY TRUE DIAGNOSIS\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    error_patterns = {}\n",
    "    \n",
    "    for true_diag in incorrect_df['ground_truth'].unique():\n",
    "        mask = incorrect_df['ground_truth'] == true_diag\n",
    "        subset = incorrect_df[mask]\n",
    "        \n",
    "        error_patterns[true_diag] = {\n",
    "            'count': len(subset),\n",
    "            'predicted_as': subset['diagnosis'].value_counts().to_dict(),\n",
    "            'sample_cases': []\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n{true_diag}:\")\n",
    "        print(f\"   Total errors: {len(subset)}\")\n",
    "        print(f\"   Predicted as:\")\n",
    "        for pred, count in subset['diagnosis'].value_counts().items():\n",
    "            pct = count / len(subset) * 100\n",
    "            print(f\"      - {pred}: {count} ({pct:.1f}%)\")\n",
    "        \n",
    "        # Sample cases\n",
    "        for idx, row in subset.head(3).iterrows():\n",
    "            error_patterns[true_diag]['sample_cases'].append({\n",
    "                'patient_id': row['patient_id'],\n",
    "                'predicted': row['diagnosis'],\n",
    "                'confidence': row.get('confidence', 'N/A'),\n",
    "                'reasoning': row.get('reasoning', 'N/A')\n",
    "            })\n",
    "    \n",
    "    return error_patterns\n",
    "\n",
    "\n",
    "def analyze_reasoning_failures(incorrect_df):\n",
    "    \"\"\"\n",
    "    Analyze which criteria fail most often\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"REASONING FAILURE ANALYSIS\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Parse all reasoning strings\n",
    "    failed_criteria_by_type = defaultdict(int)\n",
    "    failed_criteria_by_diagnosis = defaultdict(lambda: defaultdict(int))\n",
    "    \n",
    "    for idx, row in incorrect_df.iterrows():\n",
    "        parsed = parse_reasoning(row.get('reasoning', ''))\n",
    "        true_diag = row['ground_truth']\n",
    "        \n",
    "        for criterion in parsed['failed_criteria']:\n",
    "            criterion_type = extract_criterion_type(criterion)\n",
    "            failed_criteria_by_type[criterion_type] += 1\n",
    "            failed_criteria_by_diagnosis[true_diag][criterion_type] += 1\n",
    "    \n",
    "    # Print overall failure counts\n",
    "    print(\"\\nüìä Most Common Criterion Failures (Overall):\")\n",
    "    sorted_failures = sorted(failed_criteria_by_type.items(), key=lambda x: x[1], reverse=True)\n",
    "    for criterion_type, count in sorted_failures:\n",
    "        pct = count / len(incorrect_df) * 100\n",
    "        print(f\"   {criterion_type}: {count} ({pct:.1f}%)\")\n",
    "    \n",
    "    # Print by diagnosis\n",
    "    print(f\"\\n{'‚îÄ'*80}\")\n",
    "    print(\"Criterion Failures by True Diagnosis:\")\n",
    "    print(f\"{'‚îÄ'*80}\")\n",
    "    \n",
    "    for true_diag in sorted(failed_criteria_by_diagnosis.keys()):\n",
    "        print(f\"\\n{true_diag}:\")\n",
    "        failures = failed_criteria_by_diagnosis[true_diag]\n",
    "        sorted_diag_failures = sorted(failures.items(), key=lambda x: x[1], reverse=True)\n",
    "        for criterion_type, count in sorted_diag_failures[:5]:  # Top 5\n",
    "            print(f\"   - {criterion_type}: {count}\")\n",
    "    \n",
    "    return {\n",
    "        'by_type': dict(failed_criteria_by_type),\n",
    "        'by_diagnosis': dict(failed_criteria_by_diagnosis)\n",
    "    }\n",
    "\n",
    "\n",
    "def analyze_confusion_patterns(incorrect_df):\n",
    "    \"\"\"\n",
    "    Analyze which diagnoses are confused with each other\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"CONFUSION PATTERNS\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    confusion_pairs = []\n",
    "    \n",
    "    for idx, row in incorrect_df.iterrows():\n",
    "        confusion_pairs.append((row['ground_truth'], row['diagnosis']))\n",
    "    \n",
    "    confusion_counts = Counter(confusion_pairs)\n",
    "    \n",
    "    print(\"\\nMost Common Misclassifications:\")\n",
    "    for (true_diag, pred_diag), count in confusion_counts.most_common(10):\n",
    "        print(f\"   {true_diag} ‚Üí {pred_diag}: {count}\")\n",
    "    \n",
    "    return dict(confusion_counts)\n",
    "\n",
    "\n",
    "def visualize_error_analysis(analysis, output_dir='evaluation_results/symbolic'):\n",
    "    \"\"\"\n",
    "    Create visualizations for error analysis\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"GENERATING VISUALIZATIONS\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    output_dir = Path(output_dir)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # 1. Error rate by diagnosis\n",
    "    plot_error_rates(analysis, output_dir)\n",
    "    \n",
    "    # 2. Criterion failure heatmap\n",
    "    plot_criterion_failures(analysis, output_dir)\n",
    "    \n",
    "    # 3. Confusion flow diagram\n",
    "    plot_confusion_flow(analysis, output_dir)\n",
    "    \n",
    "    # 4. Detailed error breakdown\n",
    "    plot_error_breakdown(analysis, output_dir)\n",
    "\n",
    "\n",
    "def plot_error_rates(analysis, output_dir):\n",
    "    \"\"\"\n",
    "    Plot error rates for each diagnosis type\n",
    "    \"\"\"\n",
    "    \n",
    "    df = analysis['overall']\n",
    "    \n",
    "    # Calculate error rate per diagnosis\n",
    "    error_rates = {}\n",
    "    for diag in df['ground_truth'].unique():\n",
    "        mask = df['ground_truth'] == diag\n",
    "        total = mask.sum()\n",
    "        errors = ((df['ground_truth'] == diag) & (df['match'] == False)).sum()\n",
    "        error_rates[diag] = {\n",
    "            'error_rate': errors / total if total > 0 else 0,\n",
    "            'total': total,\n",
    "            'errors': errors,\n",
    "            'correct': total - errors\n",
    "        }\n",
    "    \n",
    "    # Sort by error rate\n",
    "    sorted_diags = sorted(error_rates.keys(), key=lambda x: error_rates[x]['error_rate'], reverse=True)\n",
    "    \n",
    "    # Create figure\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # Left: Error rates\n",
    "    error_pcts = [error_rates[d]['error_rate'] * 100 for d in sorted_diags]\n",
    "    colors = [COLORS['vermillion'] if pct > 50 else COLORS['orange'] if pct > 30 else COLORS['bluish_green'] for pct in error_pcts]\n",
    "    \n",
    "    bars1 = ax1.barh(range(len(sorted_diags)), error_pcts, color=colors, alpha=0.8)\n",
    "    ax1.set_yticks(range(len(sorted_diags)))\n",
    "    ax1.set_yticklabels(sorted_diags, fontsize=16)\n",
    "    ax1.set_xlabel('Error Rate (%)', fontsize=14, fontweight='bold')\n",
    "    ax1.set_title('Error Rate by Diagnosis Type', fontsize=14, fontweight='bold', pad=15)\n",
    "    ax1.axvline(50, color='red', linestyle='--', alpha=0.3, linewidth=1)\n",
    "    ax1.axvline(30, color='orange', linestyle='--', alpha=0.3, linewidth=1)\n",
    "    ax1.grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, (bar, pct) in enumerate(zip(bars1, error_pcts)):\n",
    "        ax1.text(pct + 1, i, f'{pct:.1f}%', va='center', fontsize=9)\n",
    "    \n",
    "    # Right: Correct vs Incorrect counts\n",
    "    diagnoses = sorted_diags\n",
    "    correct_counts = [error_rates[d]['correct'] for d in diagnoses]\n",
    "    error_counts = [error_rates[d]['errors'] for d in diagnoses]\n",
    "    \n",
    "    y_pos = np.arange(len(diagnoses))\n",
    "    \n",
    "    ax2.barh(y_pos, correct_counts, color=COLORS['bluish_green'], alpha=0.8, label='Correct')\n",
    "    ax2.barh(y_pos, error_counts, left=correct_counts, color=COLORS['vermillion'], alpha=0.8, label='Errors')\n",
    "    \n",
    "    ax2.set_yticks(y_pos)\n",
    "    ax2.set_yticklabels(diagnoses, fontsize=16)\n",
    "    ax2.set_xlabel('Number of Patients', fontsize=14, fontweight='bold')\n",
    "    ax2.set_title('Correct vs Incorrect Predictions', fontsize=14, fontweight='bold', pad=15)\n",
    "    ax2.legend(loc='lower right', fontsize=14)\n",
    "    ax2.grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    output_path = output_dir / 'error_rates_by_diagnosis.png'\n",
    "    plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"‚úì Saved: {output_path}\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_criterion_failures(analysis, output_dir):\n",
    "    \"\"\"\n",
    "    Plot heatmap of criterion failures by diagnosis\n",
    "    \"\"\"\n",
    "    \n",
    "    reasoning_failures = analysis['reasoning_failures']['by_diagnosis']\n",
    "    \n",
    "    if not reasoning_failures:\n",
    "        print(\"‚ö† No reasoning failure data available, skipping criterion failure plot\")\n",
    "        return\n",
    "    \n",
    "    # Create matrix\n",
    "    all_criterion_types = set()\n",
    "    for diag_failures in reasoning_failures.values():\n",
    "        all_criterion_types.update(diag_failures.keys())\n",
    "    \n",
    "    criterion_types = sorted(all_criterion_types)\n",
    "    diagnoses = sorted(reasoning_failures.keys())\n",
    "    \n",
    "    # Build matrix\n",
    "    matrix = np.zeros((len(diagnoses), len(criterion_types)))\n",
    "    \n",
    "    for i, diag in enumerate(diagnoses):\n",
    "        for j, criterion in enumerate(criterion_types):\n",
    "            matrix[i, j] = reasoning_failures[diag].get(criterion, 0)\n",
    "    \n",
    "    # Create figure\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    \n",
    "    # Plot heatmap with inferno colormap\n",
    "    sns.heatmap(matrix, annot=True, fmt='g', cmap='inferno',\n",
    "                xticklabels=criterion_types, yticklabels=diagnoses,\n",
    "                ax=ax, cbar_kws={'label': 'Failure Count'})\n",
    "    \n",
    "    ax.set_title('Criterion Failures by Diagnosis Type (Inferno)', \n",
    "                 fontsize=14, fontweight='bold', pad=15)\n",
    "    ax.set_xlabel('Criterion Type', fontsize=14, fontweight='bold')\n",
    "    ax.set_ylabel('True Diagnosis', fontsize=14, fontweight='bold')\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "    plt.setp(ax.get_yticklabels(), rotation=0)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    output_path = output_dir / 'criterion_failures_heatmap.png'\n",
    "    plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"‚úì Saved: {output_path}\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_confusion_flow(analysis, output_dir):\n",
    "    \"\"\"\n",
    "    Plot sankey-style confusion flow\n",
    "    \"\"\"\n",
    "    \n",
    "    confusion_patterns = analysis['confusion_patterns']\n",
    "    \n",
    "    if not confusion_patterns:\n",
    "        print(\"‚ö† No confusion pattern data, skipping flow plot\")\n",
    "        return\n",
    "    \n",
    "    # Get top 10 confusions\n",
    "    top_confusions = sorted(confusion_patterns.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "    \n",
    "    # Create bar chart\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    \n",
    "    labels = [f\"{true} ‚Üí {pred}\" for (true, pred), count in top_confusions]\n",
    "    counts = [count for (true, pred), count in top_confusions]\n",
    "    \n",
    "    bars = ax.barh(range(len(labels)), counts, color=COLORS['yellow'], alpha=0.8)\n",
    "    ax.set_yticks(range(len(labels)))\n",
    "    ax.set_yticklabels(labels, fontsize=16)\n",
    "    ax.set_xlabel('Number of Misclassifications', fontsize=14, fontweight='bold')\n",
    "    ax.set_title('Top 10 Confusion Patterns', fontsize=14, fontweight='bold', pad=15)\n",
    "    ax.grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, (bar, count) in enumerate(zip(bars, counts)):\n",
    "        ax.text(count + 0.5, i, str(count), va='center', fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    output_path = output_dir / 'confusion_flow.png'\n",
    "    plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"‚úì Saved: {output_path}\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_error_breakdown(analysis, output_dir):\n",
    "    \"\"\"\n",
    "    Plot detailed breakdown of failure types\n",
    "    \"\"\"\n",
    "    \n",
    "    reasoning_failures = analysis['reasoning_failures']['by_type']\n",
    "    \n",
    "    if not reasoning_failures:\n",
    "        print(\"‚ö† No failure type data, skipping breakdown plot\")\n",
    "        return\n",
    "    \n",
    "    # Sort by count\n",
    "    sorted_failures = sorted(reasoning_failures.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    criterion_types = [item[0] for item in sorted_failures]\n",
    "    counts = [item[1] for item in sorted_failures]\n",
    "    \n",
    "    # Create figure\n",
    "    fig, ax = plt.subplots(figsize=(12, 7))\n",
    "    \n",
    "    # Color by severity\n",
    "    max_count = max(counts)\n",
    "    colors = []\n",
    "    for count in counts:\n",
    "        if count > max_count * 0.6:\n",
    "            colors.append(COLORS['yellow'])\n",
    "        elif count > max_count * 0.3:\n",
    "            colors.append(COLORS['orange'])\n",
    "        else:\n",
    "            colors.append(COLORS['bluish_green'])\n",
    "    \n",
    "    bars = ax.barh(range(len(criterion_types)), counts, color=colors, alpha=0.8)\n",
    "    ax.set_yticks(range(len(criterion_types)))\n",
    "    ax.set_yticklabels(criterion_types, fontsize=16)\n",
    "    ax.set_xlabel('Number of Failures', fontsize=14, fontweight='bold')\n",
    "    ax.set_title('Criterion Failure Breakdown (All Misclassifications)', \n",
    "                 fontsize=14, fontweight='bold', pad=15)\n",
    "    ax.grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, (bar, count) in enumerate(zip(bars, counts)):\n",
    "        ax.text(count + max_count*0.01, i, str(count), va='center', fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    output_path = output_dir / 'failure_breakdown.png'\n",
    "    plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"‚úì Saved: {output_path}\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def print_detailed_examples(analysis, num_examples=5):\n",
    "    \"\"\"\n",
    "    Print detailed examples of errors\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"DETAILED ERROR EXAMPLES (First {num_examples} per diagnosis)\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    incorrect_df = analysis['incorrect']\n",
    "    \n",
    "    for true_diag in sorted(incorrect_df['ground_truth'].unique()):\n",
    "        subset = incorrect_df[incorrect_df['ground_truth'] == true_diag].head(num_examples)\n",
    "        \n",
    "        print(f\"\\n{'‚îÄ'*80}\")\n",
    "        print(f\"TRUE DIAGNOSIS: {true_diag}\")\n",
    "        print(f\"{'‚îÄ'*80}\")\n",
    "        \n",
    "        for idx, row in subset.iterrows():\n",
    "            print(f\"\\n  Patient {row['patient_id']}:\")\n",
    "            print(f\"    Predicted: {row['diagnosis']}\")\n",
    "            print(f\"    Confidence: {row.get('confidence', 'N/A')}\")\n",
    "            print(f\"    Reasoning:\")\n",
    "            \n",
    "            reasoning = str(row.get('reasoning', 'N/A'))\n",
    "            reasoning_parts = reasoning.split('|')\n",
    "            for part in reasoning_parts[:5]:  # Show first 5 parts\n",
    "                print(f\"      ‚Ä¢ {part.strip()}\")\n",
    "            if len(reasoning_parts) > 5:\n",
    "                print(f\"      ... ({len(reasoning_parts) - 5} more criteria)\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üîç CHECKING FOR ENGINE OUTPUT...\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Check for file\n",
    "    possible_paths = [\n",
    "        'engine_output_ner.csv'\n",
    "    ]\n",
    "    \n",
    "    csv_path = None\n",
    "    for path in possible_paths:\n",
    "        if Path(path).exists():\n",
    "            csv_path = path\n",
    "            print(f\"‚úì Found: {path}\")\n",
    "            break\n",
    "    \n",
    "    if csv_path is None:\n",
    "        print(\"\\n‚ùå ERROR: engine_output.csv not found!\")\n",
    "        print(\"\\nPlease run your engine first to generate engine_output.csv\")\n",
    "        print(\"See: /mnt/user-data/outputs/QUICK_START.txt\")\n",
    "        return\n",
    "    \n",
    "    # Run analysis\n",
    "    analysis = analyze_errors(csv_path)\n",
    "    \n",
    "    # Create visualizations\n",
    "    visualize_error_analysis(analysis)\n",
    "    \n",
    "    # Print detailed examples\n",
    "    print_detailed_examples(analysis, num_examples=3)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"‚úÖ ERROR ANALYSIS COMPLETE!\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"\\nGenerated files in: evaluation_results/symbolic_original/\")\n",
    "    print(\"  1. error_rates_by_diagnosis.png\")\n",
    "    print(\"  2. criterion_failures_heatmap.png (inferno colormap)\")\n",
    "    print(\"  3. confusion_flow.png\")\n",
    "    print(\"  4. failure_breakdown.png\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d724137-f205-44e9-9b70-bfbbf70377ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
