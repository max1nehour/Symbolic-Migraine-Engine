{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e058c4d-b13e-4488-8733-dd64d121e742",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.getcwd())\n",
    "os.chdir(\"/Users/M1HR/Desktop/MIGRAINE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94e1374-3fd3-429d-bb6b-d1e0565c1899",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 1. NORMALIZATION FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "def normalize_location(location_text):\n",
    "\n",
    "    if not location_text or pd.isna(location_text):\n",
    "        return 0\n",
    "    \n",
    "    loc_str = str(location_text).lower()\n",
    "    \n",
    "    # Check for bilateral\n",
    "    bilateral_terms = ['bilateral', 'both', 'everywhere', 'all over', 'entire head']\n",
    "    if any(term in loc_str for term in bilateral_terms):\n",
    "        return 2\n",
    "    \n",
    "    # Check for unilateral\n",
    "    unilateral_terms = ['left', 'right', 'side', 'temple', 'unilateral']\n",
    "    if any(term in loc_str for term in unilateral_terms):\n",
    "        return 1\n",
    "    \n",
    "    return 0\n",
    "\n",
    "\n",
    "def normalize_character(character_text):\n",
    "   \n",
    "    if not character_text or pd.isna(character_text):\n",
    "        return 0\n",
    "    \n",
    "    char_str = str(character_text).lower()\n",
    "    \n",
    "    # Check for throbbing/pulsating\n",
    "    throbbing_terms = ['throb', 'puls', 'pound', 'beat']\n",
    "    if any(term in char_str for term in throbbing_terms):\n",
    "        return 1\n",
    "    \n",
    "    # Check for constant/pressing\n",
    "    constant_terms = ['constant', 'steady', 'press', 'tight', 'squeeze']\n",
    "    if any(term in char_str for term in constant_terms):\n",
    "        return 2\n",
    "    \n",
    "    return 0\n",
    "\n",
    "\n",
    "def normalize_intensity(intensity_text, intensity_numeric):\n",
    "\n",
    "    # Try text field first (preferred)\n",
    "    if intensity_text and not pd.isna(intensity_text):\n",
    "        int_str = str(intensity_text).lower()\n",
    "        \n",
    "        if 'severe' in int_str:\n",
    "            return 3\n",
    "        elif 'moderate' in int_str or 'medium' in int_str:\n",
    "            return 2\n",
    "        elif 'mild' in int_str:\n",
    "            return 1\n",
    "    \n",
    "    # Try numeric field (fallback)\n",
    "    if intensity_numeric and not pd.isna(intensity_numeric):\n",
    "        try:\n",
    "            int_val = float(intensity_numeric)\n",
    "            if int_val >= 2.5:\n",
    "                return 3  # Severe\n",
    "            elif int_val >= 1.5:\n",
    "                return 2  # Moderate\n",
    "            elif int_val >= 0.5:\n",
    "                return 1  # Mild\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    return 0\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 2. LOAD AND PREPARE DATA\n",
    "# =============================================================================\n",
    "\n",
    "def load_ner_results(json_path):\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"LOADING NER-EXTRACTED ENTITIES\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    with open(json_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    print(f\"✓ Loaded {len(data)} patients\")\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Normalize column names (capitalize first letter)\n",
    "    df = df.rename(columns={\n",
    "        'patient_id': 'Patient_ID',\n",
    "        'duration': 'Duration',\n",
    "        'intensity': 'Intensity',\n",
    "        'intensity_text': 'Intensity_Text',\n",
    "        'location': 'Location',\n",
    "        'character': 'Character',\n",
    "        'frequency': 'Frequency',\n",
    "        'nausea': 'Nausea',\n",
    "        'vomit': 'Vomit',\n",
    "        'photophobia': 'Photophobia',\n",
    "        'phonophobia': 'Phonophobia',\n",
    "        'visual': 'Visual',\n",
    "        'sensory': 'Sensory',\n",
    "        'dysphasia': 'Dysphasia',\n",
    "        'dysarthria': 'Dysarthria',\n",
    "        'vertigo': 'Vertigo',\n",
    "        'tinnitus': 'Tinnitus',\n",
    "        'hypoacusis': 'Hypoacusis',\n",
    "        'diplopia': 'Diplopia',\n",
    "        'ataxia': 'Ataxia',\n",
    "        'conscience': 'Conscience',\n",
    "        'visual_defect': 'Visual_defect',\n",
    "        'paresthesia': 'Paresthesia',\n",
    "        'dpf': 'DPF'\n",
    "    })\n",
    "    \n",
    "    # Normalize text fields to numeric\n",
    "    print(\"\\nNormalizing text fields to numeric values...\")\n",
    "    \n",
    "    df['Location_Normalized'] = df.apply(\n",
    "        lambda row: normalize_location(row.get('Location')), axis=1\n",
    "    )\n",
    "    \n",
    "    df['Character_Normalized'] = df.apply(\n",
    "        lambda row: normalize_character(row.get('Character')), axis=1\n",
    "    )\n",
    "    \n",
    "    df['Intensity_Normalized'] = df.apply(\n",
    "        lambda row: normalize_intensity(\n",
    "            row.get('Intensity_Text'), \n",
    "            row.get('Intensity')\n",
    "        ), axis=1\n",
    "    )\n",
    "    \n",
    "    print(f\" Normalized location: {df['Location_Normalized'].value_counts().to_dict()}\")\n",
    "    print(f\" Normalized character: {df['Character_Normalized'].value_counts().to_dict()}\")\n",
    "    print(f\" Normalized intensity: {df['Intensity_Normalized'].value_counts().to_dict()}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def load_kaggle_data(csv_path):\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"LOADING ORIGINAL KAGGLE DATA\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    df = pd.read_csv(csv_path)\n",
    "    \n",
    "    # Add Patient_ID if not present\n",
    "    if 'Patient_ID' not in df.columns:\n",
    "        df['Patient_ID'] = df.index + 1\n",
    "    \n",
    "    print(f\" Loaded {len(df)} patients\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 3. COMPARISON FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "def compare_numeric_field(kaggle_val, ner_val, tolerance=0.2):\n",
    "   \n",
    "    # Handle NaN\n",
    "    kaggle_present = not pd.isna(kaggle_val) and kaggle_val != 0\n",
    "    ner_present = not pd.isna(ner_val) and ner_val != 0\n",
    "    \n",
    "    if not kaggle_present:\n",
    "        return 'na'\n",
    "    \n",
    "    if not ner_present:\n",
    "        return 'missing'\n",
    "    \n",
    "    try:\n",
    "        k_num = float(kaggle_val)\n",
    "        n_num = float(ner_val)\n",
    "        \n",
    "        # Check relative difference\n",
    "        if k_num == 0:\n",
    "            return 'match' if n_num == 0 else 'mismatch'\n",
    "        \n",
    "        rel_diff = abs(k_num - n_num) / abs(k_num)\n",
    "        return 'match' if rel_diff <= tolerance else 'mismatch'\n",
    "    \n",
    "    except:\n",
    "        return 'mismatch'\n",
    "\n",
    "\n",
    "def compare_binary_field(kaggle_val, ner_val):\n",
    "\n",
    "    kaggle_present = not pd.isna(kaggle_val) and float(kaggle_val) > 0\n",
    "    ner_present = not pd.isna(ner_val) and float(ner_val) > 0\n",
    "    \n",
    "    if kaggle_present and ner_present:\n",
    "        return 'match'\n",
    "    elif kaggle_present and not ner_present:\n",
    "        return 'missing'\n",
    "    elif not kaggle_present and ner_present:\n",
    "        return 'hallucination'\n",
    "    else:\n",
    "        return 'na'\n",
    "\n",
    "\n",
    "def compare_categorical_field(kaggle_val, ner_val):\n",
    "\n",
    "    kaggle_present = not pd.isna(kaggle_val) and float(kaggle_val) > 0\n",
    "    ner_present = not pd.isna(ner_val) and float(ner_val) > 0\n",
    "    \n",
    "    if not kaggle_present:\n",
    "        return 'na'\n",
    "    \n",
    "    if not ner_present:\n",
    "        return 'missing'\n",
    "    \n",
    "    return 'match' if float(kaggle_val) == float(ner_val) else 'mismatch'\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 4. COMPREHENSIVE COMPARISON\n",
    "# =============================================================================\n",
    "\n",
    "def compare_datasets(kaggle_df, ner_df):\n",
    "\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"COMPARING DATASETS\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Merge datasets\n",
    "    merged = kaggle_df.merge(\n",
    "        ner_df,\n",
    "        on='Patient_ID',\n",
    "        how='inner',\n",
    "        suffixes=('_kaggle', '_ner')\n",
    "    )\n",
    "    \n",
    "    print(f\"✓ Merged {len(merged)} patients\")\n",
    "    \n",
    "    # Define fields to compare\n",
    "    fields_to_compare = {\n",
    "        # Categorical (normalized)\n",
    "        'Location': {\n",
    "            'kaggle': 'Location_kaggle',\n",
    "            'ner': 'Location_Normalized',\n",
    "            'type': 'categorical'\n",
    "        },\n",
    "        'Character': {\n",
    "            'kaggle': 'Character_kaggle',\n",
    "            'ner': 'Character_Normalized',\n",
    "            'type': 'categorical'\n",
    "        },\n",
    "        'Intensity': {\n",
    "            'kaggle': 'Intensity_kaggle',\n",
    "            'ner': 'Intensity_Normalized',\n",
    "            'type': 'categorical'\n",
    "        },\n",
    "        \n",
    "        # Numeric\n",
    "        'Duration': {\n",
    "            'kaggle': 'Duration_kaggle',\n",
    "            'ner': 'Duration_ner',\n",
    "            'type': 'numeric'\n",
    "        },\n",
    "        'Frequency': {\n",
    "            'kaggle': 'Frequency_kaggle',\n",
    "            'ner': 'Frequency_ner',\n",
    "            'type': 'numeric'\n",
    "        },\n",
    "        \n",
    "        # Binary\n",
    "        'Nausea': {'kaggle': 'Nausea_kaggle', 'ner': 'Nausea_ner', 'type': 'binary'},\n",
    "        'Vomit': {'kaggle': 'Vomit_kaggle', 'ner': 'Vomit_ner', 'type': 'binary'},\n",
    "        'Photophobia': {'kaggle': 'Photophobia_kaggle', 'ner': 'Photophobia_ner', 'type': 'binary'},\n",
    "        'Phonophobia': {'kaggle': 'Phonophobia_kaggle', 'ner': 'Phonophobia_ner', 'type': 'binary'},\n",
    "        'Visual': {'kaggle': 'Visual_kaggle', 'ner': 'Visual_ner', 'type': 'binary'},\n",
    "        'Sensory': {'kaggle': 'Sensory_kaggle', 'ner': 'Sensory_ner', 'type': 'binary'},\n",
    "        'Dysphasia': {'kaggle': 'Dysphasia_kaggle', 'ner': 'Dysphasia_ner', 'type': 'binary'},\n",
    "        'Dysarthria': {'kaggle': 'Dysarthria_kaggle', 'ner': 'Dysarthria_ner', 'type': 'binary'},\n",
    "        'Vertigo': {'kaggle': 'Vertigo_kaggle', 'ner': 'Vertigo_ner', 'type': 'binary'},\n",
    "        'Tinnitus': {'kaggle': 'Tinnitus_kaggle', 'ner': 'Tinnitus_ner', 'type': 'binary'},\n",
    "        'DPF': {'kaggle': 'DPF_kaggle', 'ner': 'DPF_ner', 'type': 'binary'},\n",
    "    }\n",
    "    \n",
    "    # Compare each field\n",
    "    results = defaultdict(lambda: defaultdict(int))\n",
    "    per_patient_results = []\n",
    "    \n",
    "    for idx, row in merged.iterrows():\n",
    "        patient_id = row['Patient_ID']\n",
    "        patient_matches = 0\n",
    "        patient_total = 0\n",
    "        \n",
    "        for field_name, config in fields_to_compare.items():\n",
    "            kaggle_col = config['kaggle']\n",
    "            ner_col = config['ner']\n",
    "            field_type = config['type']\n",
    "            \n",
    "            kaggle_val = row.get(kaggle_col)\n",
    "            ner_val = row.get(ner_col)\n",
    "            \n",
    "            # Compare based on type\n",
    "            if field_type == 'numeric':\n",
    "                result = compare_numeric_field(kaggle_val, ner_val)\n",
    "            elif field_type == 'binary':\n",
    "                result = compare_binary_field(kaggle_val, ner_val)\n",
    "            elif field_type == 'categorical':\n",
    "                result = compare_categorical_field(kaggle_val, ner_val)\n",
    "            else:\n",
    "                result = 'unknown'\n",
    "            \n",
    "            # Update counters\n",
    "            results[field_name][result] += 1\n",
    "            \n",
    "            if result not in ['na']:\n",
    "                patient_total += 1\n",
    "                if result == 'match':\n",
    "                    patient_matches += 1\n",
    "        \n",
    "        per_patient_results.append({\n",
    "            'Patient_ID': patient_id,\n",
    "            'matches': patient_matches,\n",
    "            'total': patient_total,\n",
    "            'accuracy': patient_matches / patient_total if patient_total > 0 else 0\n",
    "        })\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"PER-FIELD COMPARISON RESULTS\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    summary_records = []\n",
    "    \n",
    "    for field_name, counts in sorted(results.items()):\n",
    "        total_applicable = sum(v for k, v in counts.items() if k != 'na')\n",
    "        \n",
    "        if total_applicable > 0:\n",
    "            match_rate = counts['match'] / total_applicable\n",
    "            missing_rate = counts['missing'] / total_applicable\n",
    "            mismatch_rate = counts.get('mismatch', 0) / total_applicable\n",
    "            \n",
    "            print(f\"{field_name:15s}: \"\n",
    "                  f\"Match {match_rate*100:5.1f}%  \"\n",
    "                  f\"Missing {missing_rate*100:5.1f}%  \"\n",
    "                  f\"Mismatch {mismatch_rate*100:5.1f}%  \"\n",
    "                  f\"(n={total_applicable})\")\n",
    "            \n",
    "            summary_records.append({\n",
    "                'field': field_name,\n",
    "                'match_rate': match_rate,\n",
    "                'missing_rate': missing_rate,\n",
    "                'mismatch_rate': mismatch_rate,\n",
    "                'total_applicable': total_applicable\n",
    "            })\n",
    "    \n",
    "    # Overall statistics\n",
    "    total_matches = sum(p['matches'] for p in per_patient_results)\n",
    "    total_comparisons = sum(p['total'] for p in per_patient_results)\n",
    "    overall_accuracy = total_matches / total_comparisons if total_comparisons > 0 else 0\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"OVERALL STATISTICS\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    print(f\"  Total Comparisons: {total_comparisons}\")\n",
    "    print(f\"  Matches:           {total_matches} ({overall_accuracy*100:.1f}%)\")\n",
    "    print(f\"  Mismatches:        {total_comparisons - total_matches} ({(1-overall_accuracy)*100:.1f}%)\")\n",
    "    print(f\"  Overall Accuracy:  {overall_accuracy:.4f}\")\n",
    "    \n",
    "    return {\n",
    "        'per_field': results,\n",
    "        'per_patient': per_patient_results,\n",
    "        'summary': summary_records,\n",
    "        'overall_accuracy': overall_accuracy\n",
    "    }\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 5. SAVE RESULTS\n",
    "# =============================================================================\n",
    "\n",
    "def save_comparison_results(comparison_results, output_dir):\n",
    "\n",
    "    output_dir = Path(output_dir)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"SAVING RESULTS\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Per-field summary\n",
    "    summary_df = pd.DataFrame(comparison_results['summary'])\n",
    "    summary_df = summary_df.sort_values('match_rate', ascending=False)\n",
    "    summary_path = output_dir / 'field_comparison_summary.csv'\n",
    "    summary_df.to_csv(summary_path, index=False)\n",
    "    print(f\"✓ Field summary: {summary_path}\")\n",
    "    \n",
    "    # Per-patient results\n",
    "    patient_df = pd.DataFrame(comparison_results['per_patient'])\n",
    "    patient_path = output_dir / 'patient_comparison.csv'\n",
    "    patient_df.to_csv(patient_path, index=False)\n",
    "    print(f\"✓ Patient results: {patient_path}\")\n",
    "    \n",
    "    # Overall metrics\n",
    "    overall_path = output_dir / 'overall_metrics.json'\n",
    "    with open(overall_path, 'w') as f:\n",
    "        json.dump({\n",
    "            'overall_accuracy': comparison_results['overall_accuracy']\n",
    "        }, f, indent=2)\n",
    "    print(f\"✓ Overall metrics: {overall_path}\")\n",
    "    \n",
    "    print(f\"\\n✓ All results saved to: {output_dir}\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 6. MAIN\n",
    "# =============================================================================\n",
    "\n",
    "def compare_ner_with_kaggle(\n",
    "    ner_json_path='data/ner_results/patient_summaries_fixed.json',\n",
    "    kaggle_csv_path='data/migraine_with_id.csv',\n",
    "    output_dir='evaluation_results/ner_comparison'\n",
    "):\n",
    "\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"NER EXTRACTION vs ORIGINAL DATA COMPARISON\")\n",
    "    print(\"=\"*80)\n",
    "   \n",
    "    \n",
    "    # Load data\n",
    "    ner_df = load_ner_results(ner_json_path)\n",
    "    kaggle_df = load_kaggle_data(kaggle_csv_path)\n",
    "    \n",
    "    # Compare\n",
    "    results = compare_datasets(kaggle_df, ner_df)\n",
    "    \n",
    "    # Save\n",
    "    save_comparison_results(results, output_dir)\n",
    "    \n",
    "    # Final summary\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\" COMPARISON COMPLETE!\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(f\"\\nKEY FINDINGS:\")\n",
    "    print(f\"  Overall Accuracy: {results['overall_accuracy']:.4f} ({results['overall_accuracy']*100:.1f}%)\")\n",
    "    \n",
    "    # Show best and worst fields\n",
    "    summary_df = pd.DataFrame(results['summary'])\n",
    "    summary_df = summary_df.sort_values('match_rate', ascending=False)\n",
    "    \n",
    "    print(f\"\\n  Best Preserved (Top 3):\")\n",
    "    for _, row in summary_df.head(3).iterrows():\n",
    "        print(f\"    {row['field']:15s}: {row['match_rate']*100:5.1f}%\")\n",
    "    \n",
    "    print(f\"\\n  Worst Preserved (Bottom 3):\")\n",
    "    for _, row in summary_df.tail(3).iterrows():\n",
    "        print(f\"    {row['field']:15s}: {row['match_rate']*100:5.1f}%\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    results = compare_ner_with_kaggle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308197a6-1936-423a-8482-17aa9c51d965",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 8)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 1. FIELD COMPARISON VISUALIZATIONS\n",
    "# =============================================================================\n",
    "\n",
    "def plot_field_match_rates(df, output_path):\n",
    "    \n",
    "    # Sort by match rate\n",
    "    df = df.sort_values('match_rate', ascending=True)\n",
    "    \n",
    "    okabe_ito = {\n",
    "        'orange': '#E69F00',      # Excellent\n",
    "        'sky_blue': '#56B4E9',    # Good\n",
    "        'bluish_green': '#009E73', # Fair\n",
    "        'yellow': '#F0E442'       # Poor\n",
    "    }\n",
    "    \n",
    "    colors = []\n",
    "    for rate in df['match_rate']:\n",
    "        if rate >= 0.9:\n",
    "            colors.append(okabe_ito['orange'])      # Excellent\n",
    "        elif rate >= 0.8:\n",
    "            colors.append(okabe_ito['sky_blue'])    # Good\n",
    "        elif rate >= 0.7:\n",
    "            colors.append(okabe_ito['bluish_green']) # Fair\n",
    "        else:\n",
    "            colors.append(okabe_ito['yellow'])      # Poor\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12, max(8, len(df) * 0.4)))\n",
    "    \n",
    "    bars = ax.barh(df['field'], df['match_rate'], color=colors, alpha=0.85, edgecolor='black', linewidth=1)\n",
    "    \n",
    "    ax.set_xlabel('Match Rate', fontsize=13, fontweight='bold')\n",
    "    ax.set_ylabel('Clinical Attribute', fontsize=13, fontweight='bold')\n",
    "    ax.set_title('NER Extraction: Information Preservation by Attribute', \n",
    "                 fontsize=15, fontweight='bold', pad=20)\n",
    "    ax.set_xlim([0, 1.05])\n",
    "    \n",
    "    # Add grid\n",
    "    ax.grid(axis='x', alpha=0.3, linestyle='--')\n",
    "    ax.set_axisbelow(True)\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, (bar, val, total) in enumerate(zip(bars, df['match_rate'], df['total_applicable'])):\n",
    "        width = bar.get_width()\n",
    "        label = f'{val*100:.1f}%'\n",
    "        \n",
    "        # Position text inside or outside bar depending on width\n",
    "        if width > 0.15:\n",
    "            ax.text(width - 0.03, i, label, \n",
    "                   ha='right', va='center', fontsize=10, fontweight='bold', color='white')\n",
    "        else:\n",
    "            ax.text(width + 0.02, i, label,\n",
    "                   ha='left', va='center', fontsize=10, fontweight='bold')\n",
    "        \n",
    "        # Add sample size annotation\n",
    "        ax.text(1.01, i, f'n={int(total)}',\n",
    "               ha='left', va='center', fontsize=9, style='italic', color='gray')\n",
    "    \n",
    "    # Add performance level legend - Okabe-Ito palette\n",
    "    from matplotlib.patches import Patch\n",
    "    legend_elements = [\n",
    "        Patch(facecolor='#E69F00', alpha=0.85, label='Excellent (≥90%)'),\n",
    "        Patch(facecolor='#56B4E9', alpha=0.85, label='Good (80-89%)'),\n",
    "        Patch(facecolor='#009E73', alpha=0.85, label='Fair (70-79%)'),\n",
    "        Patch(facecolor='#F0E442', alpha=0.85, label='Poor (<70%)')\n",
    "    ]\n",
    "    ax.legend(handles=legend_elements, loc='lower right', framealpha=0.95)\n",
    "    \n",
    "    # Add reference lines - Okabe-Ito palette\n",
    "    ax.axvline(x=0.9, color='#E69F00', linestyle='--', alpha=0.4, linewidth=1.5)\n",
    "    ax.axvline(x=0.8, color='#56B4E9', linestyle='--', alpha=0.4, linewidth=1.5)\n",
    "    ax.axvline(x=0.7, color='#009E73', linestyle='--', alpha=0.4, linewidth=1.5)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\" Saved: {output_path}\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_field_breakdown_stacked(df, output_path):\n",
    "\n",
    "    \n",
    "    # Sort by match rate\n",
    "    df = df.sort_values('match_rate', ascending=True)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12, max(8, len(df) * 0.4)))\n",
    "    \n",
    "    # Create stacked bars\n",
    "    fields = df['field']\n",
    "    match = df['match_rate'] * 100\n",
    "    missing = df['missing_rate'] * 100\n",
    "    mismatch = df['mismatch_rate'] * 100\n",
    "    \n",
    "    # Plot - Okabe-Ito colorblind-safe palette\n",
    "    ax.barh(fields, match, label='Match', color='#E69F00', alpha=0.85, edgecolor='black', linewidth=0.5)\n",
    "    ax.barh(fields, missing, left=match, label='Missing', color='#56B4E9', alpha=0.85, edgecolor='black', linewidth=0.5)\n",
    "    ax.barh(fields, mismatch, left=match + missing, label='Mismatch', color='#F0E442', alpha=0.85, edgecolor='black', linewidth=0.5)\n",
    "    \n",
    "    ax.set_xlabel('Percentage (%)', fontsize=13, fontweight='bold')\n",
    "    ax.set_ylabel('Clinical Attribute', fontsize=13, fontweight='bold')\n",
    "    ax.set_title('NER Extraction: Detailed Breakdown by Attribute', \n",
    "                 fontsize=15, fontweight='bold', pad=20)\n",
    "    ax.set_xlim([0, 100])\n",
    "    \n",
    "    # Add grid\n",
    "    ax.grid(axis='x', alpha=0.3, linestyle='--')\n",
    "    ax.set_axisbelow(True)\n",
    "    \n",
    "    # Legend\n",
    "    ax.legend(loc='lower right', framealpha=0.95, fontsize=11)\n",
    "    \n",
    "    # Add percentage labels for each segment\n",
    "    for i, (m, miss, misma) in enumerate(zip(match, missing, mismatch)):\n",
    "        # Match\n",
    "        if m > 8:\n",
    "            ax.text(m/2, i, f'{m:.1f}%', ha='center', va='center', \n",
    "                   fontsize=9, fontweight='bold', color='white')\n",
    "        \n",
    "        # Missing\n",
    "        if miss > 8:\n",
    "            ax.text(m + miss/2, i, f'{miss:.1f}%', ha='center', va='center',\n",
    "                   fontsize=9, fontweight='bold', color='white')\n",
    "        \n",
    "        # Mismatch\n",
    "        if misma > 8:\n",
    "            ax.text(m + miss + misma/2, i, f'{misma:.1f}%', ha='center', va='center',\n",
    "                   fontsize=9, fontweight='bold', color='white')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\" Saved: {output_path}\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_field_comparison_heatmap(df, output_path):\n",
    "\n",
    "    # Sort by match rate\n",
    "    df = df.sort_values('match_rate', ascending=False)\n",
    "    \n",
    "    # Prepare data\n",
    "    data = df[['match_rate', 'missing_rate', 'mismatch_rate']].values * 100\n",
    "    fields = df['field'].tolist()\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, max(8, len(df) * 0.4)))\n",
    "    \n",
    "    im = ax.imshow(data, cmap='inferno', aspect='auto', vmin=0, vmax=100)\n",
    "    \n",
    "    # Set ticks\n",
    "    ax.set_xticks([0, 1, 2])\n",
    "    ax.set_xticklabels(['Match', 'Missing', 'Mismatch'], fontsize=12, fontweight='bold')\n",
    "    ax.set_yticks(range(len(fields)))\n",
    "    ax.set_yticklabels(fields, fontsize=11)\n",
    "    \n",
    "    # Add text annotations\n",
    "    for i in range(len(fields)):\n",
    "        for j in range(3):\n",
    "            text = ax.text(j, i, f'{data[i, j]:.1f}%',\n",
    "                          ha='center', va='center', \n",
    "                          color='white' if data[i, j] < 50 else 'black',\n",
    "                          fontsize=10, fontweight='bold')\n",
    "    \n",
    "    # Colorbar\n",
    "    cbar = plt.colorbar(im, ax=ax)\n",
    "    cbar.set_label('Percentage (%)', rotation=270, labelpad=20, fontsize=12, fontweight='bold')\n",
    "    \n",
    "    ax.set_title('NER Extraction: Performance Heatmap', \n",
    "                 fontsize=15, fontweight='bold', pad=20)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"✓ Saved: {output_path}\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 2. PER-PATIENT VISUALIZATIONS\n",
    "# =============================================================================\n",
    "\n",
    "def plot_patient_accuracy_distribution(df, output_path):\n",
    "\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    n, bins, patches = ax1.hist(df['accuracy'] * 100, bins=20, \n",
    "                                 color='#56B4E9', alpha=0.7, edgecolor='black', linewidth=1.5)\n",
    "    \n",
    "    for i, patch in enumerate(patches):\n",
    "        bin_center = (bins[i] + bins[i+1]) / 2\n",
    "        if bin_center >= 90:\n",
    "            patch.set_facecolor('#E69F00')   # Orange - Excellent\n",
    "        elif bin_center >= 80:\n",
    "            patch.set_facecolor('#56B4E9')   # Sky blue - Good\n",
    "        elif bin_center >= 70:\n",
    "            patch.set_facecolor('#009E73')   # Bluish green - Fair\n",
    "        else:\n",
    "            patch.set_facecolor('#F0E442')   # Yellow - Poor\n",
    "    \n",
    "    ax1.set_xlabel('Accuracy (%)', fontsize=12, fontweight='bold')\n",
    "    ax1.set_ylabel('Number of Patients', fontsize=12, fontweight='bold')\n",
    "    ax1.set_title('Distribution of Per-Patient Accuracy', fontsize=14, fontweight='bold')\n",
    "    ax1.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Add statistics\n",
    "    mean_acc = df['accuracy'].mean() * 100\n",
    "    median_acc = df['accuracy'].median() * 100\n",
    "    std_acc = df['accuracy'].std() * 100\n",
    "    \n",
    "    stats_text = f'Mean: {mean_acc:.1f}%\\nMedian: {median_acc:.1f}%\\nStd: {std_acc:.1f}%'\n",
    "    ax1.text(0.02, 0.98, stats_text, transform=ax1.transAxes,\n",
    "            fontsize=11, verticalalignment='top',\n",
    "            bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "    \n",
    "    # Add reference lines \n",
    "    ax1.axvline(x=mean_acc, color='#E69F00', linestyle='--', linewidth=2, label=f'Mean: {mean_acc:.1f}%')\n",
    "    ax1.axvline(x=median_acc, color='#56B4E9', linestyle='--', linewidth=2, label=f'Median: {median_acc:.1f}%')\n",
    "    ax1.legend(loc='upper left')\n",
    "    \n",
    "    # Box plot -\n",
    "    bp = ax2.boxplot([df['accuracy'] * 100], vert=True, widths=0.5, patch_artist=True,\n",
    "                      showmeans=True, meanline=True,\n",
    "                      boxprops=dict(facecolor='#56B4E9', alpha=0.7, edgecolor='black', linewidth=2),\n",
    "                      whiskerprops=dict(color='black', linewidth=2),\n",
    "                      capprops=dict(color='black', linewidth=2),\n",
    "                      medianprops=dict(color='#E69F00', linewidth=3),\n",
    "                      meanprops=dict(color='#009E73', linewidth=3, linestyle='--'))\n",
    "    \n",
    "    ax2.set_ylabel('Accuracy (%)', fontsize=14, fontweight='bold')\n",
    "    ax2.set_title('Per-Patient Accuracy Summary', fontsize=14, fontweight='bold')\n",
    "    ax2.set_xticks([1])\n",
    "    ax2.set_xticklabels(['All Patients'], fontsize=14)\n",
    "    ax2.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Add percentile annotations\n",
    "    q25 = df['accuracy'].quantile(0.25) * 100\n",
    "    q75 = df['accuracy'].quantile(0.75) * 100\n",
    "    \n",
    "    ax2.text(1.15, median_acc, f'Median: {median_acc:.1f}%', \n",
    "            fontsize=16, va='center', fontweight='bold')\n",
    "    ax2.text(1.15, q75, f'Q3: {q75:.1f}%', fontsize=16, va='center', style='italic')\n",
    "    ax2.text(1.15, q25, f'Q1: {q25:.1f}%', fontsize=16, va='center', style='italic')\n",
    "    \n",
    "    plt.suptitle('NER Extraction: Per-Patient Accuracy Analysis', \n",
    "                 fontsize=16, fontweight='bold', y=1.02)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\" Saved: {output_path}\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_patient_accuracy_cumulative(df, output_path):\n",
    "\n",
    "    \n",
    "    # Sort accuracies\n",
    "    sorted_acc = np.sort(df['accuracy'].values) * 100\n",
    "    cumulative = np.arange(1, len(sorted_acc) + 1) / len(sorted_acc) * 100\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12, 7))\n",
    "    \n",
    "    ax.plot(sorted_acc, cumulative, linewidth=3, color='#56B4E9', label='Cumulative Distribution')\n",
    "    ax.fill_between(sorted_acc, 0, cumulative, alpha=0.3, color='#56B4E9')\n",
    "    \n",
    "    ax.set_xlabel('Accuracy (%)', fontsize=13, fontweight='bold')\n",
    "    ax.set_ylabel('Cumulative Percentage of Patients (%)', fontsize=13, fontweight='bold')\n",
    "    ax.set_title('Cumulative Distribution of Per-Patient Accuracy', \n",
    "                 fontsize=15, fontweight='bold', pad=20)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_xlim([0, 100])\n",
    "    ax.set_ylim([0, 100])\n",
    "    \n",
    "    # Add reference lines for key percentiles - Okabe-Ito palette\n",
    "    percentiles = [50, 80, 90, 95]\n",
    "    for p in percentiles:\n",
    "        acc_at_p = np.percentile(sorted_acc, p)\n",
    "        ax.axhline(y=p, color='#009E73', linestyle='--', alpha=0.5, linewidth=1)\n",
    "        ax.axvline(x=acc_at_p, color='#009E73', linestyle='--', alpha=0.5, linewidth=1)\n",
    "        ax.plot(acc_at_p, p, 'o', color='maroon', markersize=8)\n",
    "        ax.text(acc_at_p + 2, p - 3, f'{p}th: {acc_at_p:.1f}%', \n",
    "               fontsize=10, fontweight='bold',\n",
    "               bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "    \n",
    "    # Add legend\n",
    "    ax.legend(loc='lower right', fontsize=11, framealpha=0.95)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"✓ Saved: {output_path}\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_patient_performance_categories(df, output_path):\n",
    "\n",
    "    # Categorize patients\n",
    "    df['category'] = pd.cut(df['accuracy'] * 100, \n",
    "                            bins=[0, 70, 80, 90, 100],\n",
    "                            labels=['Poor (<70%)', 'Fair (70-80%)', 'Good (80-90%)', 'Excellent (≥90%)'])\n",
    "    \n",
    "    category_counts = df['category'].value_counts().sort_index()\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 7))\n",
    "    \n",
    "    colors = ['#F0E442', '#009E73', '#56B4E9', '#E69F00']\n",
    "    bars = ax.bar(range(len(category_counts)), category_counts.values, \n",
    "                  color=colors, alpha=0.85, edgecolor='black', linewidth=2)\n",
    "    \n",
    "    ax.set_xticks(range(len(category_counts)))\n",
    "    ax.set_xticklabels(category_counts.index, fontsize=16, fontweight='bold')\n",
    "    ax.set_ylabel('Number of Patients', fontsize=16, fontweight='bold')\n",
    "    ax.set_title('Patient Distribution by Performance Category', \n",
    "                 fontsize=15, fontweight='bold', pad=20)\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Add value labels and percentages\n",
    "    total = len(df)\n",
    "    for i, (bar, count) in enumerate(zip(bars, category_counts.values)):\n",
    "        height = bar.get_height()\n",
    "        percentage = (count / total) * 100\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "               f'{int(count)}\\n({percentage:.1f}%)',\n",
    "               ha='center', va='bottom', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Add total annotation\n",
    "    ax.text(0.02, 0.98, f'Total Patients: {total}', \n",
    "           transform=ax.transAxes, fontsize=16, fontweight='bold',\n",
    "           verticalalignment='top',\n",
    "           bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\" Saved: {output_path}\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 3. MAIN VISUALIZATION FUNCTION\n",
    "# =============================================================================\n",
    "\n",
    "def visualize_ner_comparison(\n",
    "    field_summary_path='evaluation_results/ner_comparison/field_comparison_summary.csv',\n",
    "    patient_results_path='evaluation_results/ner_comparison/patient_comparison.csv',\n",
    "    output_dir='evaluation_results/ner_comparison/plots'\n",
    "):\n",
    " \n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"VISUALIZING NER COMPARISON RESULTS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Create output directory\n",
    "    output_dir = Path(output_dir)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Load data\n",
    "    print(\"\\nLoading data...\")\n",
    "    field_df = pd.read_csv(field_summary_path)\n",
    "    patient_df = pd.read_csv(patient_results_path)\n",
    "    \n",
    "    print(f\" Loaded {len(field_df)} fields\")\n",
    "    print(f\" Loaded {len(patient_df)} patients\")\n",
    "    \n",
    "    # Generate visualizations\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"GENERATING FIELD VISUALIZATIONS\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    plot_field_match_rates(field_df, output_dir / '1_field_match_rates.png')\n",
    "    plot_field_breakdown_stacked(field_df, output_dir / '2_field_breakdown_stacked.png')\n",
    "    plot_field_comparison_heatmap(field_df, output_dir / '3_field_heatmap.png')\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"GENERATING PATIENT VISUALIZATIONS\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    plot_patient_accuracy_distribution(patient_df, output_dir / '4_patient_accuracy_distribution.png')\n",
    "    plot_patient_accuracy_cumulative(patient_df, output_dir / '5_patient_accuracy_cumulative.png')\n",
    "    plot_patient_performance_categories(patient_df, output_dir / '6_patient_performance_categories.png')\n",
    "    \n",
    "    # Summary\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\" VISUALIZATION COMPLETE!\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    print(f\"\\n All plots saved to: {output_dir}\")\n",
    "    \n",
    "    print(f\"\\nField Performance:\")\n",
    "    print(f\"  Best:  {field_df.loc[field_df['match_rate'].idxmax(), 'field']} \"\n",
    "          f\"({field_df['match_rate'].max()*100:.1f}%)\")\n",
    "    print(f\"  Worst: {field_df.loc[field_df['match_rate'].idxmin(), 'field']} \"\n",
    "          f\"({field_df['match_rate'].min()*100:.1f}%)\")\n",
    "    print(f\"  Mean:  {field_df['match_rate'].mean()*100:.1f}%\")\n",
    "    \n",
    "    print(f\"\\nPatient Performance:\")\n",
    "    print(f\"  Mean:   {patient_df['accuracy'].mean()*100:.1f}%\")\n",
    "    print(f\"  Median: {patient_df['accuracy'].median()*100:.1f}%\")\n",
    "    print(f\"  Std:    {patient_df['accuracy'].std()*100:.1f}%\")\n",
    "    print(f\"  Min:    {patient_df['accuracy'].min()*100:.1f}%\")\n",
    "    print(f\"  Max:    {patient_df['accuracy'].max()*100:.1f}%\")\n",
    "    \n",
    "    return output_dir\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    output_dir = visualize_ner_comparison()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb78a39-9db4-4236-842a-1ff61c8ba22e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
