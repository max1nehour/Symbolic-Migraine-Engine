{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bca4bf6-08e2-46b8-bf89-3e29f2b428ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    classification_report, confusion_matrix\n",
    ")\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 8)\n",
    "\n",
    "# =============================================================================\n",
    "# 1. LOAD RESULTS\n",
    "# =============================================================================\n",
    "\n",
    "def load_catboost_results(results_dir):\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"LOADING CATBOOST RESULTS\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    results_dir = Path(results_dir)\n",
    "    \n",
    "    # Load metrics\n",
    "    metrics_path = results_dir / 'catboost_metrics.json'\n",
    "    with open(metrics_path, 'r') as f:\n",
    "        metrics = json.load(f)\n",
    "    \n",
    "    print(f\" Loaded metrics from {metrics_path}\")\n",
    "    \n",
    "    # Load predictions\n",
    "    predictions_path = results_dir / 'catboost_predictions.csv'\n",
    "    predictions = pd.read_csv(predictions_path)\n",
    "    \n",
    "    print(f\" Loaded predictions from {predictions_path}\")\n",
    "    print(f\"  Test samples: {len(predictions)}\")\n",
    "    \n",
    "    return {\n",
    "        'name': 'CatBoost ML',\n",
    "        'metrics': metrics['test'],\n",
    "        'predictions': predictions\n",
    "    }\n",
    "\n",
    "\n",
    "def load_symbolic_results(results_dir):\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"LOADING SYMBOLIC REASONING RESULTS\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    results_dir = Path(results_dir)\n",
    "    \n",
    "    # Try to load from evaluation results\n",
    "    possible_paths = [\n",
    "        results_dir / 'evaluation_results/symbolic/evaluation_results.csv',\n",
    "        results_dir / 'ichd3_diagnoses_final.csv',\n",
    "        'data/diagnoses/ichd3_diagnoses_final.csv'\n",
    "    ]\n",
    "    \n",
    "    for path in possible_paths:\n",
    "        if path.exists():\n",
    "            print(f\" Found symbolic results: {path}\")\n",
    "            df = pd.read_csv(path)\n",
    "            \n",
    "            # Extract predictions\n",
    "            if 'predicted_diagnosis' in df.columns and 'true_diagnosis' in df.columns:\n",
    "                predictions = df[['true_diagnosis', 'predicted_diagnosis']].copy()\n",
    "                predictions.columns = ['true_label', 'predicted_label']\n",
    "            elif 'Type' in df.columns and 'diagnosis' in df.columns:\n",
    "                predictions = df[['Type', 'diagnosis']].copy()\n",
    "                predictions.columns = ['true_label', 'predicted_label']\n",
    "            else:\n",
    "                print(f\" Column names: {df.columns.tolist()}\")\n",
    "                raise ValueError(\"Could not find diagnosis columns\")\n",
    "            \n",
    "            print(f\"  Predictions: {len(predictions)}\")\n",
    "            \n",
    "            # Calculate metrics\n",
    "            metrics = {\n",
    "                'accuracy': accuracy_score(predictions['true_label'], predictions['predicted_label']),\n",
    "                'precision_macro': precision_score(predictions['true_label'], predictions['predicted_label'], average='macro'),\n",
    "                'precision_weighted': precision_score(predictions['true_label'], predictions['predicted_label'], average='weighted'),\n",
    "                'recall_macro': recall_score(predictions['true_label'], predictions['predicted_label'], average='macro'),\n",
    "                'recall_weighted': recall_score(predictions['true_label'], predictions['predicted_label'], average='weighted'),\n",
    "                'f1_macro': f1_score(predictions['true_label'], predictions['predicted_label'], average='macro'),\n",
    "                'f1_weighted': f1_score(predictions['true_label'], predictions['predicted_label'], average='weighted')\n",
    "            }\n",
    "            \n",
    "            return {\n",
    "                'name': 'Symbolic Reasoning',\n",
    "                'metrics': metrics,\n",
    "                'predictions': predictions\n",
    "            }\n",
    "    \n",
    "    print(f\" No symbolic results found in any of these paths:\")\n",
    "    for path in possible_paths:\n",
    "        print(f\"   - {path}\")\n",
    "    raise FileNotFoundError(\"Symbolic results not found. Run evaluate_symbolic_reasoning.py first!\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 2. COMPARISON METRICS\n",
    "# =============================================================================\n",
    "\n",
    "def compare_metrics(catboost_results, symbolic_results):\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"METRICS COMPARISON\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    metrics_names = ['accuracy', 'precision_weighted', 'recall_weighted', 'f1_weighted']\n",
    "    \n",
    "    comparison = []\n",
    "    \n",
    "    for metric in metrics_names:\n",
    "        catboost_val = catboost_results['metrics'][metric]\n",
    "        symbolic_val = symbolic_results['metrics'][metric]\n",
    "        diff = catboost_val - symbolic_val\n",
    "        \n",
    "        comparison.append({\n",
    "            'metric': metric,\n",
    "            'catboost': catboost_val,\n",
    "            'symbolic': symbolic_val,\n",
    "            'difference': diff,\n",
    "            'winner': 'CatBoost' if diff > 0 else 'Symbolic' if diff < 0 else 'Tie'\n",
    "        })\n",
    "        \n",
    "        print(f\"\\n{metric.upper().replace('_', ' ')}:\")\n",
    "        print(f\"  CatBoost: {catboost_val:.4f}\")\n",
    "        print(f\"  Symbolic: {symbolic_val:.4f}\")\n",
    "        print(f\"  Difference: {diff:+.4f}\")\n",
    "        print(f\"  Winner: {comparison[-1]['winner']}\")\n",
    "    \n",
    "    # Overall winner\n",
    "    catboost_wins = sum(1 for c in comparison if c['winner'] == 'CatBoost')\n",
    "    symbolic_wins = sum(1 for c in comparison if c['winner'] == 'Symbolic')\n",
    "    \n",
    "    print(f\"\\n{'─'*80}\")\n",
    "    print(\"OVERALL WINNER:\")\n",
    "    print(f\"{'─'*80}\")\n",
    "    print(f\"CatBoost wins: {catboost_wins}/{len(metrics_names)} metrics\")\n",
    "    print(f\"Symbolic wins: {symbolic_wins}/{len(metrics_names)} metrics\")\n",
    "    \n",
    "    if catboost_wins > symbolic_wins:\n",
    "        print(f\"\\n WINNER: CatBoost ML Model\")\n",
    "    elif symbolic_wins > catboost_wins:\n",
    "        print(f\"\\n WINNER: Symbolic Reasoning\")\n",
    "    else:\n",
    "        print(f\"\\n Both approaches perform equally\")\n",
    "    \n",
    "    return pd.DataFrame(comparison)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 3. VISUALIZATIONS\n",
    "# =============================================================================\n",
    "\n",
    "def plot_metrics_comparison(comparison_df, output_path):\n",
    "    \n",
    "    metrics = comparison_df['metric'].tolist()\n",
    "    catboost = comparison_df['catboost'].tolist()\n",
    "    symbolic = comparison_df['symbolic'].tolist()\n",
    "    \n",
    "    x = np.arange(len(metrics))\n",
    "    width = 0.35\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    \n",
    "    # Okabe-Ito colorblind-safe palette\n",
    "    bars1 = ax.bar(x - width/2, catboost, width, label='CatBoost ML', \n",
    "                   color='#E69F00', alpha=0.85, edgecolor='black', linewidth=1)\n",
    "    bars2 = ax.bar(x + width/2, symbolic, width, label='Symbolic Reasoning', \n",
    "                   color='#56B4E9', alpha=0.85, edgecolor='black', linewidth=1)\n",
    "    \n",
    "    ax.set_xlabel('Metric', fontsize=12, fontweight='bold')\n",
    "    ax.set_ylabel('Score', fontsize=12, fontweight='bold')\n",
    "    ax.set_title('ML vs Symbolic Reasoning: Performance Comparison', fontsize=14, fontweight='bold')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels([m.replace('_', ' ').title() for m in metrics])\n",
    "    ax.legend(fontsize=11, framealpha=0.9)\n",
    "    ax.set_ylim([0, 1.1])\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bars in [bars1, bars2]:\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                   f'{height:.3f}',\n",
    "                   ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\" Saved: {output_path}\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_difference_chart(comparison_df, output_path):\n",
    "    \n",
    "    metrics = comparison_df['metric'].tolist()\n",
    "    differences = comparison_df['difference'].tolist()\n",
    "    \n",
    "    # Okabe-Ito colors based on winner\n",
    "    colors = ['#E69F00' if d > 0 else '#56B4E9' if d < 0 else '#009E73' \n",
    "              for d in differences]\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    bars = ax.barh(metrics, differences, color=colors, alpha=0.85, edgecolor='black', linewidth=1)\n",
    "    \n",
    "    ax.set_xlabel('Difference (CatBoost - Symbolic)', fontsize=12, fontweight='bold')\n",
    "    ax.set_ylabel('Metric', fontsize=12, fontweight='bold')\n",
    "    ax.set_title('Performance Difference: CatBoost vs Symbolic', fontsize=14, fontweight='bold')\n",
    "    ax.axvline(x=0, color='black', linestyle='-', linewidth=1.5)\n",
    "    ax.grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, (bar, val) in enumerate(zip(bars, differences)):\n",
    "        label = f'{val:+.3f}'\n",
    "        ax.text(val, i, label, va='center', \n",
    "               ha='left' if val > 0 else 'right',\n",
    "               fontsize=10, fontweight='bold')\n",
    "    \n",
    "    # Add legend with Okabe-Ito colors\n",
    "    from matplotlib.patches import Patch\n",
    "    legend_elements = [\n",
    "        Patch(facecolor='#E69F00', alpha=0.85, label='CatBoost Better'),\n",
    "        Patch(facecolor='#56B4E9', alpha=0.85, label='Symbolic Better'),\n",
    "        Patch(facecolor='#009E73', alpha=0.85, label='Tie')\n",
    "    ]\n",
    "    ax.legend(handles=legend_elements, loc='best', framealpha=0.9)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\" Saved: {output_path}\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_confusion_matrices_comparison(catboost_preds, symbolic_preds, output_path):\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # CatBoost confusion matrix\n",
    "    cm_catboost = confusion_matrix(\n",
    "        catboost_preds['true_label'], \n",
    "        catboost_preds['predicted_label']\n",
    "    )\n",
    "    cm_catboost_norm = cm_catboost.astype('float') / cm_catboost.sum(axis=1)[:, np.newaxis]\n",
    "    \n",
    "    labels = sorted(catboost_preds['true_label'].unique())\n",
    "    \n",
    "    # Use inferno colormap for both\n",
    "    sns.heatmap(cm_catboost_norm, annot=True, fmt='.2f', cmap='inferno',\n",
    "                xticklabels=labels, yticklabels=labels,\n",
    "                ax=axes[0], cbar_kws={'label': 'Proportion'})\n",
    "    axes[0].set_title('CatBoost ML Model', fontsize=13, fontweight='bold', pad=10)\n",
    "    axes[0].set_xlabel('Predicted', fontweight='bold')\n",
    "    axes[0].set_ylabel('True', fontweight='bold')\n",
    "    \n",
    "    # Symbolic confusion matrix\n",
    "    cm_symbolic = confusion_matrix(\n",
    "        symbolic_preds['true_label'], \n",
    "        symbolic_preds['predicted_label']\n",
    "    )\n",
    "    cm_symbolic_norm = cm_symbolic.astype('float') / cm_symbolic.sum(axis=1)[:, np.newaxis]\n",
    "    \n",
    "    sns.heatmap(cm_symbolic_norm, annot=True, fmt='.2f', cmap='inferno',\n",
    "                xticklabels=labels, yticklabels=labels,\n",
    "                ax=axes[1], cbar_kws={'label': 'Proportion'})\n",
    "    axes[1].set_title('Symbolic Reasoning (ICHD-3)', fontsize=13, fontweight='bold', pad=10)\n",
    "    axes[1].set_xlabel('Predicted', fontweight='bold')\n",
    "    axes[1].set_ylabel('True', fontweight='bold')\n",
    "    \n",
    "    plt.suptitle('Confusion Matrix Comparison', fontsize=15, fontweight='bold', y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\" Saved: {output_path}\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 4. DETAILED ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "def analyze_per_class_performance(catboost_preds, symbolic_preds, output_path):\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"PER-CLASS ANALYSIS\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Get unique classes\n",
    "    classes = sorted(set(catboost_preds['true_label'].unique()) | \n",
    "                    set(symbolic_preds['true_label'].unique()))\n",
    "    \n",
    "    per_class_results = []\n",
    "    \n",
    "    for cls in classes:\n",
    "        # CatBoost\n",
    "        catboost_mask = catboost_preds['true_label'] == cls\n",
    "        catboost_correct = (catboost_preds[catboost_mask]['true_label'] == \n",
    "                           catboost_preds[catboost_mask]['predicted_label']).sum()\n",
    "        catboost_total = catboost_mask.sum()\n",
    "        catboost_acc = catboost_correct / catboost_total if catboost_total > 0 else 0\n",
    "        \n",
    "        # Symbolic\n",
    "        symbolic_mask = symbolic_preds['true_label'] == cls\n",
    "        symbolic_correct = (symbolic_preds[symbolic_mask]['true_label'] == \n",
    "                          symbolic_preds[symbolic_mask]['predicted_label']).sum()\n",
    "        symbolic_total = symbolic_mask.sum()\n",
    "        symbolic_acc = symbolic_correct / symbolic_total if symbolic_total > 0 else 0\n",
    "        \n",
    "        per_class_results.append({\n",
    "            'class': cls,\n",
    "            'samples': max(catboost_total, symbolic_total),\n",
    "            'catboost_accuracy': catboost_acc,\n",
    "            'symbolic_accuracy': symbolic_acc,\n",
    "            'difference': catboost_acc - symbolic_acc\n",
    "        })\n",
    "        \n",
    "        print(f\"\\n{cls}:\")\n",
    "        print(f\"  Samples: {max(catboost_total, symbolic_total)}\")\n",
    "        print(f\"  CatBoost: {catboost_acc:.3f}\")\n",
    "        print(f\"  Symbolic: {symbolic_acc:.3f}\")\n",
    "        print(f\"  Difference: {catboost_acc - symbolic_acc:+.3f}\")\n",
    "    \n",
    "    # Save to CSV\n",
    "    df = pd.DataFrame(per_class_results)\n",
    "    df.to_csv(output_path, index=False)\n",
    "    print(f\"\\n Saved per-class analysis: {output_path}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 5. GENERATE COMPARISON REPORT\n",
    "# =============================================================================\n",
    "\n",
    "def generate_comparison_report(catboost_results, symbolic_results, output_dir):\n",
    "    \n",
    "    output_dir = Path(output_dir)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"GENERATING COMPARISON REPORT\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # 1. Compare metrics\n",
    "    comparison_df = compare_metrics(catboost_results, symbolic_results)\n",
    "    \n",
    "    # Save comparison table\n",
    "    comparison_path = output_dir / 'metrics_comparison.csv'\n",
    "    comparison_df.to_csv(comparison_path, index=False)\n",
    "    print(f\"\\n✓ Saved comparison table: {comparison_path}\")\n",
    "    \n",
    "    # 2. Generate visualizations\n",
    "    print(\"\\nGenerating visualizations...\")\n",
    "    \n",
    "    plot_metrics_comparison(\n",
    "        comparison_df,\n",
    "        output_dir / 'metrics_comparison.png'\n",
    "    )\n",
    "    \n",
    "    plot_difference_chart(\n",
    "        comparison_df,\n",
    "        output_dir / 'performance_difference.png'\n",
    "    )\n",
    "    \n",
    "    plot_confusion_matrices_comparison(\n",
    "        catboost_results['predictions'],\n",
    "        symbolic_results['predictions'],\n",
    "        output_dir / 'confusion_matrices_comparison.png'\n",
    "    )\n",
    "    \n",
    "    # 3. Per-class analysis\n",
    "    per_class_df = analyze_per_class_performance(\n",
    "        catboost_results['predictions'],\n",
    "        symbolic_results['predictions'],\n",
    "        output_dir / 'per_class_comparison.csv'\n",
    "    )\n",
    "    \n",
    "    # 4. Generate summary report\n",
    "    summary = {\n",
    "        'catboost': {\n",
    "            'name': 'CatBoost ML Model',\n",
    "            'accuracy': catboost_results['metrics']['accuracy'],\n",
    "            'f1_weighted': catboost_results['metrics']['f1_weighted'],\n",
    "            'precision_weighted': catboost_results['metrics']['precision_weighted'],\n",
    "            'recall_weighted': catboost_results['metrics']['recall_weighted']\n",
    "        },\n",
    "        'symbolic': {\n",
    "            'name': 'Symbolic Reasoning (ICHD-3)',\n",
    "            'accuracy': symbolic_results['metrics']['accuracy'],\n",
    "            'f1_weighted': symbolic_results['metrics']['f1_weighted'],\n",
    "            'precision_weighted': symbolic_results['metrics']['precision_weighted'],\n",
    "            'recall_weighted': symbolic_results['metrics']['recall_weighted']\n",
    "        },\n",
    "        'comparison': {\n",
    "            'accuracy_difference': catboost_results['metrics']['accuracy'] - symbolic_results['metrics']['accuracy'],\n",
    "            'f1_difference': catboost_results['metrics']['f1_weighted'] - symbolic_results['metrics']['f1_weighted'],\n",
    "            'winner': 'CatBoost' if catboost_results['metrics']['accuracy'] > symbolic_results['metrics']['accuracy'] else 'Symbolic'\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    summary_path = output_dir / 'comparison_summary.json'\n",
    "    with open(summary_path, 'w') as f:\n",
    "        json.dump(summary, f, indent=2)\n",
    "    print(f\" Saved summary: {summary_path}\")\n",
    "    \n",
    "    print(f\"\\n All comparison results saved to: {output_dir}\")\n",
    "    \n",
    "    return summary\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 6. MAIN\n",
    "# =============================================================================\n",
    "\n",
    "def run_comparison(catboost_dir='/Users/M1HR/Desktop/MIGRAINE/evaluation_results/catboost',\n",
    "                  symbolic_dir='/Users/M1HR/Desktop/MIGRAINE/',\n",
    "                  output_dir='/Users/M1HR/Desktop/MIGRAINE/evaluation_results/comparison'):\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ML vs SYMBOLIC REASONING COMPARISON\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Load results\n",
    "    catboost_results = load_catboost_results(catboost_dir)\n",
    "    symbolic_results = load_symbolic_results(symbolic_dir)\n",
    "    \n",
    "    # Generate comparison\n",
    "    summary = generate_comparison_report(\n",
    "        catboost_results,\n",
    "        symbolic_results,\n",
    "        output_dir\n",
    "    )\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\" COMPARISON COMPLETE!\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(f\"\\n FINAL RESULTS:\")\n",
    "    print(f\"\\nCatBoost ML:\")\n",
    "    print(f\"  Accuracy: {summary['catboost']['accuracy']:.4f}\")\n",
    "    print(f\"  F1-Score: {summary['catboost']['f1_weighted']:.4f}\")\n",
    "    \n",
    "    print(f\"\\nSymbolic Reasoning:\")\n",
    "    print(f\"  Accuracy: {summary['symbolic']['accuracy']:.4f}\")\n",
    "    print(f\"  F1-Score: {summary['symbolic']['f1_weighted']:.4f}\")\n",
    "    \n",
    "    print(f\"\\nDifference:\")\n",
    "    print(f\"  Accuracy: {summary['comparison']['accuracy_difference']:+.4f}\")\n",
    "    print(f\"  F1-Score: {summary['comparison']['f1_difference']:+.4f}\")\n",
    "    \n",
    "    print(f\"\\n Winner: {summary['comparison']['winner']}\")\n",
    "    \n",
    "    print(f\"\\n Results: {output_dir}\")\n",
    "    \n",
    "    return summary\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    summary = run_comparison()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8813612a-6286-413f-88af-3104cb1e2bf1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
