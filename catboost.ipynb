{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f11cf3b-e12d-4743-81ef-d6aa9d14974b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65a32b3d-9414-4d64-811a-818e0a0072fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/M1HR/Desktop/MIGRAINE/data\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())\n",
    "os.chdir(\"/Users/M1HR/Desktop/MIGRAINE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d944b6e-3548-4719-a816-2e454d9e07df",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    classification_report, confusion_matrix\n",
    ")\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "# =============================================================================\n",
    "# 1. LOAD AND PREPARE DATA\n",
    "# =============================================================================\n",
    "\n",
    "def load_data(csv_path):\n",
    "   \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"LOADING DATA\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    df = pd.read_csv(csv_path)\n",
    "    \n",
    "    print(f\"✓ Loaded {len(df)} patients\")\n",
    "    print(f\"✓ Features: {df.shape[1]} columns\")\n",
    "    \n",
    "    # Show data info\n",
    "    print(f\"\\nColumns: {list(df.columns)}\")\n",
    "    print(f\"\\nFirst few rows:\")\n",
    "    print(df.head())\n",
    "    \n",
    "    # Check for missing values\n",
    "    missing = df.isnull().sum()\n",
    "    if missing.any():\n",
    "        print(f\"\\n Missing values:\")\n",
    "        print(missing[missing > 0])\n",
    "    else:\n",
    "        print(f\"\\n✓ No missing values\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def prepare_data(df):\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"PREPARING DATA\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Separate features (X) and target (y)\n",
    "    X = df.drop(columns=['Type', 'Patient_ID', 'Age'])\n",
    "    y = df['Type']\n",
    "    \n",
    "    print(f\" Features (X): {X.shape}\")\n",
    "    print(f\" Target (y): {y.shape}\")\n",
    "    \n",
    "    # Show feature names\n",
    "    print(f\"\\nFeature columns ({len(X.columns)}):\")\n",
    "    for i, col in enumerate(X.columns, 1):\n",
    "        print(f\"  {i:2d}. {col}\")\n",
    "    \n",
    "    # Show target distribution\n",
    "    print(f\"\\nTarget distribution:\")\n",
    "    print(y.value_counts().sort_index())\n",
    "    \n",
    "    # Check for class imbalance\n",
    "    class_counts = y.value_counts()\n",
    "    imbalance_ratio = class_counts.max() / class_counts.min()\n",
    "    \n",
    "    if imbalance_ratio > 3:\n",
    "        print(f\"\\n Class imbalance detected (ratio: {imbalance_ratio:.1f})\")\n",
    "        print(f\"   Most common: {class_counts.idxmax()} ({class_counts.max()} samples)\")\n",
    "        print(f\"   Least common: {class_counts.idxmin()} ({class_counts.min()} samples)\")\n",
    "    else:\n",
    "        print(f\"\\n Classes reasonably balanced (ratio: {imbalance_ratio:.1f})\")\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 2. TRAIN-TEST SPLIT\n",
    "# =============================================================================\n",
    "\n",
    "def split_data(X, y, test_size=0.2, random_state=42):\n",
    "    ]    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"TRAIN-TEST SPLIT\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, \n",
    "        test_size=test_size,\n",
    "        random_state=random_state,\n",
    "        stratify=y  # Maintain class distribution\n",
    "    )\n",
    "    \n",
    "    print(f\"Train size: {len(X_train)} ({(1-test_size)*100:.0f}%)\")\n",
    "    print(f\"Test size:  {len(X_test)} ({test_size*100:.0f}%)\")\n",
    "    \n",
    "    # Check distributions\n",
    "    print(f\"\\nTrain distribution:\")\n",
    "    print(y_train.value_counts().sort_index())\n",
    "    \n",
    "    print(f\"\\nTest distribution:\")\n",
    "    print(y_test.value_counts().sort_index())\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 3. TRAIN CATBOOST MODEL\n",
    "# =============================================================================\n",
    "\n",
    "def train_catboost(X_train, y_train, X_test, y_test):\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"TRAINING CATBOOST MODEL\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Initialize model\n",
    "    model = CatBoostClassifier(\n",
    "        iterations=1000,\n",
    "        learning_rate=0.1,\n",
    "        depth=6,\n",
    "        loss_function='MultiClass',\n",
    "        eval_metric='Accuracy',\n",
    "        random_seed=42,\n",
    "        verbose=100,  # Print every 100 iterations\n",
    "        early_stopping_rounds=50,\n",
    "        task_type='CPU'\n",
    "    )\n",
    "    \n",
    "    # Create pools for efficient training\n",
    "    train_pool = Pool(X_train, y_train)\n",
    "    test_pool = Pool(X_test, y_test)\n",
    "    \n",
    "    print(\"\\nTraining...\")\n",
    "    \n",
    "    # Train\n",
    "    model.fit(\n",
    "        train_pool,\n",
    "        eval_set=test_pool,\n",
    "        plot=False\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n✓ Training complete!\")\n",
    "    print(f\"  Best iteration: {model.get_best_iteration()}\")\n",
    "    print(f\"  Best score: {model.get_best_score()['validation']['Accuracy']:.4f}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 4. EVALUATE MODEL\n",
    "# =============================================================================\n",
    "\n",
    "def evaluate_model(model, X_train, y_train, X_test, y_test):\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"MODEL EVALUATION\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Predictions\n",
    "    y_train_pred = model.predict(X_train).flatten()\n",
    "    y_test_pred = model.predict(X_test).flatten()\n",
    "    \n",
    "    # Metrics\n",
    "    results = {\n",
    "        'train': {\n",
    "            'accuracy': accuracy_score(y_train, y_train_pred),\n",
    "            'precision_macro': precision_score(y_train, y_train_pred, average='macro'),\n",
    "            'precision_weighted': precision_score(y_train, y_train_pred, average='weighted'),\n",
    "            'recall_macro': recall_score(y_train, y_train_pred, average='macro'),\n",
    "            'recall_weighted': recall_score(y_train, y_train_pred, average='weighted'),\n",
    "            'f1_macro': f1_score(y_train, y_train_pred, average='macro'),\n",
    "            'f1_weighted': f1_score(y_train, y_train_pred, average='weighted')\n",
    "        },\n",
    "        'test': {\n",
    "            'accuracy': accuracy_score(y_test, y_test_pred),\n",
    "            'precision_macro': precision_score(y_test, y_test_pred, average='macro'),\n",
    "            'precision_weighted': precision_score(y_test, y_test_pred, average='weighted'),\n",
    "            'recall_macro': recall_score(y_test, y_test_pred, average='macro'),\n",
    "            'recall_weighted': recall_score(y_test, y_test_pred, average='weighted'),\n",
    "            'f1_macro': f1_score(y_test, y_test_pred, average='macro'),\n",
    "            'f1_weighted': f1_score(y_test, y_test_pred, average='weighted')\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Print results\n",
    "    print(\"\\nTRAIN METRICS:\")\n",
    "    print(f\"  Accuracy:           {results['train']['accuracy']:.4f}\")\n",
    "    print(f\"  Precision (macro):  {results['train']['precision_macro']:.4f}\")\n",
    "    print(f\"  Precision (weighted): {results['train']['precision_weighted']:.4f}\")\n",
    "    print(f\"  Recall (macro):     {results['train']['recall_macro']:.4f}\")\n",
    "    print(f\"  Recall (weighted):  {results['train']['recall_weighted']:.4f}\")\n",
    "    print(f\"  F1 (macro):         {results['train']['f1_macro']:.4f}\")\n",
    "    print(f\"  F1 (weighted):      {results['train']['f1_weighted']:.4f}\")\n",
    "    \n",
    "    print(\"\\nTEST METRICS:\")\n",
    "    print(f\"  Accuracy:           {results['test']['accuracy']:.4f}\")\n",
    "    print(f\"  Precision (macro):  {results['test']['precision_macro']:.4f}\")\n",
    "    print(f\"  Precision (weighted): {results['test']['precision_weighted']:.4f}\")\n",
    "    print(f\"  Recall (macro):     {results['test']['recall_macro']:.4f}\")\n",
    "    print(f\"  Recall (weighted):  {results['test']['recall_weighted']:.4f}\")\n",
    "    print(f\"  F1 (macro):         {results['test']['f1_macro']:.4f}\")\n",
    "    print(f\"  F1 (weighted):      {results['test']['f1_weighted']:.4f}\")\n",
    "    \n",
    "    # Classification report\n",
    "    print(\"\\nDETAILED CLASSIFICATION REPORT (Test Set):\")\n",
    "    print(classification_report(y_test, y_test_pred))\n",
    "    \n",
    "    return results, y_train_pred, y_test_pred\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 5. VISUALIZATIONS\n",
    "# =============================================================================\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, title, output_path):\n",
    "    \n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    # Get unique labels\n",
    "    labels = sorted(y_true.unique())\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # Raw counts\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=labels, yticklabels=labels,\n",
    "                ax=axes[0], cbar_kws={'label': 'Count'})\n",
    "    axes[0].set_title(f'{title} - Raw Counts')\n",
    "    axes[0].set_xlabel('Predicted')\n",
    "    axes[0].set_ylabel('True')\n",
    "    \n",
    "    # Normalized\n",
    "    cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    sns.heatmap(cm_norm, annot=True, fmt='.2f', cmap='Blues',\n",
    "                xticklabels=labels, yticklabels=labels,\n",
    "                ax=axes[1], cbar_kws={'label': 'Proportion'})\n",
    "    axes[1].set_title(f'{title} - Normalized')\n",
    "    axes[1].set_xlabel('Predicted')\n",
    "    axes[1].set_ylabel('True')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"Saved: {output_path}\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_feature_importance(model, feature_names, output_path):\n",
    "    \n",
    "    # Get feature importance\n",
    "    importance = model.get_feature_importance()\n",
    "    \n",
    "    # Create dataframe\n",
    "    importance_df = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'importance': importance\n",
    "    }).sort_values('importance', ascending=True)\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, max(8, len(feature_names) * 0.3)))\n",
    "    \n",
    "    colors = plt.cm.viridis(importance_df['importance'] / importance_df['importance'].max())\n",
    "    \n",
    "    plt.barh(range(len(importance_df)), importance_df['importance'], color=colors)\n",
    "    plt.yticks(range(len(importance_df)), importance_df['feature'])\n",
    "    plt.xlabel('Importance')\n",
    "    plt.title('CatBoost Feature Importance')\n",
    "    plt.grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"Saved: {output_path}\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_metrics_comparison(results, output_path):\n",
    "    \n",
    "    metrics = ['accuracy', 'precision_weighted', 'recall_weighted', 'f1_weighted']\n",
    "    metric_names = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
    "    \n",
    "    train_scores = [results['train'][m] for m in metrics]\n",
    "    test_scores = [results['test'][m] for m in metrics]\n",
    "    \n",
    "    x = np.arange(len(metrics))\n",
    "    width = 0.35\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    bars1 = ax.bar(x - width/2, train_scores, width, label='Train', color='skyblue')\n",
    "    bars2 = ax.bar(x + width/2, test_scores, width, label='Test', color='coral')\n",
    "    \n",
    "    ax.set_xlabel('Metric')\n",
    "    ax.set_ylabel('Score')\n",
    "    ax.set_title('CatBoost Model Performance: Train vs Test')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(metric_names)\n",
    "    ax.legend()\n",
    "    ax.set_ylim([0, 1.1])\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bars in [bars1, bars2]:\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                   f'{height:.3f}',\n",
    "                   ha='center', va='bottom', fontsize=9)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"✓ Saved: {output_path}\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 6. SAVE RESULTS\n",
    "# =============================================================================\n",
    "\n",
    "def save_results(model, results, y_test, y_test_pred, feature_names, output_dir):\n",
    "    \n",
    "    output_dir = Path(output_dir)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"SAVING RESULTS\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # 1. Save model\n",
    "    model_path = output_dir / 'catboost_model.cbm'\n",
    "    model.save_model(str(model_path))\n",
    "    print(f\"Model saved: {model_path}\")\n",
    "    \n",
    "    # 2. Save metrics\n",
    "    metrics_path = output_dir / 'catboost_metrics.json'\n",
    "    with open(metrics_path, 'w') as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    print(f\"Metrics saved: {metrics_path}\")\n",
    "    \n",
    "    # 3. Save predictions\n",
    "    predictions_df = pd.DataFrame({\n",
    "        'true_label': y_test,\n",
    "        'predicted_label': y_test_pred\n",
    "    })\n",
    "    predictions_path = output_dir / 'catboost_predictions.csv'\n",
    "    predictions_df.to_csv(predictions_path, index=False)\n",
    "    print(f\"Predictions saved: {predictions_path}\")\n",
    "    \n",
    "    # 4. Save feature importance\n",
    "    importance_df = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'importance': model.get_feature_importance()\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    importance_path = output_dir / 'feature_importance.csv'\n",
    "    importance_df.to_csv(importance_path, index=False)\n",
    "    print(f\"Feature importance saved: {importance_path}\")\n",
    "    \n",
    "    # 5. Generate visualizations\n",
    "    print(\"\\nGenerating visualizations...\")\n",
    "    \n",
    "    # Confusion matrix\n",
    "    plot_confusion_matrix(\n",
    "        y_test, y_test_pred, \n",
    "        'CatBoost Confusion Matrix (Test Set)',\n",
    "        output_dir / 'confusion_matrix.png'\n",
    "    )\n",
    "    \n",
    "    # Feature importance\n",
    "    plot_feature_importance(\n",
    "        model, feature_names,\n",
    "        output_dir / 'feature_importance.png'\n",
    "    )\n",
    "    \n",
    "    # Metrics comparison\n",
    "    plot_metrics_comparison(\n",
    "        results,\n",
    "        output_dir / 'metrics_comparison.png'\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nAll results saved to: {output_dir}\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 7. MAIN PIPELINE\n",
    "# =============================================================================\n",
    "\n",
    "def run_catboost_pipeline(data_path, output_dir='evaluation_results/catboost'):\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"CATBOOST MODEL TRAINING PIPELINE\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Load data\n",
    "    df = load_data(data_path)\n",
    "    \n",
    "    # Prepare data\n",
    "    X, y = prepare_data(df)\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = split_data(X, y, test_size=0.2)\n",
    "    \n",
    "    # Train model\n",
    "    model = train_catboost(X_train, y_train, X_test, y_test)\n",
    "    \n",
    "    # Evaluate model\n",
    "    results, y_train_pred, y_test_pred = evaluate_model(\n",
    "        model, X_train, y_train, X_test, y_test\n",
    "    )\n",
    "    \n",
    "    # Save results\n",
    "    save_results(\n",
    "        model, results, y_test, y_test_pred, \n",
    "        X.columns.tolist(), output_dir\n",
    "    )\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"CATBOOST TRAINING COMPLETE\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(f\"\\nSUMMARY:\")\n",
    "    print(f\"  Test Accuracy: {results['test']['accuracy']:.4f}\")\n",
    "    print(f\"  Test F1 (weighted): {results['test']['f1_weighted']:.4f}\")\n",
    "    print(f\"\\nResults saved to: {output_dir}\")\n",
    "    print(f\"\\nNext step: Compare with symbolic reasoning results!\")\n",
    "    \n",
    "    return model, results\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 8. MAIN\n",
    "# =============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    data_path = 'data/migraine_with_id.csv'\n",
    "\n",
    "    model, results = run_catboost_pipeline(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013ac23a-efba-4710-ad23-0f0ff9fe7a34",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
